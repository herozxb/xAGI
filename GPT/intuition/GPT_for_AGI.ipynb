{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03098f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc3da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4cb0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# ==================small size=================\\n# hyperparameters\\nbatch_size = 8 # how many independent sequences will we process in parallel?\\nblock_size = 512 # what is the maximum input context length for predictions?\\nmax_iters = 5000\\neval_interval = 500\\nlearning_rate = 3e-4\\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\\neval_iters = 200\\nn_embedding = 128 # 32 =  head_size = n_embedding / n_head\\nn_head = 8\\nn_layer = 20\\ndropout = 0.2\\n# ------------\\n\\n#torch.manual_seed(1337)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# ==================small size=================\n",
    "# hyperparameters\n",
    "batch_size = 8 # how many independent sequences will we process in parallel?\n",
    "block_size = 512 # what is the maximum input context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embedding = 128 # 32 =  head_size = n_embedding / n_head\n",
    "n_head = 8\n",
    "n_layer = 20\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "#torch.manual_seed(1337)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ded7e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 8 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "max_iters = 20000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "eval_iters = 200\n",
    "n_embedding = 384\n",
    "n_head = 24\n",
    "n_layer = 12\n",
    "dropout = 0.2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2f46e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e904a097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Shakespeare\\nwith open('input.txt', 'r', encoding='utf-8') as f:\\n    text = f.read()\\n    #print(text)\\nprint(len(text))\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Shakespeare\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    #print(text)\n",
    "print(len(text))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4354225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649645\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7 habits of highly effective people\n",
    "with open('7_habits.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    #print(text)\n",
    "print(len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a5fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a6c2d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',', '|', 'R', 'y', '0', 'v', ' ', '#', 'M', 'J', 'L', 'A', 'C', '<', ':', '6', 'F', '&', 'i', '*', 'd', 'U', 'n', '}', '+', 'O', '!', '—', 'a', '□', 'm', 'Y', 'p', ')', '>', '^', 'u', '7', '2', ';', 'G', '”', '4', '9', '_', '.', '1', 'w', 'l', 'r', 'D', 'f', '’', 'T', \"'\", '\\\\', 'V', 'j', '=', 'c', 'E', '?', 'q', 'k', 'I', 'P', 'z', '¬', 't', 'H', 'K', 'Q', 'Z', 'N', '8', '\"', 'S', 'b', 'g', '$', 'o', '‘', '(', '/', 'h', '3', '\\n', 'X', '-', 'x', 'B', '[', 'e', '“', '5', 'W', 's'}\n",
      "[',', '|', 'R', 'y', '0', 'v', ' ', '#', 'M', 'J', 'L', 'A', 'C', '<', ':', '6', 'F', '&', 'i', '*', 'd', 'U', 'n', '}', '+', 'O', '!', '—', 'a', '□', 'm', 'Y', 'p', ')', '>', '^', 'u', '7', '2', ';', 'G', '”', '4', '9', '_', '.', '1', 'w', 'l', 'r', 'D', 'f', '’', 'T', \"'\", '\\\\', 'V', 'j', '=', 'c', 'E', '?', 'q', 'k', 'I', 'P', 'z', '¬', 't', 'H', 'K', 'Q', 'Z', 'N', '8', '\"', 'S', 'b', 'g', '$', 'o', '‘', '(', '/', 'h', '3', '\\n', 'X', '-', 'x', 'B', '[', 'e', '“', '5', 'W', 's']\n",
      "['\\n', ' ', '!', '\"', '#', '$', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', '¬', '—', '‘', '’', '“', '”', '□']\n",
      "97\n",
      "0\n",
      "[32, 33, 34]\n",
      "ABC\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "print(set(text))\n",
    "print(list(set(text)))\n",
    "print(sorted(list(set(text))))\n",
    "print(len(sorted(list(set(text)))))\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(stoi['\\n'])\n",
    "print(encode(['A','B','C']))\n",
    "print(decode(encode(['A','B','C'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3c395a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990416\n"
     ]
    }
   ],
   "source": [
    "# rich dad, poor dad\n",
    "with open('rich_dad_poor_dad.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    #print(text)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b1e2305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\\\', 'w', ',', 'A', '#', '—', 'Z', '?', '5', \"'\", 'p', 'q', '(', 'm', 'M', '&', 'd', 'Q', 'b', '}', '^', '¬', '1', 'I', '+', '=', '2', 'E', '□', '$', 'f', '\"', 'c', 'B', '|', 'N', ' ', ':', '’', '/', 'a', 'o', 'R', '0', 'g', 'K', 'W', 'l', '”', 'y', 'e', '6', '8', 't', 'n', '\\n', 'S', 'j', 'z', '[', 'J', '!', 'i', 'L', 'F', '>', 'x', 'v', '.', '3', 'X', '*', ')', 'h', 'P', 's', '-', 'C', 'U', '9', 'D', 'H', 'G', 'Y', '<', '4', '“', '7', 'O', 'r', '‘', 'k', 'T', 'V', '_', 'u', ';'}\n",
      "['\\\\', 'w', ',', 'A', '#', '—', 'Z', '?', '5', \"'\", 'p', 'q', '(', 'm', 'M', '&', 'd', 'Q', 'b', '}', '^', '¬', '1', 'I', '+', '=', '2', 'E', '□', '$', 'f', '\"', 'c', 'B', '|', 'N', ' ', ':', '’', '/', 'a', 'o', 'R', '0', 'g', 'K', 'W', 'l', '”', 'y', 'e', '6', '8', 't', 'n', '\\n', 'S', 'j', 'z', '[', 'J', '!', 'i', 'L', 'F', '>', 'x', 'v', '.', '3', 'X', '*', ')', 'h', 'P', 's', '-', 'C', 'U', '9', 'D', 'H', 'G', 'Y', '<', '4', '“', '7', 'O', 'r', '‘', 'k', 'T', 'V', '_', 'u', ';']\n",
      "['\\n', ' ', '!', '\"', '#', '$', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', '¬', '—', '‘', '’', '“', '”', '□']\n",
      "97\n",
      "0\n",
      "[32, 33, 34]\n",
      "ABC\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "print(set(text))\n",
    "print(list(set(text)))\n",
    "print(sorted(list(set(text))))\n",
    "print(len(sorted(list(set(text)))))\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(stoi['\\n'])\n",
    "print(encode(['A','B','C']))\n",
    "print(decode(encode(['A','B','C'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9436653d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([51, 39, 36,  1, 50, 36, 53, 36, 45,  1, 39, 32, 33, 39, 50,  1, 46, 37,\n",
      "         1, 39, 40, 38, 39, 43, 56,  1, 36, 37, 37, 36, 34, 51, 40, 53, 36,  1,\n",
      "        47, 36, 46, 47, 43, 36,  1,  0,  0,  0, 33, 79, 62, 71, 64, 71, 81,  1,\n",
      "        81, 76,  1, 86, 76, 82,  1, 81, 86, 39, 86, 39, 80, 82, 81,  1,  0,  0,\n",
      "         0, 51, 39, 36,  1, 50, 36, 53, 36, 45,  1, 39, 32, 33, 40, 51, 50,  1,\n",
      "        46, 37,  1, 39, 40, 38, 39, 43, 56,  1])\n",
      "THE SEVEN HABHS OF HIGHLY EFFECTIVE PEOPLE \n",
      "\n",
      "\n",
      "Brajcjt to you tyHyHsut \n",
      "\n",
      "\n",
      "THE SEVEN HABITS OF HIGHLY \n",
      "['\\n', ' ', '!', '\"', '#', '$', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', '¬', '—', '‘', '’', '“', '”', '□']\n",
      "tensor([51, 39, 36,  1, 50, 36, 53, 36, 45,  1, 39, 32, 33, 39, 50,  1, 46, 37,\n",
      "         1, 39, 40, 38, 39, 43, 56,  1, 36, 37, 37, 36, 34, 51, 40, 53, 36,  1,\n",
      "        47, 36, 46, 47, 43, 36,  1,  0,  0,  0, 33, 79, 62, 71, 64, 71, 81,  1,\n",
      "        81, 76,  1, 86, 76, 82,  1, 81, 86, 39, 86, 39, 80, 82, 81,  1,  0,  0,\n",
      "         0, 51, 39, 36,  1, 50, 36, 53, 36, 45,  1, 39, 32, 33, 40, 51, 50,  1,\n",
      "        46, 37,  1, 39, 40, 38, 39, 43, 56,  1, 36, 37, 37, 36, 34, 51, 40, 53,\n",
      "        36,  1, 47, 36, 46, 47, 43, 36,  1,  0,  0, 50, 81, 66, 77, 69, 66, 75,\n",
      "        49, 14,  1])\n",
      "THE SEVEN HABHS OF HIGHLY EFFECTIVE PEOPLE \n",
      "\n",
      "\n",
      "Brajcjt to you tyHyHsut \n",
      "\n",
      "\n",
      "THE SEVEN HABITS OF HIGHLY EFFECTIVE PEOPLE \n",
      "\n",
      "StephenR. \n"
     ]
    }
   ],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(train_data[0:100])\n",
    "print(text[0:100])\n",
    "print(sorted(list(set(text))))\n",
    "\n",
    "\n",
    "#block_size = 8\n",
    "print(train_data[0:block_size+1])\n",
    "print(text[0:block_size+1])\n",
    "\n",
    "\n",
    "x = train_data[0:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "  context = x[0:t+1]\n",
    "  target = y[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e4056d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "torch.Size([8, 128])\n",
      "tensor([[81, 79, 70,  ...,  0,  0, 69],\n",
      "        [82, 73,  1,  ..., 81, 66, 79],\n",
      "        [67, 66, 62,  ..., 81, 66, 62],\n",
      "        ...,\n",
      "        [82,  1, 81,  ..., 77, 80,  1],\n",
      "        [76, 67, 66,  ..., 69, 81,  1],\n",
      "        [ 1, 65, 70,  ..., 75, 66, 84]], device='cuda:0')\n",
      "target:\n",
      "torch.Size([8, 128])\n",
      "tensor([[79, 70, 66,  ...,  0, 69, 81],\n",
      "        [73,  1, 67,  ..., 66, 79,  1],\n",
      "        [66, 62, 79,  ..., 66, 62, 64],\n",
      "        ...,\n",
      "        [ 1, 81, 69,  ..., 80,  1, 62],\n",
      "        [67, 66, 80,  ..., 81,  1, 81],\n",
      "        [65, 70, 80,  ..., 66, 84, 12]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "#torch.manual_seed(1337)\n",
    "#batch_size = 4\n",
    "#block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"input:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"target:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "  for t in range(block_size):\n",
    "    context = xb[ b, 0:t+1 ]\n",
    "    target = yb[ b, t ]\n",
    "    #print(context,\"->\",target)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e3aad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GateLayer, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, n_embedding)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, encoder_output):\n",
    "        # Compute gating vector\n",
    "        gating_vector = self.linear(encoder_output)\n",
    "        gating_vector = self.sigmoid(gating_vector)\n",
    "        # Apply gating vector to encoder output\n",
    "        gated_encoder_output = encoder_output * gating_vector\n",
    "        \n",
    "        return gated_encoder_output\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key  = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gate = GateLayer( n_embedding, head_size )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        #print(\"====x=====\")\n",
    "        #print(x.shape)\n",
    "        #x = self.gate(x)\n",
    "\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        #print(k.shape)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        attention_part = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        attention_part = attention_part.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        attention_part = F.softmax(attention_part, dim=-1) # (B, T, T)\n",
    "        attention_part = self.dropout(attention_part)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = attention_part @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(n_embedding, n_embedding)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.linear(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embedding):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embedding, 4 * n_embedding),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embedding, n_embedding),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embedding, n_head):\n",
    "        # n_embedding: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embedding // n_head\n",
    "        self.multi_head_attention = MultiHeadAttention(n_head, head_size)\n",
    "        self.feed_forward = FeedFoward(n_embedding)\n",
    "        self.layer_norm_1 = nn.LayerNorm(n_embedding)\n",
    "        self.layer_norm_2 = nn.LayerNorm(n_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.multi_head_attention(self.layer_norm_1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class GPT2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_into_embedding = nn.Embedding(vocab_size, n_embedding)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embedding)\n",
    "        \n",
    "        #Input: (∗), IntTensor or LongTensor of arbitrary shape containing the indices to extract\n",
    "        #Output: (∗,H), where * is the input shape and H=embedding_dim\n",
    "        \n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block(n_embedding, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.layer_norm = nn.LayerNorm(n_embedding) # final layer norm\n",
    "        #self.ffwd = FeedFoward(n_embedding)\n",
    "        #self.sa_head = MultiHeadAttention(4,n_embedding//4)\n",
    "        self.linear_head = nn.Linear(n_embedding, vocab_size)\n",
    "\n",
    "    def forward(self, id_number_of_vector_x, targets=None):#target (B,T)\n",
    "        #B,T =id_number_of_vector_x.shape\n",
    "        #tok_embed = self.token_into_embedding(id_number_of_vector_x) #(B,T,C) (batch,Time,Channel)\n",
    "        #logits = self.lm_head(tok_embed) #(B,T,vocab_size)\n",
    "\n",
    "        B, T = id_number_of_vector_x.shape\n",
    "\n",
    "        # id_number_of_vector_x and targets are both (B,T) tensor of integers\n",
    "        #tok_emb = self.token_into_embedding(id_number_of_vector_x) # (B,T,C)\n",
    "        #pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        #x = tok_emb + pos_emb # (B,T,C) + pos_emb across the batch\n",
    "        #x = self.blocks(x) # (B,T,C)\n",
    "        #x = self.ln_f(x) # (B,T,C)\n",
    "        \n",
    "        #x = self.sa_head(x) # (B,T,vocab_size)\n",
    "        #logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        # id_number_of_vector_x and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_into_embedding(id_number_of_vector_x) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        #x = tok_emb + pos_emb # (B,T,C)\n",
    "        #x = self.blocks(x) # (B,T,C)\n",
    "        #x = self.ln_f(x) # (B,T,C)\n",
    "        #x = self.sa_head(x) # (B,T,vocab_size)\n",
    "        #x = self.ffwd(x)\n",
    "        #logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,T,C) + pos_emb across the batch\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.layer_norm(x) # (B,T,C)\n",
    "        \n",
    "        #x = self.sa_head(x) # (B,T,vocab_size)\n",
    "        logits = self.linear_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets == None:\n",
    "          loss = None\n",
    "        else:\n",
    "          B, T, C = logits.shape # logit(p) = ln( p / ( 1 - p ) )\n",
    "          logits = logits.view(B*T,C)\n",
    "          targets = targets.view(B*T)\n",
    "          loss = F.cross_entropy(logits,targets) # H( P, Q ) = -0.9 * log( 0.8 ) - 0.1 * log( 0.2 ) = 0.311, the lower the better matching\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, id_number_of_vector_x, max_new_tokens):\n",
    "        # id_number_of_vector_x is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop id_number_of_vector_x to the last block_size tokens\n",
    "            id_number_of_vector_x_cut = id_number_of_vector_x[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(id_number_of_vector_x_cut)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            id_number_of_vector_x_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            id_number_of_vector_x = torch.cat((id_number_of_vector_x, id_number_of_vector_x_next), dim=1) # (B, T+1)\n",
    "        return id_number_of_vector_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "109d8731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.982177 M parameters\n",
      "torch.Size([1024, 97]) tensor(4.7217, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "model.load_state_dict(torch.load(\"./model\"))\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "'''\n",
    "\n",
    "model = GPT2()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "logits, loss = m(xb,yb)\n",
    "print(logits.shape,loss) # -ln(1/65) = 4.174387269896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a47f00fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.982177 M parameters\n",
      "tensor([[0]])\n",
      "tensor([[32]], device='cuda:0')\n",
      "A Personal M inds \n",
      "\n",
      "You wearehour is greater income; and hence the \n",
      "fame of retirement. Paradigm Shif\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GPT2()\n",
    "#model.load_state_dict(torch.load(\"./GPT2_Shakespeare\"))\n",
    "#model.load_state_dict(torch.load(\"./GPT2_rich_dad_poor_dad\"))\n",
    "model.load_state_dict(torch.load(\"./GPT2_rich_dad_poor_dad_Fine-tuning_with_Custom_Datasets\"))\n",
    "\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "id_number_of_vector_x = torch.zeros((1,1), dtype=torch.long)\n",
    "print(id_number_of_vector_x)\n",
    "print(torch.tensor([[32]]).to(device))\n",
    "#print(decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=10)[0].tolist()))\n",
    "\n",
    "print(decode(m.generate(torch.tensor([[32]]).to(device), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84b21179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is what world famous speaker and author Anthony Robbins says about Robert’s work. \n",
      "\n",
      "The reality of change is in the entire center’s common. \n",
      "We sensee of empathic affects the other a\n"
     ]
    }
   ],
   "source": [
    "# not working because of the small size of the LLM\n",
    "text = \"This is what world famous speaker and author Anthony Robbins says about Robert’s work. \"\n",
    "context = torch.tensor( [ encode(text) ], dtype=torch.long, device=device )\n",
    "print(decode(m.generate( context, max_new_tokens=100 )[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01f22354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how about the PC, and money, how do you think about the PC and learning \n",
      "\n",
      "\"Dady hard more than just going along with your friends and you did not crease?\" \n",
      "\n",
      "Because those a character empowers us. With correct principles, with panic hand bitterms and \n",
      "are important for malfilure in management. Balances in which you are freedom. Try to accomplish these principles, \n",
      "knowledge and there are more in the second creation, no appreciation; win someone else's words have enough \n",
      "creating. Put another good reeply. N eighborce to feedback with a “best-selling author,”</p> <div \n",
      "class=’story_text’>\" And how I do went because, we do not respondence, and sittle two training for \n",
      "a conspencial mental while Was So on the big -- selling script, and was recipited with old patern. \n",
      "\n",
      "\"Who're the gold meminatry was happening in our interactions with others, what these popular habits \n",
      "are habits. \n",
      "\n",
      "My habits of real estate and marriage are so copeaning other untity. \n",
      "Desp everything I have never communication \n",
      "- someone who significently seeking to win--win, no regarding commun\n"
     ]
    }
   ],
   "source": [
    "# not working because of the small size of the LLM\n",
    "text = \"how about the PC, and money, how do you think about the PC and learning\"\n",
    "context = torch.tensor( [ encode(text) ], dtype=torch.long, device=device )\n",
    "print(decode(m.generate( context, max_new_tokens=1000 )[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc45184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how about the Production Capability, and money, how do you think about the Porduction Capability and learning \n",
      "communication exposite skills that have enorcept others. In cases it, simply because they find \n",
      "a Power or Psyptent Ce, sitting a collecting child good way, a decision that wanted to understand \n",
      "either or forced to inside his energy. \n",
      "\n",
      "Spent the future coffee interned opening to enjuy Mike. And more intimate interuth to her. \n",
      "\n",
      "There before we expensive wikly on the Seven Habits is, too many real self-reliant with our own \n",
      "confront is evaparane, ulevision. There are times to go through these when they \n",
      "were subject. \n",
      "\n",
      "No our schedule to think, on the other hand, performance the divi difference in an attempt of \n",
      "principles and disciplineess. Many five major income up soundies to \n",
      "work to master than money. They’re too our speaker -1, So The jobs one and terribure show hard from \n",
      "rich. Don’t get on my own company can career. Instead of the United \n",
      "States Corp mode to stay \"Just listnane.\" \n",
      "At the evening of my vuote to do the stament, I wouldn't be financial ing to subward. \n",
      "My stock-to\n"
     ]
    }
   ],
   "source": [
    "# not working because of the small size of the LLM\n",
    "text = \"how about the Production Capability, and money, how do you think about the Porduction Capability and learning\"\n",
    "context = torch.tensor( [ encode(text) ], dtype=torch.long, device=device )\n",
    "print(decode(m.generate( context, max_new_tokens=1000 )[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1d211ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to make money is to inice ut of school to school because pulled and no \n",
      "supervise methods. Dearn freedoms in the reality that changes we \n",
      "did not come to love. \n",
      "\n",
      "\n",
      "Suppose time I say to many to manipuls into the marriages considerations of love to significantly \n",
      "describe about those rich ship that most; they are the most, looking at their lives, their next only \n",
      "tax door because on their taxes.” Most people are interest of us around. Most peoplesis are \n",
      "not mire people are more objective and maintance in their work. \n",
      "\n",
      "Going on the main juni insigh school is going to be a coming in the perspective wealth. \n",
      "\n",
      "We inside the earlier man, revenues to give picture them. Teaning sidestily new captured into their \n",
      "incomes paking and give in their spirit because you go to scope to where yonce you make the time into \n",
      "fact, and I went to schedule out of that obne, our boss!\" \n",
      "\n",
      "We went into a ti me al\n",
      "Our envision of considerating coum from than the problem. \n",
      "\n",
      "I'm not intrinsic society. Indifying words, every wo\n"
     ]
    }
   ],
   "source": [
    "# not working because of the small size of the LLM\n",
    "text = \"how to make money\"\n",
    "context = torch.tensor( [ encode(text) ], dtype=torch.long, device=device )\n",
    "print(decode(m.generate( context, max_new_tokens=1000 )[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be071f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another \n",
      "day, whicholocing work against the glocum. They might decide to tell you the same this tyle \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.tensor([[32]]).to(device), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ef93a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_name', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'bias', 'buffers', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'half', 'in_features', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'out_features', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'requires_grad_', 'reset_parameters', 'share_memory', 'state_dict', 'to', 'train', 'training', 'type', 'weight', 'zero_grad']\n",
      "<bound method Module.type of Linear(in_features=384, out_features=16, bias=False)>\n",
      "384\n",
      "16\n",
      "torch.Size([16, 384])\n",
      "torch.Size([384])\n",
      "['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_name', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'buffers', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'embedding_dim', 'eval', 'extra_repr', 'float', 'forward', 'from_pretrained', 'half', 'load_state_dict', 'max_norm', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'norm_type', 'num_embeddings', 'padding_idx', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'requires_grad_', 'reset_parameters', 'scale_grad_by_freq', 'share_memory', 'sparse', 'state_dict', 'to', 'train', 'training', 'type', 'weight', 'zero_grad']\n",
      "['T', '__abs__', '__add__', '__and__', '__array__', '__array_priority__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', '__cuda_array_interface__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__idiv__', '__ifloordiv__', '__ilshift__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__long__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rfloordiv__', '__rmul__', '__rpow__', '__rshift__', '__rsub__', '__rtruediv__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__torch_function__', '__truediv__', '__weakref__', '__xor__', '_backward_hooks', '_base', '_cdata', '_coalesced_', '_dimI', '_dimV', '_grad', '_grad_fn', '_indices', '_is_view', '_make_subclass', '_nnz', '_update_names', '_values', '_version', 'abs', 'abs_', 'absolute', 'absolute_', 'acos', 'acos_', 'acosh', 'acosh_', 'add', 'add_', 'addbmm', 'addbmm_', 'addcdiv', 'addcdiv_', 'addcmul', 'addcmul_', 'addmm', 'addmm_', 'addmv', 'addmv_', 'addr', 'addr_', 'align_as', 'align_to', 'all', 'allclose', 'amax', 'amin', 'angle', 'any', 'apply_', 'arccos', 'arccos_', 'arccosh', 'arccosh_', 'arcsin', 'arcsin_', 'arcsinh', 'arcsinh_', 'arctan', 'arctan_', 'arctanh', 'arctanh_', 'argmax', 'argmin', 'argsort', 'as_strided', 'as_strided_', 'as_subclass', 'asin', 'asin_', 'asinh', 'asinh_', 'atan', 'atan2', 'atan2_', 'atan_', 'atanh', 'atanh_', 'backward', 'baddbmm', 'baddbmm_', 'bernoulli', 'bernoulli_', 'bfloat16', 'bincount', 'bitwise_and', 'bitwise_and_', 'bitwise_not', 'bitwise_not_', 'bitwise_or', 'bitwise_or_', 'bitwise_xor', 'bitwise_xor_', 'bmm', 'bool', 'byte', 'cauchy_', 'ceil', 'ceil_', 'char', 'cholesky', 'cholesky_inverse', 'cholesky_solve', 'chunk', 'clamp', 'clamp_', 'clamp_max', 'clamp_max_', 'clamp_min', 'clamp_min_', 'clip', 'clip_', 'clone', 'coalesce', 'conj', 'contiguous', 'copy_', 'cos', 'cos_', 'cosh', 'cosh_', 'count_nonzero', 'cpu', 'cross', 'cuda', 'cummax', 'cummin', 'cumprod', 'cumsum', 'data', 'data_ptr', 'deg2rad', 'deg2rad_', 'dense_dim', 'dequantize', 'det', 'detach', 'detach_', 'device', 'diag', 'diag_embed', 'diagflat', 'diagonal', 'digamma', 'digamma_', 'dim', 'dist', 'div', 'div_', 'divide', 'divide_', 'dot', 'double', 'dtype', 'eig', 'element_size', 'eq', 'eq_', 'equal', 'erf', 'erf_', 'erfc', 'erfc_', 'erfinv', 'erfinv_', 'exp', 'exp2', 'exp2_', 'exp_', 'expand', 'expand_as', 'expm1', 'expm1_', 'exponential_', 'fft', 'fill_', 'fill_diagonal_', 'fix', 'fix_', 'flatten', 'flip', 'fliplr', 'flipud', 'float', 'floor', 'floor_', 'floor_divide', 'floor_divide_', 'fmod', 'fmod_', 'frac', 'frac_', 'gather', 'gcd', 'gcd_', 'ge', 'ge_', 'geometric_', 'geqrf', 'ger', 'get_device', 'grad', 'grad_fn', 'greater', 'greater_', 'greater_equal', 'greater_equal_', 'gt', 'gt_', 'half', 'hardshrink', 'has_names', 'heaviside', 'heaviside_', 'histc', 'hypot', 'hypot_', 'i0', 'i0_', 'ifft', 'imag', 'index_add', 'index_add_', 'index_copy', 'index_copy_', 'index_fill', 'index_fill_', 'index_put', 'index_put_', 'index_select', 'indices', 'int', 'int_repr', 'inverse', 'irfft', 'is_coalesced', 'is_complex', 'is_contiguous', 'is_cuda', 'is_distributed', 'is_floating_point', 'is_leaf', 'is_meta', 'is_mkldnn', 'is_nonzero', 'is_pinned', 'is_quantized', 'is_same_size', 'is_set_to', 'is_shared', 'is_signed', 'is_sparse', 'isclose', 'isfinite', 'isinf', 'isnan', 'isneginf', 'isposinf', 'isreal', 'istft', 'item', 'kthvalue', 'layout', 'lcm', 'lcm_', 'le', 'le_', 'lerp', 'lerp_', 'less', 'less_', 'less_equal', 'less_equal_', 'lgamma', 'lgamma_', 'log', 'log10', 'log10_', 'log1p', 'log1p_', 'log2', 'log2_', 'log_', 'log_normal_', 'log_softmax', 'logaddexp', 'logaddexp2', 'logcumsumexp', 'logdet', 'logical_and', 'logical_and_', 'logical_not', 'logical_not_', 'logical_or', 'logical_or_', 'logical_xor', 'logical_xor_', 'logit', 'logit_', 'logsumexp', 'long', 'lstsq', 'lt', 'lt_', 'lu', 'lu_solve', 'map2_', 'map_', 'masked_fill', 'masked_fill_', 'masked_scatter', 'masked_scatter_', 'masked_select', 'matmul', 'matrix_exp', 'matrix_power', 'max', 'maximum', 'mean', 'median', 'min', 'minimum', 'mm', 'mode', 'movedim', 'mul', 'mul_', 'multinomial', 'multiply', 'multiply_', 'mv', 'mvlgamma', 'mvlgamma_', 'name', 'names', 'nanquantile', 'nansum', 'narrow', 'narrow_copy', 'ndim', 'ndimension', 'ne', 'ne_', 'neg', 'neg_', 'negative', 'negative_', 'nelement', 'new', 'new_empty', 'new_full', 'new_ones', 'new_tensor', 'new_zeros', 'nextafter', 'nextafter_', 'nonzero', 'norm', 'normal_', 'not_equal', 'not_equal_', 'numel', 'numpy', 'orgqr', 'ormqr', 'outer', 'output_nr', 'permute', 'pin_memory', 'pinverse', 'polygamma', 'polygamma_', 'pow', 'pow_', 'prelu', 'prod', 'put_', 'q_per_channel_axis', 'q_per_channel_scales', 'q_per_channel_zero_points', 'q_scale', 'q_zero_point', 'qr', 'qscheme', 'quantile', 'rad2deg', 'rad2deg_', 'random_', 'real', 'reciprocal', 'reciprocal_', 'record_stream', 'refine_names', 'register_hook', 'reinforce', 'relu', 'relu_', 'remainder', 'remainder_', 'rename', 'rename_', 'renorm', 'renorm_', 'repeat', 'repeat_interleave', 'requires_grad', 'requires_grad_', 'reshape', 'reshape_as', 'resize', 'resize_', 'resize_as', 'resize_as_', 'retain_grad', 'rfft', 'roll', 'rot90', 'round', 'round_', 'rsqrt', 'rsqrt_', 'scatter', 'scatter_', 'scatter_add', 'scatter_add_', 'select', 'set_', 'sgn', 'sgn_', 'shape', 'share_memory_', 'short', 'sigmoid', 'sigmoid_', 'sign', 'sign_', 'signbit', 'sin', 'sin_', 'sinh', 'sinh_', 'size', 'slogdet', 'smm', 'softmax', 'solve', 'sort', 'sparse_dim', 'sparse_mask', 'sparse_resize_', 'sparse_resize_and_clear_', 'split', 'split_with_sizes', 'sqrt', 'sqrt_', 'square', 'square_', 'squeeze', 'squeeze_', 'sspaddmm', 'std', 'stft', 'storage', 'storage_offset', 'storage_type', 'stride', 'sub', 'sub_', 'subtract', 'subtract_', 'sum', 'sum_to_size', 'svd', 'symeig', 't', 't_', 'take', 'tan', 'tan_', 'tanh', 'tanh_', 'to', 'to_dense', 'to_mkldnn', 'to_sparse', 'tolist', 'topk', 'trace', 'transpose', 'transpose_', 'triangular_solve', 'tril', 'tril_', 'triu', 'triu_', 'true_divide', 'true_divide_', 'trunc', 'trunc_', 'type', 'type_as', 'unbind', 'unflatten', 'unfold', 'uniform_', 'unique', 'unique_consecutive', 'unsafe_chunk', 'unsafe_split', 'unsafe_split_with_sizes', 'unsqueeze', 'unsqueeze_', 'values', 'var', 'vdot', 'view', 'view_as', 'where', 'zero_']\n",
      "torch.Size([97, 384])\n",
      "tensor([ 2.1269e+00, -2.0395e+00,  2.5961e-01,  9.5541e-02,  9.1339e-01,\n",
      "        -1.7481e+00, -8.1398e-02, -5.1054e-01,  8.5719e-01,  1.2983e+00,\n",
      "        -8.0849e-01,  9.0590e-01,  5.2967e-01,  6.2901e-02,  2.7608e-01,\n",
      "        -5.7113e-01, -3.9285e-01,  8.9938e-01, -1.5385e+00, -3.9240e-01,\n",
      "        -4.1678e-01,  2.1557e-01, -4.3753e-01,  8.6859e-01, -5.4442e-01,\n",
      "         3.8927e-01,  6.3001e-01, -1.9358e+00, -1.4013e+00, -4.6383e-02,\n",
      "        -1.4897e+00,  3.7499e-01,  1.8508e-02, -1.3191e+00,  1.9535e+00,\n",
      "         3.4521e-01,  3.3860e-01, -4.1581e-01,  5.8886e-01,  1.0504e+00,\n",
      "         5.8645e-01, -6.0785e-03,  4.3526e-01, -3.3291e-01,  2.4840e-01,\n",
      "        -2.6360e-01, -1.2484e+00, -9.4945e-01, -1.1142e+00,  1.3004e-01,\n",
      "        -8.0515e-01, -4.6200e-02, -2.0734e+00,  6.5180e-01,  3.4154e-01,\n",
      "        -8.0090e-01, -1.8174e+00, -9.8826e-01,  2.1338e+00,  1.4834e+00,\n",
      "         7.5346e-01, -2.2291e+00,  3.3728e-01,  2.6908e-01,  6.2142e-01,\n",
      "        -1.5069e-01, -4.4471e-01,  6.6964e-01, -1.3737e-01,  5.1125e-01,\n",
      "         2.3740e+00, -7.4091e-01, -1.1674e-01,  6.6623e-01,  2.3672e-01,\n",
      "         1.1234e-01, -8.6804e-01,  7.8235e-01,  1.0296e+00,  4.6530e-02,\n",
      "        -1.0530e+00,  4.3812e-01, -8.5605e-01,  9.1103e-01,  1.7699e+00,\n",
      "         1.2674e+00, -2.7373e-02, -1.3853e-01, -1.1644e+00, -3.8123e-01,\n",
      "        -1.6592e-01, -6.0663e-01,  1.0265e+00, -3.9307e-02, -1.5577e+00,\n",
      "        -6.5905e-02, -4.7115e-01,  5.2016e-01, -5.8893e-01, -8.2977e-01,\n",
      "        -7.2130e-01, -1.0459e+00,  9.4647e-01, -8.7606e-01,  3.7588e-01,\n",
      "         2.6210e-01, -2.3477e+00,  3.1995e-01,  8.0577e-01,  7.6998e-01,\n",
      "         1.0649e+00, -2.0572e-01,  1.6405e-01, -1.1984e-01,  4.0687e-01,\n",
      "        -1.0159e+00,  5.3326e-01, -1.5562e-01, -3.7526e-01,  5.8510e-01,\n",
      "         2.1961e+00, -6.5123e-01, -3.3842e-01,  1.0468e+00, -7.6303e-01,\n",
      "         8.2922e-04, -2.4391e-01, -8.2397e-01, -2.4124e-01, -7.3340e-01,\n",
      "         5.4333e-03,  2.4826e+00,  3.9712e-01,  3.7020e-01,  5.3856e-01,\n",
      "        -1.2254e+00,  1.2661e+00,  1.1846e+00, -9.0545e-01,  9.2077e-01,\n",
      "        -8.7936e-01, -5.4233e-02, -1.1978e-01, -1.5905e+00,  2.2893e-01,\n",
      "        -1.0086e+00,  6.1280e-01,  1.4413e+00,  5.9554e-01,  5.4255e-01,\n",
      "        -6.3807e-01, -1.2439e+00,  3.8310e-01,  5.9511e-01, -8.3407e-01,\n",
      "        -5.9118e-01,  4.6579e-01,  1.9147e-01, -2.5134e-01,  3.6625e-01,\n",
      "         7.5122e-01, -3.3905e-01, -4.4546e-01, -1.6980e+00,  9.5995e-01,\n",
      "        -3.2678e-01,  4.2131e-01,  2.1774e+00, -1.0843e+00, -8.6936e-01,\n",
      "        -5.3588e-01,  1.2823e+00, -8.2346e-01, -2.1255e-01,  7.2281e-01,\n",
      "         1.2097e+00,  6.1014e-01,  8.8117e-01,  1.9023e+00,  1.3907e+00,\n",
      "         1.6825e-01, -6.5788e-01, -1.0307e-02,  1.4466e+00, -8.0055e-01,\n",
      "        -1.6728e+00, -5.7993e-01,  7.1323e-01,  5.6879e-01, -7.7887e-01,\n",
      "        -1.0842e+00,  7.2075e-01,  6.8573e-01, -1.1133e+00,  1.1173e+00,\n",
      "        -3.8681e-01, -1.2410e+00, -8.0433e-01, -5.7997e-01, -4.2775e-01,\n",
      "        -3.2923e-01, -1.8655e-01, -1.0566e+00,  2.3735e-01,  9.1803e-01,\n",
      "         3.7928e-01,  1.3675e+00, -1.4783e+00, -1.2871e+00, -9.3189e-01,\n",
      "        -1.7066e+00, -3.0681e-01, -1.5072e+00, -1.5573e+00, -6.7013e-02,\n",
      "         2.7653e+00, -1.8260e-01, -4.6451e-01,  7.5942e-01,  1.7074e+00,\n",
      "         4.8824e-02, -6.1820e-01, -1.8774e-01, -1.0696e+00,  1.0783e+00,\n",
      "         1.8795e-01,  9.3305e-01, -2.0026e-01,  3.8856e-01,  5.0691e-01,\n",
      "         1.0675e-02, -5.4979e-01, -1.7846e+00, -1.1100e+00,  2.0636e+00,\n",
      "         2.6253e-01, -2.3136e+00, -1.2063e+00,  1.3911e+00, -9.9457e-01,\n",
      "         5.7873e-01, -5.6773e-01, -4.7378e-01, -1.7931e+00,  2.3515e+00,\n",
      "        -2.2399e-01,  4.6759e-01,  1.7207e+00, -7.9840e-02, -3.5390e-01,\n",
      "        -7.5369e-01, -1.2345e+00, -4.1047e-01, -1.5228e+00, -8.8261e-01,\n",
      "         1.7434e+00,  9.6923e-01,  1.2321e+00,  2.0764e+00, -3.6215e-01,\n",
      "         1.1009e+00, -5.9349e-01,  5.7077e-01, -6.7528e-01, -1.3500e+00,\n",
      "         1.4107e+00, -3.0131e-01, -9.8692e-01, -2.7574e-01, -1.8682e+00,\n",
      "        -1.1844e-01, -1.0044e+00, -4.6465e-01, -1.1770e+00,  7.0828e-01,\n",
      "        -7.7304e-03,  6.5804e-01,  1.7231e+00, -1.4853e+00, -2.7397e-01,\n",
      "        -1.4821e+00,  1.3277e+00,  9.3266e-01, -9.0373e-01,  3.6015e-01,\n",
      "        -6.0971e-01,  3.7567e-02, -2.7786e-01,  4.6531e-01, -1.1814e+00,\n",
      "        -7.5224e-01, -3.9828e-01, -5.0218e-01,  6.5651e-02, -1.3106e+00,\n",
      "         1.4760e+00, -6.8209e-01,  1.9163e+00,  1.4572e-01,  5.4662e-01,\n",
      "         9.2018e-01, -1.2895e-01,  4.9333e-01,  8.0799e-01, -5.1462e-01,\n",
      "         7.6985e-01, -2.0641e-01,  7.5646e-01, -4.2316e-02, -9.9039e-01,\n",
      "         2.1431e-02, -1.8287e+00,  1.3643e+00, -1.9093e-01, -5.3346e-01,\n",
      "        -7.6759e-01,  1.2618e+00, -6.8474e-01, -1.1438e-02,  1.1340e+00,\n",
      "         5.2220e-01, -1.8218e+00, -4.1254e-01, -6.2404e-01, -2.6557e-01,\n",
      "        -4.1504e-01, -4.2614e-01,  1.1044e+00,  3.7886e-01,  6.5875e-01,\n",
      "        -4.2848e-01, -1.6400e+00, -9.0911e-01,  1.9152e-01, -3.2350e-02,\n",
      "        -6.5861e-01,  1.6174e+00, -1.4425e-01, -2.8234e+00,  5.0984e-01,\n",
      "        -1.3729e+00,  1.0894e+00, -5.5381e-01,  4.0725e-03,  3.1486e-01,\n",
      "        -1.0320e+00,  1.2713e+00, -8.8077e-01,  5.6102e-02, -4.4876e-01,\n",
      "        -1.3286e+00, -1.9133e-02,  1.4787e+00,  3.9918e-01,  1.2327e+00,\n",
      "         8.9135e-01, -1.5263e+00, -2.7284e+00, -2.6203e-01, -6.1997e-01,\n",
      "        -3.0127e-01, -6.2635e-02,  6.6416e-01, -6.8009e-01, -2.2462e+00,\n",
      "        -9.6207e-01,  9.1753e-01,  1.7563e+00, -5.2691e-01,  1.0751e+00,\n",
      "        -1.2777e+00,  5.6987e-01,  3.1891e-01,  1.7145e+00, -1.1630e-01,\n",
      "         1.7999e+00, -2.6821e-01,  2.7658e-01, -7.3768e-01, -2.7505e-02,\n",
      "         4.8893e-01,  1.1687e+00,  7.9646e-01,  3.9564e-01], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_name', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'bias', 'buffers', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'half', 'in_features', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'out_features', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'requires_grad_', 'reset_parameters', 'share_memory', 'state_dict', 'to', 'train', 'training', 'type', 'weight', 'zero_grad']\\n<bound method Module.type of Linear(in_features=384, out_features=16, bias=False)>\\n384\\n16\\ntorch.Size([16, 384])\\ntorch.Size([384])\\n['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_name', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'buffers', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'embedding_dim', 'eval', 'extra_repr', 'float', 'forward', 'from_pretrained', 'half', 'load_state_dict', 'max_norm', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'norm_type', 'num_embeddings', 'padding_idx', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'requires_grad_', 'reset_parameters', 'scale_grad_by_freq', 'share_memory', 'sparse', 'state_dict', 'to', 'train', 'training', 'type', 'weight', 'zero_grad']\\n['T', '__abs__', '__add__', '__and__', '__array__', '__array_priority__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', '__cuda_array_interface__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__idiv__', '__ifloordiv__', '__ilshift__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__long__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rfloordiv__', '__rmul__', '__rpow__', '__rshift__', '__rsub__', '__rtruediv__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__torch_function__', '__truediv__', '__weakref__', '__xor__', '_backward_hooks', '_base', '_cdata', '_coalesced_', '_dimI', '_dimV', '_grad', '_grad_fn', '_indices', '_is_view', '_make_subclass', '_nnz', '_update_names', '_values', '_version', 'abs', 'abs_', 'absolute', 'absolute_', 'acos', 'acos_', 'acosh', 'acosh_', 'add', 'add_', 'addbmm', 'addbmm_', 'addcdiv', 'addcdiv_', 'addcmul', 'addcmul_', 'addmm', 'addmm_', 'addmv', 'addmv_', 'addr', 'addr_', 'align_as', 'align_to', 'all', 'allclose', 'amax', 'amin', 'angle', 'any', 'apply_', 'arccos', 'arccos_', 'arccosh', 'arccosh_', 'arcsin', 'arcsin_', 'arcsinh', 'arcsinh_', 'arctan', 'arctan_', 'arctanh', 'arctanh_', 'argmax', 'argmin', 'argsort', 'as_strided', 'as_strided_', 'as_subclass', 'asin', 'asin_', 'asinh', 'asinh_', 'atan', 'atan2', 'atan2_', 'atan_', 'atanh', 'atanh_', 'backward', 'baddbmm', 'baddbmm_', 'bernoulli', 'bernoulli_', 'bfloat16', 'bincount', 'bitwise_and', 'bitwise_and_', 'bitwise_not', 'bitwise_not_', 'bitwise_or', 'bitwise_or_', 'bitwise_xor', 'bitwise_xor_', 'bmm', 'bool', 'byte', 'cauchy_', 'ceil', 'ceil_', 'char', 'cholesky', 'cholesky_inverse', 'cholesky_solve', 'chunk', 'clamp', 'clamp_', 'clamp_max', 'clamp_max_', 'clamp_min', 'clamp_min_', 'clip', 'clip_', 'clone', 'coalesce', 'conj', 'contiguous', 'copy_', 'cos', 'cos_', 'cosh', 'cosh_', 'count_nonzero', 'cpu', 'cross', 'cuda', 'cummax', 'cummin', 'cumprod', 'cumsum', 'data', 'data_ptr', 'deg2rad', 'deg2rad_', 'dense_dim', 'dequantize', 'det', 'detach', 'detach_', 'device', 'diag', 'diag_embed', 'diagflat', 'diagonal', 'digamma', 'digamma_', 'dim', 'dist', 'div', 'div_', 'divide', 'divide_', 'dot', 'double', 'dtype', 'eig', 'element_size', 'eq', 'eq_', 'equal', 'erf', 'erf_', 'erfc', 'erfc_', 'erfinv', 'erfinv_', 'exp', 'exp2', 'exp2_', 'exp_', 'expand', 'expand_as', 'expm1', 'expm1_', 'exponential_', 'fft', 'fill_', 'fill_diagonal_', 'fix', 'fix_', 'flatten', 'flip', 'fliplr', 'flipud', 'float', 'floor', 'floor_', 'floor_divide', 'floor_divide_', 'fmod', 'fmod_', 'frac', 'frac_', 'gather', 'gcd', 'gcd_', 'ge', 'ge_', 'geometric_', 'geqrf', 'ger', 'get_device', 'grad', 'grad_fn', 'greater', 'greater_', 'greater_equal', 'greater_equal_', 'gt', 'gt_', 'half', 'hardshrink', 'has_names', 'heaviside', 'heaviside_', 'histc', 'hypot', 'hypot_', 'i0', 'i0_', 'ifft', 'imag', 'index_add', 'index_add_', 'index_copy', 'index_copy_', 'index_fill', 'index_fill_', 'index_put', 'index_put_', 'index_select', 'indices', 'int', 'int_repr', 'inverse', 'irfft', 'is_coalesced', 'is_complex', 'is_contiguous', 'is_cuda', 'is_distributed', 'is_floating_point', 'is_leaf', 'is_meta', 'is_mkldnn', 'is_nonzero', 'is_pinned', 'is_quantized', 'is_same_size', 'is_set_to', 'is_shared', 'is_signed', 'is_sparse', 'isclose', 'isfinite', 'isinf', 'isnan', 'isneginf', 'isposinf', 'isreal', 'istft', 'item', 'kthvalue', 'layout', 'lcm', 'lcm_', 'le', 'le_', 'lerp', 'lerp_', 'less', 'less_', 'less_equal', 'less_equal_', 'lgamma', 'lgamma_', 'log', 'log10', 'log10_', 'log1p', 'log1p_', 'log2', 'log2_', 'log_', 'log_normal_', 'log_softmax', 'logaddexp', 'logaddexp2', 'logcumsumexp', 'logdet', 'logical_and', 'logical_and_', 'logical_not', 'logical_not_', 'logical_or', 'logical_or_', 'logical_xor', 'logical_xor_', 'logit', 'logit_', 'logsumexp', 'long', 'lstsq', 'lt', 'lt_', 'lu', 'lu_solve', 'map2_', 'map_', 'masked_fill', 'masked_fill_', 'masked_scatter', 'masked_scatter_', 'masked_select', 'matmul', 'matrix_exp', 'matrix_power', 'max', 'maximum', 'mean', 'median', 'min', 'minimum', 'mm', 'mode', 'movedim', 'mul', 'mul_', 'multinomial', 'multiply', 'multiply_', 'mv', 'mvlgamma', 'mvlgamma_', 'name', 'names', 'nanquantile', 'nansum', 'narrow', 'narrow_copy', 'ndim', 'ndimension', 'ne', 'ne_', 'neg', 'neg_', 'negative', 'negative_', 'nelement', 'new', 'new_empty', 'new_full', 'new_ones', 'new_tensor', 'new_zeros', 'nextafter', 'nextafter_', 'nonzero', 'norm', 'normal_', 'not_equal', 'not_equal_', 'numel', 'numpy', 'orgqr', 'ormqr', 'outer', 'output_nr', 'permute', 'pin_memory', 'pinverse', 'polygamma', 'polygamma_', 'pow', 'pow_', 'prelu', 'prod', 'put_', 'q_per_channel_axis', 'q_per_channel_scales', 'q_per_channel_zero_points', 'q_scale', 'q_zero_point', 'qr', 'qscheme', 'quantile', 'rad2deg', 'rad2deg_', 'random_', 'real', 'reciprocal', 'reciprocal_', 'record_stream', 'refine_names', 'register_hook', 'reinforce', 'relu', 'relu_', 'remainder', 'remainder_', 'rename', 'rename_', 'renorm', 'renorm_', 'repeat', 'repeat_interleave', 'requires_grad', 'requires_grad_', 'reshape', 'reshape_as', 'resize', 'resize_', 'resize_as', 'resize_as_', 'retain_grad', 'rfft', 'roll', 'rot90', 'round', 'round_', 'rsqrt', 'rsqrt_', 'scatter', 'scatter_', 'scatter_add', 'scatter_add_', 'select', 'set_', 'sgn', 'sgn_', 'shape', 'share_memory_', 'short', 'sigmoid', 'sigmoid_', 'sign', 'sign_', 'signbit', 'sin', 'sin_', 'sinh', 'sinh_', 'size', 'slogdet', 'smm', 'softmax', 'solve', 'sort', 'sparse_dim', 'sparse_mask', 'sparse_resize_', 'sparse_resize_and_clear_', 'split', 'split_with_sizes', 'sqrt', 'sqrt_', 'square', 'square_', 'squeeze', 'squeeze_', 'sspaddmm', 'std', 'stft', 'storage', 'storage_offset', 'storage_type', 'stride', 'sub', 'sub_', 'subtract', 'subtract_', 'sum', 'sum_to_size', 'svd', 'symeig', 't', 't_', 'take', 'tan', 'tan_', 'tanh', 'tanh_', 'to', 'to_dense', 'to_mkldnn', 'to_sparse', 'tolist', 'topk', 'trace', 'transpose', 'transpose_', 'triangular_solve', 'tril', 'tril_', 'triu', 'triu_', 'true_divide', 'true_divide_', 'trunc', 'trunc_', 'type', 'type_as', 'unbind', 'unflatten', 'unfold', 'uniform_', 'unique', 'unique_consecutive', 'unsafe_chunk', 'unsafe_split', 'unsafe_split_with_sizes', 'unsqueeze', 'unsqueeze_', 'values', 'var', 'vdot', 'view', 'view_as', 'where', 'zero_']\\ntorch.Size([97, 384])\\ntensor([-1.3664e-01, -4.0590e-01, -2.2219e-01,  5.3633e-01,  1.3847e+00,\\n         6.0940e-01,  1.0424e+00,  1.1116e+00, -2.6177e-01, -1.1758e-01,\\n        -8.0097e-01,  3.9556e-01,  1.8125e-01,  4.5105e-01,  2.7599e-01,\\n         5.1245e-01,  5.1657e-01,  1.4031e+00, -6.5616e-01,  3.6901e-01,\\n         7.3739e-01, -2.9283e-01, -1.2868e+00, -7.9836e-01, -2.4348e-01,\\n        -4.3813e-01,  3.0676e-01,  8.2756e-01, -1.9864e-01, -4.6908e-01,\\n         4.0065e-01,  2.0083e+00, -5.0702e-01,  7.8803e-01, -9.9201e-01,\\n         2.9955e-01, -1.2661e-01,  6.3483e-02,  5.6666e-01,  5.4232e-01,\\n         9.1668e-01, -1.0347e-01,  1.1669e+00,  1.8649e+00,  5.9467e-02,\\n         8.1702e-01, -4.9200e-01,  8.2695e-01, -9.3656e-01, -7.9871e-01,\\n         1.0738e+00, -4.7494e-01, -7.2439e-01, -2.2799e-01,  3.0416e-01,\\n         1.1063e+00, -4.2677e-01,  6.1318e-01,  1.3603e+00, -1.1408e+00,\\n        -7.3598e-01, -7.5957e-01, -6.9394e-02, -2.5286e-01,  4.9853e-01,\\n         4.6131e-01, -6.1774e-01,  7.9585e-01,  4.2797e-01, -1.7809e-01,\\n        -1.9401e+00, -5.5100e-01,  2.6329e-01,  1.1866e+00, -5.6975e-01,\\n        -1.1178e-01, -5.2068e-02,  1.3399e+00,  1.2692e-01, -1.9369e+00,\\n        -1.5599e-01, -3.0305e-01, -2.8277e+00,  1.5071e-02,  2.3361e+00,\\n        -3.8259e-01, -3.2923e-01, -5.6894e-01,  7.5279e-01, -1.4578e+00,\\n        -1.9438e+00, -2.3516e+00, -4.1028e-02, -3.2798e-02, -5.7765e-01,\\n        -3.2116e-01, -2.5557e-01,  1.0580e-01,  4.3000e-01,  3.2293e-01,\\n         2.7354e-01,  6.9304e-01,  1.8769e+00, -3.9564e-01, -1.3200e+00,\\n         2.2713e-01,  4.8751e-01, -1.3580e+00, -3.8609e-01, -1.0239e+00,\\n        -1.4512e-01, -2.3120e-01,  4.3180e-01, -5.5675e-01, -7.9909e-01,\\n        -7.5038e-01,  8.4322e-01, -3.1666e-01,  3.4654e-01,  7.7588e-01,\\n         3.8141e-01, -2.0563e-01,  3.1469e-01, -8.0654e-01,  4.7067e-01,\\n         1.0281e+00, -4.8603e-01, -2.9031e+00,  4.1955e-01,  1.6954e+00,\\n        -9.4966e-01, -2.0211e-01, -5.6915e-02, -7.5391e-01,  5.8532e-01,\\n         1.3990e+00, -8.8408e-02,  1.9064e-01,  7.5883e-01,  2.4874e-03,\\n        -2.5647e-02,  2.1273e-01,  1.1395e+00, -5.0308e-01,  8.5667e-01,\\n         8.8261e-01,  8.0532e-01, -8.7556e-01, -1.4758e-01, -1.4905e-01,\\n         5.1957e-02, -9.1003e-01,  6.8148e-01, -5.4598e-01,  5.3370e-01,\\n         3.8573e-01,  3.3385e-02,  1.6973e-01, -1.8141e-01, -7.0944e-01,\\n        -1.2175e+00, -1.1221e+00,  1.2512e-01,  7.7090e-01, -5.4690e-01,\\n         9.2581e-01, -1.1107e-01,  7.5230e-01,  1.5444e+00, -1.1679e+00,\\n        -5.8597e-01,  1.0054e+00,  5.6257e-02,  4.0462e-01,  3.9183e-01,\\n         6.4785e-01,  1.2511e-02,  7.7656e-01,  1.0831e+00, -7.0587e-01,\\n        -1.8596e-01, -1.8027e+00, -6.2220e-01,  1.2311e+00, -1.7799e+00,\\n        -1.0808e+00, -6.7453e-01, -1.1141e-01,  8.4733e-02,  1.9948e-02,\\n        -4.0087e-01,  3.3899e-01,  8.1522e-01,  1.5621e+00, -7.0142e-01,\\n        -4.8223e-01, -6.6697e-01, -1.8925e-01,  1.1880e+00, -1.2273e+00,\\n         8.9999e-01,  5.9492e-01,  8.1438e-01,  1.5666e-02, -8.8533e-01,\\n         4.1776e-01, -6.4685e-01,  4.5221e-01,  2.5957e-02,  3.9510e-01,\\n        -8.5163e-01,  8.2882e-01,  9.5060e-01, -4.4255e-01, -9.2770e-01,\\n        -1.5975e+00,  2.9371e-01, -1.2315e+00, -8.7343e-01,  8.9428e-02,\\n         1.7647e-01, -7.4679e-01,  2.8629e+00, -4.5990e-01, -1.6549e-01,\\n         1.1021e+00, -1.6458e+00, -4.1318e-02,  2.1957e+00,  1.1366e+00,\\n         1.5442e-01, -1.3475e+00, -1.0809e+00,  1.2546e+00, -8.4330e-01,\\n         4.5855e-01, -4.8445e-02, -6.4026e-01, -2.1022e+00, -1.0389e-01,\\n        -1.2566e+00,  1.0140e+00,  4.5058e-01, -6.1631e-01, -8.6026e-01,\\n         1.2391e+00, -6.6977e-01,  7.2814e-01,  6.4342e-01, -7.0971e-01,\\n        -9.4501e-01,  7.4483e-01,  2.8113e-01,  1.3425e+00, -1.0013e+00,\\n        -9.8773e-01,  1.6508e+00,  1.0094e+00, -1.3246e+00,  4.7838e-02,\\n        -5.5132e-01,  1.8356e+00,  4.9750e-01,  8.4641e-01, -6.9083e-01,\\n        -4.2841e-01, -4.1163e-01,  6.7305e-01, -7.4259e-01, -3.8063e-01,\\n         1.4060e-01, -1.2704e-01, -4.7451e-01,  2.1874e-01,  1.1581e-01,\\n         4.9308e-01,  1.5480e-01,  4.7753e-01, -1.5080e+00,  1.0588e+00,\\n        -1.9669e+00,  2.1036e+00,  7.2395e-01, -6.3684e-01, -7.8241e-01,\\n         6.4944e-01,  2.8066e-03,  1.3208e+00, -1.5772e+00,  1.0723e+00,\\n        -8.2203e-01,  1.4328e+00, -2.3067e-01,  7.0783e-01, -4.4902e-01,\\n        -1.3776e+00, -4.4486e-01, -6.3538e-01,  2.1044e-01, -1.1066e+00,\\n         1.2276e+00, -1.0044e+00,  7.8033e-01, -1.0372e-01, -7.7795e-01,\\n        -8.6399e-01,  1.1657e+00, -7.7011e-01,  6.8821e-01, -2.3464e+00,\\n        -1.0903e+00,  1.0466e+00, -2.4827e-01,  8.8626e-01, -1.4259e+00,\\n        -1.1011e+00, -5.7526e-01,  8.8903e-01,  1.0949e+00, -8.0354e-01,\\n        -3.5087e-01, -5.7875e-02,  6.7720e-01, -8.5290e-02, -6.5205e-02,\\n        -3.7540e-01, -5.5354e-01, -2.8629e-01,  3.4709e-01, -8.3628e-01,\\n        -1.1885e+00,  1.1538e+00,  2.3160e-01, -1.4859e-01, -1.4890e+00,\\n         5.2647e-01,  6.6733e-01, -8.2302e-01,  3.9545e-01, -2.9798e-01,\\n        -7.5579e-01,  7.2871e-01, -3.2641e-01, -9.1883e-01, -4.0755e-01,\\n         1.5744e-01, -5.8806e-02, -6.9044e-01,  4.2294e-01, -1.4729e+00,\\n        -4.8012e-01, -1.0481e+00, -4.4776e-01, -2.7175e-01,  2.2290e-02,\\n        -2.1795e+00,  1.0271e+00, -7.6579e-01,  3.9064e-01,  7.7011e-01,\\n        -6.5713e-01,  2.2870e-01,  6.1872e-01, -2.0098e-01,  2.5962e-02,\\n        -1.5894e+00, -8.6020e-01, -1.8814e+00, -7.1663e-02,  9.1778e-01,\\n         9.2523e-02, -1.2062e+00, -1.1386e+00, -8.6978e-01,  1.0440e+00,\\n        -2.7735e+00,  1.2994e+00,  1.2769e+00, -2.4975e-01, -1.2188e+00,\\n        -5.0826e-01,  8.4310e-01,  8.4368e-01, -7.9602e-02], device='cuda:0',\\n       grad_fn=<SelectBackward>)\\n       \\ntensor([ 2.1269e+00, -2.0395e+00,  2.5961e-01,  9.5541e-02,  9.1339e-01,\\n       \\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dir(m.blocks[0].multi_head_attention.heads[0].key))\n",
    "print(m.blocks[0].multi_head_attention.heads[0].key.type)\n",
    "print(m.blocks[0].multi_head_attention.heads[0].key.in_features)\n",
    "print(m.blocks[0].multi_head_attention.heads[0].key.out_features)\n",
    "print(m.blocks[0].multi_head_attention.heads[0].key.weight.shape)\n",
    "print(m.blocks[0].multi_head_attention.heads[0].key.weight[0].shape)\n",
    "#print(m.blocks[0].multi_head_attention.heads[0].key.weight[0])\n",
    "print(dir(m.token_into_embedding))\n",
    "print(dir(m.token_into_embedding.weight))\n",
    "print(m.token_into_embedding.weight.shape)\n",
    "print(m.token_into_embedding.weight[96])\n",
    "\n",
    "#Input: (∗), IntTensor or LongTensor of arbitrary shape containing the indices to extract\n",
    "\n",
    "#Output: (∗,H), where * is the input shape and =embedding_dim H=embedding_dim\n",
    "\n",
    "'''\n",
    "['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_name', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'bias', 'buffers', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'half', 'in_features', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'out_features', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'requires_grad_', 'reset_parameters', 'share_memory', 'state_dict', 'to', 'train', 'training', 'type', 'weight', 'zero_grad']\n",
    "<bound method Module.type of Linear(in_features=384, out_features=16, bias=False)>\n",
    "384\n",
    "16\n",
    "torch.Size([16, 384])\n",
    "torch.Size([384])\n",
    "['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_name', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'buffers', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'embedding_dim', 'eval', 'extra_repr', 'float', 'forward', 'from_pretrained', 'half', 'load_state_dict', 'max_norm', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'norm_type', 'num_embeddings', 'padding_idx', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'requires_grad_', 'reset_parameters', 'scale_grad_by_freq', 'share_memory', 'sparse', 'state_dict', 'to', 'train', 'training', 'type', 'weight', 'zero_grad']\n",
    "['T', '__abs__', '__add__', '__and__', '__array__', '__array_priority__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', '__cuda_array_interface__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__idiv__', '__ifloordiv__', '__ilshift__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__long__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rfloordiv__', '__rmul__', '__rpow__', '__rshift__', '__rsub__', '__rtruediv__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__torch_function__', '__truediv__', '__weakref__', '__xor__', '_backward_hooks', '_base', '_cdata', '_coalesced_', '_dimI', '_dimV', '_grad', '_grad_fn', '_indices', '_is_view', '_make_subclass', '_nnz', '_update_names', '_values', '_version', 'abs', 'abs_', 'absolute', 'absolute_', 'acos', 'acos_', 'acosh', 'acosh_', 'add', 'add_', 'addbmm', 'addbmm_', 'addcdiv', 'addcdiv_', 'addcmul', 'addcmul_', 'addmm', 'addmm_', 'addmv', 'addmv_', 'addr', 'addr_', 'align_as', 'align_to', 'all', 'allclose', 'amax', 'amin', 'angle', 'any', 'apply_', 'arccos', 'arccos_', 'arccosh', 'arccosh_', 'arcsin', 'arcsin_', 'arcsinh', 'arcsinh_', 'arctan', 'arctan_', 'arctanh', 'arctanh_', 'argmax', 'argmin', 'argsort', 'as_strided', 'as_strided_', 'as_subclass', 'asin', 'asin_', 'asinh', 'asinh_', 'atan', 'atan2', 'atan2_', 'atan_', 'atanh', 'atanh_', 'backward', 'baddbmm', 'baddbmm_', 'bernoulli', 'bernoulli_', 'bfloat16', 'bincount', 'bitwise_and', 'bitwise_and_', 'bitwise_not', 'bitwise_not_', 'bitwise_or', 'bitwise_or_', 'bitwise_xor', 'bitwise_xor_', 'bmm', 'bool', 'byte', 'cauchy_', 'ceil', 'ceil_', 'char', 'cholesky', 'cholesky_inverse', 'cholesky_solve', 'chunk', 'clamp', 'clamp_', 'clamp_max', 'clamp_max_', 'clamp_min', 'clamp_min_', 'clip', 'clip_', 'clone', 'coalesce', 'conj', 'contiguous', 'copy_', 'cos', 'cos_', 'cosh', 'cosh_', 'count_nonzero', 'cpu', 'cross', 'cuda', 'cummax', 'cummin', 'cumprod', 'cumsum', 'data', 'data_ptr', 'deg2rad', 'deg2rad_', 'dense_dim', 'dequantize', 'det', 'detach', 'detach_', 'device', 'diag', 'diag_embed', 'diagflat', 'diagonal', 'digamma', 'digamma_', 'dim', 'dist', 'div', 'div_', 'divide', 'divide_', 'dot', 'double', 'dtype', 'eig', 'element_size', 'eq', 'eq_', 'equal', 'erf', 'erf_', 'erfc', 'erfc_', 'erfinv', 'erfinv_', 'exp', 'exp2', 'exp2_', 'exp_', 'expand', 'expand_as', 'expm1', 'expm1_', 'exponential_', 'fft', 'fill_', 'fill_diagonal_', 'fix', 'fix_', 'flatten', 'flip', 'fliplr', 'flipud', 'float', 'floor', 'floor_', 'floor_divide', 'floor_divide_', 'fmod', 'fmod_', 'frac', 'frac_', 'gather', 'gcd', 'gcd_', 'ge', 'ge_', 'geometric_', 'geqrf', 'ger', 'get_device', 'grad', 'grad_fn', 'greater', 'greater_', 'greater_equal', 'greater_equal_', 'gt', 'gt_', 'half', 'hardshrink', 'has_names', 'heaviside', 'heaviside_', 'histc', 'hypot', 'hypot_', 'i0', 'i0_', 'ifft', 'imag', 'index_add', 'index_add_', 'index_copy', 'index_copy_', 'index_fill', 'index_fill_', 'index_put', 'index_put_', 'index_select', 'indices', 'int', 'int_repr', 'inverse', 'irfft', 'is_coalesced', 'is_complex', 'is_contiguous', 'is_cuda', 'is_distributed', 'is_floating_point', 'is_leaf', 'is_meta', 'is_mkldnn', 'is_nonzero', 'is_pinned', 'is_quantized', 'is_same_size', 'is_set_to', 'is_shared', 'is_signed', 'is_sparse', 'isclose', 'isfinite', 'isinf', 'isnan', 'isneginf', 'isposinf', 'isreal', 'istft', 'item', 'kthvalue', 'layout', 'lcm', 'lcm_', 'le', 'le_', 'lerp', 'lerp_', 'less', 'less_', 'less_equal', 'less_equal_', 'lgamma', 'lgamma_', 'log', 'log10', 'log10_', 'log1p', 'log1p_', 'log2', 'log2_', 'log_', 'log_normal_', 'log_softmax', 'logaddexp', 'logaddexp2', 'logcumsumexp', 'logdet', 'logical_and', 'logical_and_', 'logical_not', 'logical_not_', 'logical_or', 'logical_or_', 'logical_xor', 'logical_xor_', 'logit', 'logit_', 'logsumexp', 'long', 'lstsq', 'lt', 'lt_', 'lu', 'lu_solve', 'map2_', 'map_', 'masked_fill', 'masked_fill_', 'masked_scatter', 'masked_scatter_', 'masked_select', 'matmul', 'matrix_exp', 'matrix_power', 'max', 'maximum', 'mean', 'median', 'min', 'minimum', 'mm', 'mode', 'movedim', 'mul', 'mul_', 'multinomial', 'multiply', 'multiply_', 'mv', 'mvlgamma', 'mvlgamma_', 'name', 'names', 'nanquantile', 'nansum', 'narrow', 'narrow_copy', 'ndim', 'ndimension', 'ne', 'ne_', 'neg', 'neg_', 'negative', 'negative_', 'nelement', 'new', 'new_empty', 'new_full', 'new_ones', 'new_tensor', 'new_zeros', 'nextafter', 'nextafter_', 'nonzero', 'norm', 'normal_', 'not_equal', 'not_equal_', 'numel', 'numpy', 'orgqr', 'ormqr', 'outer', 'output_nr', 'permute', 'pin_memory', 'pinverse', 'polygamma', 'polygamma_', 'pow', 'pow_', 'prelu', 'prod', 'put_', 'q_per_channel_axis', 'q_per_channel_scales', 'q_per_channel_zero_points', 'q_scale', 'q_zero_point', 'qr', 'qscheme', 'quantile', 'rad2deg', 'rad2deg_', 'random_', 'real', 'reciprocal', 'reciprocal_', 'record_stream', 'refine_names', 'register_hook', 'reinforce', 'relu', 'relu_', 'remainder', 'remainder_', 'rename', 'rename_', 'renorm', 'renorm_', 'repeat', 'repeat_interleave', 'requires_grad', 'requires_grad_', 'reshape', 'reshape_as', 'resize', 'resize_', 'resize_as', 'resize_as_', 'retain_grad', 'rfft', 'roll', 'rot90', 'round', 'round_', 'rsqrt', 'rsqrt_', 'scatter', 'scatter_', 'scatter_add', 'scatter_add_', 'select', 'set_', 'sgn', 'sgn_', 'shape', 'share_memory_', 'short', 'sigmoid', 'sigmoid_', 'sign', 'sign_', 'signbit', 'sin', 'sin_', 'sinh', 'sinh_', 'size', 'slogdet', 'smm', 'softmax', 'solve', 'sort', 'sparse_dim', 'sparse_mask', 'sparse_resize_', 'sparse_resize_and_clear_', 'split', 'split_with_sizes', 'sqrt', 'sqrt_', 'square', 'square_', 'squeeze', 'squeeze_', 'sspaddmm', 'std', 'stft', 'storage', 'storage_offset', 'storage_type', 'stride', 'sub', 'sub_', 'subtract', 'subtract_', 'sum', 'sum_to_size', 'svd', 'symeig', 't', 't_', 'take', 'tan', 'tan_', 'tanh', 'tanh_', 'to', 'to_dense', 'to_mkldnn', 'to_sparse', 'tolist', 'topk', 'trace', 'transpose', 'transpose_', 'triangular_solve', 'tril', 'tril_', 'triu', 'triu_', 'true_divide', 'true_divide_', 'trunc', 'trunc_', 'type', 'type_as', 'unbind', 'unflatten', 'unfold', 'uniform_', 'unique', 'unique_consecutive', 'unsafe_chunk', 'unsafe_split', 'unsafe_split_with_sizes', 'unsqueeze', 'unsqueeze_', 'values', 'var', 'vdot', 'view', 'view_as', 'where', 'zero_']\n",
    "torch.Size([97, 384])\n",
    "tensor([-1.3664e-01, -4.0590e-01, -2.2219e-01,  5.3633e-01,  1.3847e+00,\n",
    "         6.0940e-01,  1.0424e+00,  1.1116e+00, -2.6177e-01, -1.1758e-01,\n",
    "        -8.0097e-01,  3.9556e-01,  1.8125e-01,  4.5105e-01,  2.7599e-01,\n",
    "         5.1245e-01,  5.1657e-01,  1.4031e+00, -6.5616e-01,  3.6901e-01,\n",
    "         7.3739e-01, -2.9283e-01, -1.2868e+00, -7.9836e-01, -2.4348e-01,\n",
    "        -4.3813e-01,  3.0676e-01,  8.2756e-01, -1.9864e-01, -4.6908e-01,\n",
    "         4.0065e-01,  2.0083e+00, -5.0702e-01,  7.8803e-01, -9.9201e-01,\n",
    "         2.9955e-01, -1.2661e-01,  6.3483e-02,  5.6666e-01,  5.4232e-01,\n",
    "         9.1668e-01, -1.0347e-01,  1.1669e+00,  1.8649e+00,  5.9467e-02,\n",
    "         8.1702e-01, -4.9200e-01,  8.2695e-01, -9.3656e-01, -7.9871e-01,\n",
    "         1.0738e+00, -4.7494e-01, -7.2439e-01, -2.2799e-01,  3.0416e-01,\n",
    "         1.1063e+00, -4.2677e-01,  6.1318e-01,  1.3603e+00, -1.1408e+00,\n",
    "        -7.3598e-01, -7.5957e-01, -6.9394e-02, -2.5286e-01,  4.9853e-01,\n",
    "         4.6131e-01, -6.1774e-01,  7.9585e-01,  4.2797e-01, -1.7809e-01,\n",
    "        -1.9401e+00, -5.5100e-01,  2.6329e-01,  1.1866e+00, -5.6975e-01,\n",
    "        -1.1178e-01, -5.2068e-02,  1.3399e+00,  1.2692e-01, -1.9369e+00,\n",
    "        -1.5599e-01, -3.0305e-01, -2.8277e+00,  1.5071e-02,  2.3361e+00,\n",
    "        -3.8259e-01, -3.2923e-01, -5.6894e-01,  7.5279e-01, -1.4578e+00,\n",
    "        -1.9438e+00, -2.3516e+00, -4.1028e-02, -3.2798e-02, -5.7765e-01,\n",
    "        -3.2116e-01, -2.5557e-01,  1.0580e-01,  4.3000e-01,  3.2293e-01,\n",
    "         2.7354e-01,  6.9304e-01,  1.8769e+00, -3.9564e-01, -1.3200e+00,\n",
    "         2.2713e-01,  4.8751e-01, -1.3580e+00, -3.8609e-01, -1.0239e+00,\n",
    "        -1.4512e-01, -2.3120e-01,  4.3180e-01, -5.5675e-01, -7.9909e-01,\n",
    "        -7.5038e-01,  8.4322e-01, -3.1666e-01,  3.4654e-01,  7.7588e-01,\n",
    "         3.8141e-01, -2.0563e-01,  3.1469e-01, -8.0654e-01,  4.7067e-01,\n",
    "         1.0281e+00, -4.8603e-01, -2.9031e+00,  4.1955e-01,  1.6954e+00,\n",
    "        -9.4966e-01, -2.0211e-01, -5.6915e-02, -7.5391e-01,  5.8532e-01,\n",
    "         1.3990e+00, -8.8408e-02,  1.9064e-01,  7.5883e-01,  2.4874e-03,\n",
    "        -2.5647e-02,  2.1273e-01,  1.1395e+00, -5.0308e-01,  8.5667e-01,\n",
    "         8.8261e-01,  8.0532e-01, -8.7556e-01, -1.4758e-01, -1.4905e-01,\n",
    "         5.1957e-02, -9.1003e-01,  6.8148e-01, -5.4598e-01,  5.3370e-01,\n",
    "         3.8573e-01,  3.3385e-02,  1.6973e-01, -1.8141e-01, -7.0944e-01,\n",
    "        -1.2175e+00, -1.1221e+00,  1.2512e-01,  7.7090e-01, -5.4690e-01,\n",
    "         9.2581e-01, -1.1107e-01,  7.5230e-01,  1.5444e+00, -1.1679e+00,\n",
    "        -5.8597e-01,  1.0054e+00,  5.6257e-02,  4.0462e-01,  3.9183e-01,\n",
    "         6.4785e-01,  1.2511e-02,  7.7656e-01,  1.0831e+00, -7.0587e-01,\n",
    "        -1.8596e-01, -1.8027e+00, -6.2220e-01,  1.2311e+00, -1.7799e+00,\n",
    "        -1.0808e+00, -6.7453e-01, -1.1141e-01,  8.4733e-02,  1.9948e-02,\n",
    "        -4.0087e-01,  3.3899e-01,  8.1522e-01,  1.5621e+00, -7.0142e-01,\n",
    "        -4.8223e-01, -6.6697e-01, -1.8925e-01,  1.1880e+00, -1.2273e+00,\n",
    "         8.9999e-01,  5.9492e-01,  8.1438e-01,  1.5666e-02, -8.8533e-01,\n",
    "         4.1776e-01, -6.4685e-01,  4.5221e-01,  2.5957e-02,  3.9510e-01,\n",
    "        -8.5163e-01,  8.2882e-01,  9.5060e-01, -4.4255e-01, -9.2770e-01,\n",
    "        -1.5975e+00,  2.9371e-01, -1.2315e+00, -8.7343e-01,  8.9428e-02,\n",
    "         1.7647e-01, -7.4679e-01,  2.8629e+00, -4.5990e-01, -1.6549e-01,\n",
    "         1.1021e+00, -1.6458e+00, -4.1318e-02,  2.1957e+00,  1.1366e+00,\n",
    "         1.5442e-01, -1.3475e+00, -1.0809e+00,  1.2546e+00, -8.4330e-01,\n",
    "         4.5855e-01, -4.8445e-02, -6.4026e-01, -2.1022e+00, -1.0389e-01,\n",
    "        -1.2566e+00,  1.0140e+00,  4.5058e-01, -6.1631e-01, -8.6026e-01,\n",
    "         1.2391e+00, -6.6977e-01,  7.2814e-01,  6.4342e-01, -7.0971e-01,\n",
    "        -9.4501e-01,  7.4483e-01,  2.8113e-01,  1.3425e+00, -1.0013e+00,\n",
    "        -9.8773e-01,  1.6508e+00,  1.0094e+00, -1.3246e+00,  4.7838e-02,\n",
    "        -5.5132e-01,  1.8356e+00,  4.9750e-01,  8.4641e-01, -6.9083e-01,\n",
    "        -4.2841e-01, -4.1163e-01,  6.7305e-01, -7.4259e-01, -3.8063e-01,\n",
    "         1.4060e-01, -1.2704e-01, -4.7451e-01,  2.1874e-01,  1.1581e-01,\n",
    "         4.9308e-01,  1.5480e-01,  4.7753e-01, -1.5080e+00,  1.0588e+00,\n",
    "        -1.9669e+00,  2.1036e+00,  7.2395e-01, -6.3684e-01, -7.8241e-01,\n",
    "         6.4944e-01,  2.8066e-03,  1.3208e+00, -1.5772e+00,  1.0723e+00,\n",
    "        -8.2203e-01,  1.4328e+00, -2.3067e-01,  7.0783e-01, -4.4902e-01,\n",
    "        -1.3776e+00, -4.4486e-01, -6.3538e-01,  2.1044e-01, -1.1066e+00,\n",
    "         1.2276e+00, -1.0044e+00,  7.8033e-01, -1.0372e-01, -7.7795e-01,\n",
    "        -8.6399e-01,  1.1657e+00, -7.7011e-01,  6.8821e-01, -2.3464e+00,\n",
    "        -1.0903e+00,  1.0466e+00, -2.4827e-01,  8.8626e-01, -1.4259e+00,\n",
    "        -1.1011e+00, -5.7526e-01,  8.8903e-01,  1.0949e+00, -8.0354e-01,\n",
    "        -3.5087e-01, -5.7875e-02,  6.7720e-01, -8.5290e-02, -6.5205e-02,\n",
    "        -3.7540e-01, -5.5354e-01, -2.8629e-01,  3.4709e-01, -8.3628e-01,\n",
    "        -1.1885e+00,  1.1538e+00,  2.3160e-01, -1.4859e-01, -1.4890e+00,\n",
    "         5.2647e-01,  6.6733e-01, -8.2302e-01,  3.9545e-01, -2.9798e-01,\n",
    "        -7.5579e-01,  7.2871e-01, -3.2641e-01, -9.1883e-01, -4.0755e-01,\n",
    "         1.5744e-01, -5.8806e-02, -6.9044e-01,  4.2294e-01, -1.4729e+00,\n",
    "        -4.8012e-01, -1.0481e+00, -4.4776e-01, -2.7175e-01,  2.2290e-02,\n",
    "        -2.1795e+00,  1.0271e+00, -7.6579e-01,  3.9064e-01,  7.7011e-01,\n",
    "        -6.5713e-01,  2.2870e-01,  6.1872e-01, -2.0098e-01,  2.5962e-02,\n",
    "        -1.5894e+00, -8.6020e-01, -1.8814e+00, -7.1663e-02,  9.1778e-01,\n",
    "         9.2523e-02, -1.2062e+00, -1.1386e+00, -8.6978e-01,  1.0440e+00,\n",
    "        -2.7735e+00,  1.2994e+00,  1.2769e+00, -2.4975e-01, -1.2188e+00,\n",
    "        -5.0826e-01,  8.4310e-01,  8.4368e-01, -7.9602e-02], device='cuda:0',\n",
    "       grad_fn=<SelectBackward>)\n",
    "       \n",
    "tensor([ 2.1269e+00, -2.0395e+00,  2.5961e-01,  9.5541e-02,  9.1339e-01,\n",
    "\n",
    "with 7 habits\n",
    "tensor([ 2.1269e+00, -2.0395e+00,  2.5961e-01,  9.5541e-02,  9.1339e-01,\n",
    "       \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a83412",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# low 6.689373 M parameters\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 8 # how many independent sequences will we process in parallel?\n",
    "block_size = 512 # what is the maximum input context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embedding = 128 # 32 =  head_size = n_embedding / n_head\n",
    "n_head = 8\n",
    "n_layer = 20\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "#torch.manual_seed(1337)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f6d3e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.982177 M parameters\n",
      "step 0: train loss 0.5041, val loss 1.5775\n",
      "step 500: train loss 0.4960, val loss 1.5948\n",
      "step 1000: train loss 0.4876, val loss 1.6042\n",
      "step 1500: train loss 0.4814, val loss 1.5957\n",
      "step 2000: train loss 0.4724, val loss 1.6051\n",
      "step 2500: train loss 0.4674, val loss 1.5999\n",
      "step 3000: train loss 0.4651, val loss 1.6204\n",
      "step 3500: train loss 0.4581, val loss 1.6069\n",
      "step 4000: train loss 0.4572, val loss 1.6743\n",
      "step 4500: train loss 0.4436, val loss 1.6507\n",
      "step 5000: train loss 0.4426, val loss 1.6450\n",
      "step 5500: train loss 0.4402, val loss 1.6680\n",
      "step 6000: train loss 0.4293, val loss 1.6619\n",
      "step 6500: train loss 0.4234, val loss 1.6557\n",
      "step 7000: train loss 0.4204, val loss 1.7017\n",
      "step 7500: train loss 0.4148, val loss 1.7061\n",
      "step 8000: train loss 0.4148, val loss 1.7196\n",
      "step 8500: train loss 0.4082, val loss 1.6866\n",
      "step 9000: train loss 0.4068, val loss 1.7168\n",
      "step 9500: train loss 0.3944, val loss 1.7250\n",
      "step 10000: train loss 0.3875, val loss 1.7125\n",
      "step 10500: train loss 0.3862, val loss 1.7243\n",
      "step 11000: train loss 0.3783, val loss 1.7316\n",
      "step 11500: train loss 0.3764, val loss 1.7366\n",
      "step 12000: train loss 0.3713, val loss 1.7537\n",
      "step 12500: train loss 0.3685, val loss 1.7805\n",
      "step 13000: train loss 0.3586, val loss 1.7585\n",
      "step 13500: train loss 0.3613, val loss 1.7535\n",
      "step 14000: train loss 0.3540, val loss 1.7720\n",
      "step 14500: train loss 0.3512, val loss 1.7621\n",
      "step 15000: train loss 0.3485, val loss 1.7887\n",
      "step 15500: train loss 0.3434, val loss 1.7918\n",
      "step 16000: train loss 0.3415, val loss 1.8070\n",
      "step 16500: train loss 0.3310, val loss 1.7918\n",
      "step 17000: train loss 0.3361, val loss 1.7842\n",
      "step 17500: train loss 0.3295, val loss 1.7885\n",
      "step 18000: train loss 0.3271, val loss 1.8415\n",
      "step 18500: train loss 0.3208, val loss 1.8318\n",
      "step 19000: train loss 0.3225, val loss 1.8136\n",
      "step 19500: train loss 0.3142, val loss 1.8395\n",
      "step 19999: train loss 0.3166, val loss 1.8423\n",
      "step 20000: train loss 0.3151, val loss 1.8687\n",
      "step 20500: train loss 0.3071, val loss 1.8498\n",
      "step 21000: train loss 0.3076, val loss 1.8665\n",
      "step 21500: train loss 0.3038, val loss 1.8683\n",
      "step 22000: train loss 0.3032, val loss 1.8600\n",
      "step 22500: train loss 0.2923, val loss 1.8663\n",
      "step 23000: train loss 0.2890, val loss 1.8898\n",
      "step 23500: train loss 0.2891, val loss 1.8839\n",
      "step 24000: train loss 0.2869, val loss 1.9214\n",
      "step 24500: train loss 0.2852, val loss 1.9079\n",
      "step 25000: train loss 0.2820, val loss 1.9279\n",
      "step 25500: train loss 0.2752, val loss 1.9074\n",
      "step 26000: train loss 0.2767, val loss 1.9066\n",
      "step 26500: train loss 0.2715, val loss 1.9458\n",
      "step 27000: train loss 0.2743, val loss 1.9186\n",
      "step 27500: train loss 0.2653, val loss 1.9219\n",
      "step 28000: train loss 0.2690, val loss 1.9540\n",
      "step 28500: train loss 0.2612, val loss 1.9703\n",
      "step 29000: train loss 0.2618, val loss 1.9753\n",
      "step 29500: train loss 0.2634, val loss 1.9569\n",
      "step 30000: train loss 0.2574, val loss 1.9697\n",
      "step 30500: train loss 0.2549, val loss 1.9692\n",
      "step 31000: train loss 0.2520, val loss 1.9604\n",
      "step 31500: train loss 0.2473, val loss 2.0035\n",
      "step 32000: train loss 0.2467, val loss 1.9741\n",
      "step 32500: train loss 0.2466, val loss 2.0079\n",
      "step 33000: train loss 0.2462, val loss 1.9983\n",
      "step 33500: train loss 0.2431, val loss 2.0193\n",
      "step 34000: train loss 0.2431, val loss 2.0063\n",
      "step 34500: train loss 0.2353, val loss 2.0010\n",
      "step 35000: train loss 0.2402, val loss 2.0214\n",
      "step 35500: train loss 0.2362, val loss 2.0346\n",
      "step 36000: train loss 0.2292, val loss 2.0338\n",
      "step 36500: train loss 0.2358, val loss 2.0353\n",
      "step 37000: train loss 0.2296, val loss 2.0594\n",
      "step 37500: train loss 0.2268, val loss 2.0607\n",
      "step 38000: train loss 0.2273, val loss 2.0799\n",
      "step 38500: train loss 0.2242, val loss 2.0694\n",
      "step 39000: train loss 0.2251, val loss 2.0608\n",
      "step 39500: train loss 0.2201, val loss 2.0990\n",
      "step 40000: train loss 0.2212, val loss 2.0866\n",
      "step 40500: train loss 0.2189, val loss 2.0537\n",
      "step 41000: train loss 0.2188, val loss 2.0669\n",
      "step 41500: train loss 0.2156, val loss 2.1015\n",
      "step 42000: train loss 0.2123, val loss 2.0927\n",
      "step 42500: train loss 0.2126, val loss 2.0768\n",
      "step 43000: train loss 0.2103, val loss 2.1144\n",
      "step 43500: train loss 0.2089, val loss 2.1298\n",
      "step 44000: train loss 0.2129, val loss 2.0987\n",
      "step 44500: train loss 0.2099, val loss 2.1173\n",
      "step 45000: train loss 0.2071, val loss 2.1530\n",
      "step 45500: train loss 0.2057, val loss 2.1302\n",
      "step 46000: train loss 0.2033, val loss 2.1189\n",
      "step 46500: train loss 0.2036, val loss 2.1602\n",
      "step 47000: train loss 0.1989, val loss 2.1790\n",
      "step 47500: train loss 0.2023, val loss 2.1787\n",
      "step 48000: train loss 0.2011, val loss 2.1429\n",
      "step 48500: train loss 0.1981, val loss 2.1653\n",
      "step 49000: train loss 0.1975, val loss 2.1693\n",
      "step 49500: train loss 0.1957, val loss 2.1767\n",
      "====result_loss====\n",
      "0.38579297065734863\n",
      "\n",
      "What does you have? \n",
      "\n",
      "None. \n",
      "\n",
      "No, it the whole moves your way this performance is a worth a time. You can quit when you just give \n",
      "it to the week three days of the world, it also chooses to be the \n",
      "soft step on the company that will through experience started on the that participle in the \n",
      "relationship was it wwasn't a phis and I had been getting social mirror. \n",
      "\n",
      "4. I have went through the principles of time was the gold where all the money and spend the time \n",
      "in Quadrant II, they don’t know what they’re talking about. Most peoply understand what \n",
      "“poor less is,” he wants to do what is so embarrass of money. Spend it is the love that hat \n",
      "really as falling because the purs a big witch the purnistead opinion is an old foundation for the \n",
      "funiture of the weelf. Your financial planners wisdom of the story of bow. So go to \n",
      "learn to fore you and the middle class ystead. 10 cents an hour. You’re glass out, you notice do \n",
      "that.” One exchanged language it other decided to achieve what is deli\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#torch.manual_seed(1337)\\nB,T,C = 4,8,32\\nx = torch.randn(B,T,C)\\n\\nhead_size = 16\\nkey  = nn.Linear(C,head_size,bias=False)\\nquery = nn.Linear(C,head_size,bias=False)\\nvalue = nn.Linear(C,head_size,bias=False)\\n\\nk = key(x) #(B,T,16)\\nq = query(x) #(B,T,16)\\nv = value(x)\\n\\nwei = q @ k.transpose(-2,-1) # (B,T,16) @ (B,16,T) => (B,T,T)\\ntril = torch.tril(torch.ones(T,T))\\n#wei = torch.zeros((T,T))\\nwei = wei.masked_fill(tril==0,float('-inf'))\\nwei = F.softmax(wei,dim=-1)\\n\\noutput = wei @ v\\noutput.shape\\nwei[0]\\n\\n\\n\\ntorch.softmax(torch.tensor([0.1,-0.2,0.3,-0.2,0.5]),dim=-1)\\ntorch.softmax(torch.tensor([0.1,-0.2,0.3,-0.2,0.5])*8,dim=-1)\\n\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=17e-5)\n",
    "\n",
    "for iter in range(50000):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"====result_loss====\")\n",
    "print(loss.item())\n",
    "\n",
    "#torch.save(m.state_dict(), \"./GPT2_Shakespeare\")\n",
    "#torch.save(m.state_dict(), \"./GPT2_7_habits\")\n",
    "torch.save(m.state_dict(), \"./GPT2_rich_dad_poor_dad_Fine-tuning_with_Custom_Datasets\")\n",
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long,device=device), max_new_tokens=1000)[0].tolist()))\n",
    "\n",
    "'''\n",
    "#torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key  = nn.Linear(C,head_size,bias=False)\n",
    "query = nn.Linear(C,head_size,bias=False)\n",
    "value = nn.Linear(C,head_size,bias=False)\n",
    "\n",
    "k = key(x) #(B,T,16)\n",
    "q = query(x) #(B,T,16)\n",
    "v = value(x)\n",
    "\n",
    "wei = q @ k.transpose(-2,-1) # (B,T,16) @ (B,16,T) => (B,T,T)\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0,float('-inf'))\n",
    "wei = F.softmax(wei,dim=-1)\n",
    "\n",
    "output = wei @ v\n",
    "output.shape\n",
    "wei[0]\n",
    "\n",
    "\n",
    "\n",
    "torch.softmax(torch.tensor([0.1,-0.2,0.3,-0.2,0.5]),dim=-1)\n",
    "torch.softmax(torch.tensor([0.1,-0.2,0.3,-0.2,0.5])*8,dim=-1)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long,device=device), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eecff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"How to get rich fast\"\n",
    "context = torch.tensor( [ encode(text) ], dtype=torch.long, device=device )\n",
    "print(decode(m.generate( context, max_new_tokens=1000 )[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a55d768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to get rich faster, I suggested that the interpretation of what my highest was born and \n",
      "pause the problem. My flow dad I could not have the credit with the bank to buy another house, since he was \n",
      "almost skills. They need $7,000 at the difference between way. \n",
      "\n",
      "The extent of the problem was the 60 years of the three generally thried with his \n",
      "6 country went to enjoy in the 303es. \n",
      "\n",
      "\"Look, there is days always gone?\" I asked myself goals to a start they listening nothing to manage. \n",
      "\n",
      "Rich dad guestors for a pattern. But I was the other and his life went to riding good grades, and they not only to \n",
      "line out. In other words, most people because they would encourage to money to go because they \n",
      "want to love them with the most pain forward. They were created, dealing long \n",
      "his neither words. \n",
      "\n",
      "The wall today, there were antimal panic is most important. Give memore than our \n",
      "moods and the weaknesses of other people. They think simply genuinely \n",
      "happy for the masses being getting some and soon often working\n"
     ]
    }
   ],
   "source": [
    "text = \"How to get rich fast\"\n",
    "context = torch.tensor( [ encode(text) ], dtype=torch.long, device=device )\n",
    "print(decode(m.generate( context, max_new_tokens=1000 )[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2f779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa8d7e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is what world famous speaker and author Anthony Robbins says about Robert’s work. \n",
      "\n",
      "“Robert Kiyosaki’s work in education he’s not payer,” I said with a sly grin. \n",
      "\n",
      "“Look, I guess for you to have to be job, better smarter. Then you are the money, instead of being an \n",
      "little changes that there are no recal today. There are more and the things \n",
      "\n",
      "Power \n",
      "\n",
      "\"Isea, we walk you to think school. You something something kind of the second doing to these \n",
      "committed-the boy.\" \n",
      "\n",
      "\"Id yes?\" \n",
      "\n",
      "\"No, not really. Dad, I don't turn you with me to the schoolists trainked in the happy marriage,\" I say. \n",
      "\"Sound you panice the game off the things first go \n",
      "net into opening. You cannot pretend your standard of listening in your life and to be \n",
      "influenced. \n",
      "\n",
      "When you listen, you have a profession and will be wealthy promoted law. But, as a result, \n",
      "communication is problemed, correctly inspired or over and interpreted in loyalty to old these \n",
      "methods may be limited in purpose and even body the subtial force. It may be very \n",
      "different frequential, areas if your friends would be involved in your mind, in your \n",
      "both the marriage job and the regions of the three goose. \n",
      "\n",
      "My friend was asked him when he explored his working hard all his been a goor had to play it safe. \n",
      "The real estate is too only and torture differently. The boys I worked with me into \n",
      "my heart was weakness, and the course, I think my attention can ready for better them. Besides, my \n",
      "instead of thinking, the world is foound that encountry you and seek first to understand. That's Pale It \n",
      "was before. A great pressure to get by pieces. Although I paid myself turned \n",
      "on from admitMas, and they had been taught to pay it to the \n",
      "principles of cons her. S\n",
      "\n",
      "Pend it give it some ambiguity self. There was no sense to both getting more frundamental \n",
      "than sources of any particular groups of interdependent in their employer. \n",
      "\n",
      "The Constitution worked in the middle of the other person, there are friends who are not a driven \n",
      "because between involved in a principle center. It is also a distatement, a child an employee with a \n",
      "source of irritation, a stumbling block, and wishing it comes to money. Matterion is \n",
      "like going to the store managers of that trust in the subject. \n",
      "\n",
      "The middle class is trying to read with the synergistic conversation. \n",
      "\n",
      "Most people have learned or an loving the real key to investment it. By the appying job, I was like every \n",
      "day to walk me in his office and spending. While he had well hammered look out, and why we read the \n",
      "supermarket house or a small company expenses and self-worth solutions and \n",
      "feelings. The common occasionally have you not the power to experience your problems and \n",
      "concerns. That process are a safer making greater, writer do some script. \n",
      "\n",
      "When you Begin with the End in Mind often determines your days are often a regular basis, \n",
      "which making relationships in the content of independent will, your limage, and would benefit \n",
      "and substane because are and management in each other in \n",
      "parts and aids. \n",
      "\n",
      "There are the change that to our chases for me to out there and methods of influence that \n",
      "matters, they have reflect with some of us hard many. It's suggestionstication, it's my office \n",
      "to method of happy and to inte take the construction of a home with a big problem. \n",
      "\n",
      "I considered the forundation of a year-win being that the man's problem circumstances, and there's form a \n",
      "\"Boy, and that experience.\" \n",
      "\n",
      "I had intrined View Vietnam president that made a president of 80 percent of the population, and \n",
      "straight to addiving the four life-suppoilor tring toward the Indian force-upting getting some and \n",
      "asked them Balance Sheet. \n",
      "\n",
      "His patent of First obst creation, and then you live the seasoning has you not greatered the \n",
      "golden egg. \n",
      "\n",
      "I this norrowing all that time, increased it was to get out of the “Rat Race,” for the other stacket \n",
      "on the “Fast Track”. As I have said, so much people about the source of power, it became other \n",
      "environment. I say that the confession I have way to live. \n",
      "\n",
      "I don’t want to pay the mortgage, the rich almost every poor on in the stock. The \n",
      "problem was that Bank on thinking and the other people never see the way we see other woman. I \n",
      "had a new problem on the one-win particular unisting the office in management is at the office \n",
      "will, beyond unanticipated productive, conability, and positive and huge integrity you and work together \n",
      "to see yourself for structure, and then to be doing something in your boss's \n",
      "Circleof Influence is logical/verb. \n",
      "\n",
      "Clearing the money will solve all you have that income definition of working four things you may make \n",
      "because heroes it create the fear of small. \n",
      "\n",
      "The Centers In the short new, suppose to are win for most people to avoid the funeral win-lose \n",
      "does not knowing the difference between an asset and a liability. \n",
      "\n",
      "Moreover, not major people and the technical, skills and get up and skill, but they can \n",
      "fail to trust them back to disappear. They \n",
      "look at their dreamatoches in perception as a child and they work for them be, a day come and to the \n",
      "government. The next time sheet prove in their home. \n",
      "\n",
      "What is the character? They searching form their motives taxes, and increase, They become defensive, \n",
      "and the seizing of thinking that advented the school. They instead of taking the time to admit \n",
      "payments with their first livejumn, take those who have allow to done I make my offer. I think I \n",
      "know there was a 90 percent who run to work with have your family, and then I sure that they \n",
      "fear of them right and fear and develop their sense of power. Or you are ever destiny \n",
      "that something elsements outside found in the funeral process of written family, compassion, and \n",
      "help you understand the materials, the will be able to truly everything in your life. \n",
      "\n",
      "An important meeti ng when the suburbs, or a yonge employees keep grades up, go to work, people became \n",
      "like buck they want to achieve them. Problems would do not be are interested in getting a different \n",
      "are just to work and get on more money. So they can afford their sons that average form only \n",
      "in real ways. \n",
      "\n",
      "Page 78/114 \n",
      "\n",
      "\n",
      "http : / motsach . info \n",
      "\n",
      "\n",
      "Rich Dad Poor Dad \n",
      "\n",
      "\n",
      "Robert T. Kiyosaki \n",
      "\n",
      "\n",
      "“Choosing our thoughts?” Mike asked, put the different spingle way were the old woman \n",
      "I understand the simple leadership. The script was the Joney and Madericy was a man home of the \n",
      "time Management in the middle of the month. First about and told him we agreed that shown these \n",
      "characters at top foten their mouth. We seem to interpret in some conduction to and help a person \n",
      "groups of people to achieve results. \n",
      "\n",
      "Because seeking to understand how we are going to throw the exercise present of working on the \n",
      "stomes agon, asked them when I was the only received to keep a book, I did not say the \n",
      "promise of the greater fundamental mate, the income increased your spouse, \n",
      "on the context of the weekly board with the employees go up. There things where there eroging \n",
      "\n",
      "\n",
      "\n",
      "THE SEVEN HABHS OF HIGHLY EFFECTIVE PEOPLE \n",
      "\n",
      "\n",
      "Brcu^t toyu tyH^Havt \n",
      "\n",
      "\n",
      "empowers us to create customer or against whicher, the immediate reposed concires \n",
      "that are by the underlying shopping incope is full to creational ambity. There's a bigger house \n",
      "come extended and the sixth tree stack. The course of study too increase it, the tool extent that \n",
      "muture their metaphy, writing and families and insights that are illegal. \n",
      "\n",
      "I wonderful personal stages are courage, as the standard thought counterproductive represents. \n",
      "\n",
      "I have had to get the problem in managing a prioristy skyle of like programs, bringing and continued \n",
      "experiences, there is a transformer where never quate to draw for short of help. \n",
      "\n",
      "Pe seemed at the leaves of experience room, contains heroes for me, and there is also the whole is \n",
      "applicable. There is a great habit of success in the “Forment Georden Process?” The only woman \n",
      "have experienced on their synergy -- how, in really makes their deducity and \n",
      "in working with their may involvement with simply changing their jobs and desire of \n",
      "investment in their groups. It is also begin to realize this to ouch first seeking to understand \n",
      "of each home is uniqueto another person. A swing manager, or a paradigm of attempt. The traditional \n",
      "conscience, it has been a combinanced of developing an effective presentation and the other is \n",
      "problem. \n",
      "\n",
      "This Perception Centeredness. It was a cold saying, \"if only to learn somewhere this golden \n",
      "grades that count.\" In the mids of human interaction, management, and the \"security or independent \n",
      "personal lives, which give meaning to timeleve and to livether integrity and commitments \n",
      "of interdependency and effectiveness te life-one. It's talents, and it's a territ of Principle-Centered, it based on \n",
      "control principles and interpersonal growth. \n",
      "\n",
      "- M arge Public Victory \n",
      "\n",
      "Third, author of Peak Performan \n",
      "\n",
      "The Seven Habits of Higher Egypt and People \n",
      "Communicate rein Habits 4, asked them if every qualner had “give me and those it.\" They won't be alive the \n",
      "stephn -- getting and then from line or when they really listen with it be? What they can look at things it \n",
      "\n",
      "\n",
      "\n",
      "THE SEVEN HABHS OF HIGHLY EFFECTIVE PEOPLE \n",
      "\n",
      "\n",
      "Brwcft to you tyHyHsnt \n",
      "\n",
      "\n",
      "application of \"Begin with them.\" With the beginning and reality to other person as well. \n",
      "\n",
      "Now in other people are by the Creation of it. Accountability is really the other weakness, or \n",
      "acqui instead of the first creation, the territory is a dramatic choice for may were ready to buy \n",
      "also. Mutual funds are the foundation of trust and defits just the first, the family thing underlying creative \n",
      "problems and is what the Choice of actord enation. That third generation do not change, the \n",
      "challes becomes hour constant, and the build of cash, graduates. \n",
      "\n",
      "In the words of accountability, such as many problems began the truthful in the vision you \n",
      "necessary for other ego i n the suburbs. \n",
      "\n",
      "I often each of these encourroged adults to compensate for the line: \"What happens to us go understand \n",
      "how the makes will admit,\" he told him that the highest paid soft parents will go on forever. \n",
      "\n",
      "So He was\n"
     ]
    }
   ],
   "source": [
    "# not working because of the small size of the LLM\n",
    "text = \"This is what world famous speaker and author Anthony Robbins says about Robert’s work. \"\n",
    "context = torch.tensor( [ encode(text) ], dtype=torch.long, device=device )\n",
    "print(decode(m.generate( context, max_new_tokens=10000 )[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b435fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. linear neural network learning the price, w is the price, x is the amount, the loss is the total price\n",
    "what is the GPT learning\n",
    "w is [not known, maybe key, query, value], x is the word chain, the loss is the next char of the word chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff50fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
