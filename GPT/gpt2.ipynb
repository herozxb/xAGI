{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4285e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38f26ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 20000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 24\n",
    "n_layer = 24\n",
    "dropout = 0.2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71e5a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "#with open('code.txt', 'r', encoding='utf-8') as f:\n",
    "#    text = f.read()\n",
    "    \n",
    "with open('rich_dad_poor_dad.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest va\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c2efe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "head_size = n_embd // n_head\n",
    "\n",
    "global_query = nn.Linear(n_embd, head_size, bias=False)\n",
    "global_value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = global_query\n",
    "        self.value = global_value\n",
    "        #self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        #self.value = nn.Linear(n_embd, head_size, bias=False)        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e649dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.664477 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "#model.load_state_dict(torch.load(\"./model_code\"))\n",
    "#model.load_state_dict(torch.load(\"./model_rich\"))\n",
    "\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# 1, 35.664477 M parameters\n",
    "# 2, 42.730077 M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccfa3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6785c618-393e-4358-b668-0924d6944759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.7303, val loss 4.7276\n",
      "Iteration 0 took 62.8014 seconds\n",
      "step 500: train loss 2.1050, val loss 2.1421\n",
      "Iteration 500 took 419.9329 seconds\n",
      "step 1000: train loss 1.4377, val loss 1.5771\n",
      "Iteration 1000 took 794.4735 seconds\n",
      "step 1500: train loss 1.1789, val loss 1.4344\n",
      "Iteration 1500 took 1176.9022 seconds\n",
      "step 2000: train loss 0.9970, val loss 1.4013\n",
      "Iteration 2000 took 1562.0558 seconds\n",
      "step 2500: train loss 0.8500, val loss 1.4090\n",
      "Iteration 2500 took 1943.2764 seconds\n",
      "step 3000: train loss 0.7051, val loss 1.4867\n",
      "Iteration 3000 took 2325.5763 seconds\n",
      "step 3500: train loss 0.5735, val loss 1.5514\n",
      "Iteration 3500 took 2712.5934 seconds\n",
      "step 4000: train loss 0.4482, val loss 1.6840\n",
      "Iteration 4000 took 3122.4039 seconds\n",
      "step 4500: train loss 0.3458, val loss 1.7912\n",
      "Iteration 4500 took 3601.7585 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()  # Record the start time\n",
    "for iter in range(5000):\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        end_time = time.time()  # Record the end time\n",
    "        time_taken = end_time - start_time  # Calculate time taken\n",
    "\n",
    "        print(f\"Iteration {iter} took {time_taken:.4f} seconds\")  # Print the time taken\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fec1796-2866-4333-8563-d7625170085b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstep 0: train loss 4.7373, val loss 4.7395\\nIteration 0 took 65.3062 seconds\\nstep 500: train loss 2.1451, val loss 2.1847\\nIteration 500 took 446.0307 seconds\\nstep 1000: train loss 1.4346, val loss 1.5838\\nIteration 1000 took 830.1166 seconds\\nstep 1500: train loss 1.1743, val loss 1.4413\\nIteration 1500 took 1212.6841 seconds\\nstep 2000: train loss 1.0143, val loss 1.4061\\nIteration 2000 took 1592.8847 seconds\\nstep 2500: train loss 0.8537, val loss 1.4149\\nIteration 2500 took 1973.1572 seconds\\nstep 3000: train loss 0.7194, val loss 1.4786\\nIteration 3000 took 2351.7374 seconds\\nstep 3500: train loss 0.5820, val loss 1.5589\\nIteration 3500 took 2731.0541 seconds\\nstep 4000: train loss 0.4605, val loss 1.6484\\nIteration 4000 took 3110.3526 seconds\\nstep 4500: train loss 0.3581, val loss 1.7627\\nIteration 4500 took 3490.8806 seconds\\nstep 5000: train loss 0.2721, val loss 1.8673\\nIteration 5000 took 3867.1339 seconds\\nstep 5500: train loss 0.1988, val loss 2.0157\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 single Q and V\n",
    "'''\n",
    "step 0: train loss 4.7373, val loss 4.7395\n",
    "Iteration 0 took 65.3062 seconds\n",
    "step 500: train loss 2.1451, val loss 2.1847\n",
    "Iteration 500 took 446.0307 seconds\n",
    "step 1000: train loss 1.4346, val loss 1.5838\n",
    "Iteration 1000 took 830.1166 seconds\n",
    "step 1500: train loss 1.1743, val loss 1.4413\n",
    "Iteration 1500 took 1212.6841 seconds\n",
    "step 2000: train loss 1.0143, val loss 1.4061\n",
    "Iteration 2000 took 1592.8847 seconds\n",
    "step 2500: train loss 0.8537, val loss 1.4149\n",
    "Iteration 2500 took 1973.1572 seconds\n",
    "step 3000: train loss 0.7194, val loss 1.4786\n",
    "Iteration 3000 took 2351.7374 seconds\n",
    "step 3500: train loss 0.5820, val loss 1.5589\n",
    "Iteration 3500 took 2731.0541 seconds\n",
    "step 4000: train loss 0.4605, val loss 1.6484\n",
    "Iteration 4000 took 3110.3526 seconds\n",
    "step 4500: train loss 0.3581, val loss 1.7627\n",
    "Iteration 4500 took 3490.8806 seconds\n",
    "step 5000: train loss 0.2721, val loss 1.8673\n",
    "Iteration 5000 took 3867.1339 seconds\n",
    "step 5500: train loss 0.1988, val loss 2.0157\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ea56bca-b6de-4191-b52d-1600902c01ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstep 0: train loss 4.6681, val loss 4.6663\\nIteration 0 took 63.5645 seconds\\nstep 500: train loss 2.3191, val loss 2.3342\\nIteration 500 took 423.8687 seconds\\nstep 1000: train loss 1.5196, val loss 1.6449\\nIteration 1000 took 805.0172 seconds\\nstep 1500: train loss 1.1968, val loss 1.4544\\nIteration 1500 took 1184.8161 seconds\\nstep 2000: train loss 0.9982, val loss 1.4144\\nIteration 2000 took 1560.8502 seconds\\nstep 2500: train loss 0.8258, val loss 1.4542\\nIteration 2500 took 1939.0441 seconds\\nstep 3000: train loss 0.6541, val loss 1.5132\\nIteration 3000 took 2321.2756 seconds\\nstep 3500: train loss 0.4890, val loss 1.6229\\nIteration 3500 took 2707.2526 seconds\\nstep 4000: train loss 0.3466, val loss 1.7947\\nIteration 4000 took 3094.3183 seconds\\nstep 4500: train loss 0.2485, val loss 1.9147\\nIteration 4500 took 3477.0767 seconds\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2,  multiple Q and V\n",
    "'''\n",
    "step 0: train loss 4.6681, val loss 4.6663\n",
    "Iteration 0 took 63.5645 seconds\n",
    "step 500: train loss 2.3191, val loss 2.3342\n",
    "Iteration 500 took 423.8687 seconds\n",
    "step 1000: train loss 1.5196, val loss 1.6449\n",
    "Iteration 1000 took 805.0172 seconds\n",
    "step 1500: train loss 1.1968, val loss 1.4544\n",
    "Iteration 1500 took 1184.8161 seconds\n",
    "step 2000: train loss 0.9982, val loss 1.4144\n",
    "Iteration 2000 took 1560.8502 seconds\n",
    "step 2500: train loss 0.8258, val loss 1.4542\n",
    "Iteration 2500 took 1939.0441 seconds\n",
    "step 3000: train loss 0.6541, val loss 1.5132\n",
    "Iteration 3000 took 2321.2756 seconds\n",
    "step 3500: train loss 0.4890, val loss 1.6229\n",
    "Iteration 3500 took 2707.2526 seconds\n",
    "step 4000: train loss 0.3466, val loss 1.7947\n",
    "Iteration 4000 took 3094.3183 seconds\n",
    "step 4500: train loss 0.2485, val loss 1.9147\n",
    "Iteration 4500 took 3477.0767 seconds\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a57e1828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The truth about what?” I asked. \n",
      "\n",
      "our financial cople earn that something reasons, but the fear of doing the ight themselves of \n",
      "business or ctable sheetelves a broke, and there in bent out people from of foten the door, because \n",
      "a\n",
      "Iteration 4999 took 77.5299 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  # Record the start time\n",
    "\n",
    "text = \"“The truth about what?” I asked.\"\n",
    "context = torch.tensor([encode(text)], dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))\n",
    "\n",
    "end_time = time.time()  # Record the end time\n",
    "time_taken = end_time - start_time  # Calculate time taken\n",
    "\n",
    "print(f\"Iteration {iter} took {time_taken:.4f} seconds\")  # Print the time taken, \n",
    "# 1. took 33.3500 seconds\n",
    "# 2. 33.3966 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6763ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(m.state_dict(), \"./model_code\")\n",
    "torch.save(m.state_dict(), \"./model_rich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "750401b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I get rich quicked and with anger and head of the counntry. Soons after they are \n",
      "leader, they better again, and interesting areallyzerge. And the merize was laws shying. \n",
      "\n",
      "Many empired and Alerge was he got bring sets column. The sats are the cources even \n",
      "when it wages mostly are estated to being good doors exciting from greatenters, my most wife parents \n",
      "are goup because the money and investment. They car not tellicy the poor and \n",
      "middle class tente and me. \n",
      "\n",
      "Our will heaving dreams over greater deals on financial intellligence how thinking would geney. His \n",
      "focused it benefits by angring patcher was dear. The other hander was need to both \n",
      "and the veroxing potent. \n",
      "\n",
      "I looked to understand why I love worked for the road table. \n",
      "\n",
      "1. \n",
      "\n",
      "No 4. I Even se a hourt again it was designed and a pension. Remember Badly was able to \n",
      "tea financial expense. It took his promotion and member when it was a rich man and just for \n",
      "the Teconomic of Education conninerding ’being an intellectual, ’Keep your expens\n",
      "Iteration 4999 took 371.7012 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  # Record the start time\n",
    "\n",
    "text = \"How do I get rich\"\n",
    "context = torch.tensor([encode(text)], dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))\n",
    "\n",
    "end_time = time.time()  # Record the end time\n",
    "time_taken = end_time - start_time  # Calculate time taken\n",
    "\n",
    "print(f\"Iteration {iter} took {time_taken:.4f} seconds\")  # Print the time taken\n",
    "\n",
    "# 3. took 165.9984 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e2960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08755470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f4ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
