{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03098f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b4cb0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# ==================small size=================\\n# hyperparameters\\nbatch_size = 8 # how many independent sequences will we process in parallel?\\nblock_size = 512 # what is the maximum input context length for predictions?\\nmax_iters = 5000\\neval_interval = 500\\nlearning_rate = 3e-4\\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\\neval_iters = 200\\nn_embedding = 128 # 32 =  head_size = n_embedding / n_head\\nn_head = 8\\nn_layer = 20\\ndropout = 0.2\\n# ------------\\n\\n#torch.manual_seed(1337)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# ==================small size=================\n",
    "# hyperparameters\n",
    "batch_size = 8 # how many independent sequences will we process in parallel?\n",
    "block_size = 512 # what is the maximum input context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embedding = 128 # 32 =  head_size = n_embedding / n_head\n",
    "n_head = 8\n",
    "n_layer = 20\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "#torch.manual_seed(1337)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ded7e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 8 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 20000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "eval_iters = 200\n",
    "n_embedding = 384\n",
    "n_head = 24\n",
    "n_layer = 24\n",
    "dropout = 0.2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d2f46e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e904a097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Shakespeare\\nwith open('input.txt', 'r', encoding='utf-8') as f:\\n    text = f.read()\\n    #print(text)\\nprint(len(text))\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Shakespeare\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    #print(text)\n",
    "print(len(text))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4354225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649645\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7 habits of highly effective people\n",
    "with open('7_habits.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    #print(text)\n",
    "print(len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a6c2d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x', 'R', '\\n', 'J', 'j', 'S', 'W', 'o', '+', 'v', '8', 'e', 'k', 'h', 'z', '¬', '.', '7', ',', ';', '|', '#', '□', '?', '(', 'w', 'b', 't', '4', \"'\", '’', 'N', '2', '-', 'H', 'l', ':', ')', '$', '/', '=', '9', '\\\\', '<', '&', '—', 'q', 'O', ' ', 'p', 'Q', 'f', '\"', 'B', 's', '}', 'D', '1', '>', 'i', 'Y', 'M', 'A', 'I', '*', '0', '6', 'G', 'L', 'r', '5', '”', '‘', 'K', '3', 'c', 'u', 'm', 'd', 'X', 'V', 'y', '[', 'g', '“', '_', 'C', 'U', 'a', 'n', '^', '!', 'Z', 'F', 'T', 'E', 'P'}\n",
      "['x', 'R', '\\n', 'J', 'j', 'S', 'W', 'o', '+', 'v', '8', 'e', 'k', 'h', 'z', '¬', '.', '7', ',', ';', '|', '#', '□', '?', '(', 'w', 'b', 't', '4', \"'\", '’', 'N', '2', '-', 'H', 'l', ':', ')', '$', '/', '=', '9', '\\\\', '<', '&', '—', 'q', 'O', ' ', 'p', 'Q', 'f', '\"', 'B', 's', '}', 'D', '1', '>', 'i', 'Y', 'M', 'A', 'I', '*', '0', '6', 'G', 'L', 'r', '5', '”', '‘', 'K', '3', 'c', 'u', 'm', 'd', 'X', 'V', 'y', '[', 'g', '“', '_', 'C', 'U', 'a', 'n', '^', '!', 'Z', 'F', 'T', 'E', 'P']\n",
      "['\\n', ' ', '!', '\"', '#', '$', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', '¬', '—', '‘', '’', '“', '”', '□']\n",
      "97\n",
      "0\n",
      "[32, 33, 34]\n",
      "ABC\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "print(set(text))\n",
    "print(list(set(text)))\n",
    "print(sorted(list(set(text))))\n",
    "print(len(sorted(list(set(text)))))\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(stoi['\\n'])\n",
    "print(encode(['A','B','C']))\n",
    "print(decode(encode(['A','B','C'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3c395a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990416\n"
     ]
    }
   ],
   "source": [
    "# rich dad, poor dad\n",
    "with open('rich_dad_poor_dad.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    #print(text)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b1e2305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L', '&', 'M', '>', 'w', '.', 'I', '”', '?', '#', 'V', '\\n', 'Z', '9', 'G', '4', '“', 'y', 'E', 'q', 'X', ',', 'k', 'W', 'o', \"'\", 'r', '|', 'Q', 'b', 'J', '+', 'd', '3', '—', 'K', ':', '¬', 'R', '’', '□', 'c', 'A', '5', '1', 'u', 'P', 'z', 'l', '6', 'p', 'C', 'x', 'Y', 'N', 'O', 'g', '\\\\', 's', 'j', '8', '=', 'U', 'H', 'v', '/', '$', 'e', 'T', 'S', 'i', '_', '*', '!', 'F', 'a', '-', 'D', '\"', '<', 'f', '2', ')', '^', ' ', '0', 'h', 't', '7', 'B', '}', '[', 'n', ';', 'm', '(', '‘'}\n",
      "['L', '&', 'M', '>', 'w', '.', 'I', '”', '?', '#', 'V', '\\n', 'Z', '9', 'G', '4', '“', 'y', 'E', 'q', 'X', ',', 'k', 'W', 'o', \"'\", 'r', '|', 'Q', 'b', 'J', '+', 'd', '3', '—', 'K', ':', '¬', 'R', '’', '□', 'c', 'A', '5', '1', 'u', 'P', 'z', 'l', '6', 'p', 'C', 'x', 'Y', 'N', 'O', 'g', '\\\\', 's', 'j', '8', '=', 'U', 'H', 'v', '/', '$', 'e', 'T', 'S', 'i', '_', '*', '!', 'F', 'a', '-', 'D', '\"', '<', 'f', '2', ')', '^', ' ', '0', 'h', 't', '7', 'B', '}', '[', 'n', ';', 'm', '(', '‘']\n",
      "['\\n', ' ', '!', '\"', '#', '$', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', '¬', '—', '‘', '’', '“', '”', '□']\n",
      "97\n",
      "0\n",
      "[32, 33, 34]\n",
      "ABC\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "print(set(text))\n",
    "print(list(set(text)))\n",
    "print(sorted(list(set(text))))\n",
    "print(len(sorted(list(set(text)))))\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(stoi['\\n'])\n",
    "print(encode(['A','B','C']))\n",
    "print(decode(encode(['A','B','C'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9436653d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49, 70, 64, 69,  1, 35, 62, 65,  1, 47, 76, 76, 79,  1, 35, 62, 65,  1,\n",
      "         0,  0,  0, 49, 76, 63, 66, 79, 81,  1, 51, 14,  1, 42, 70, 86, 76, 80,\n",
      "        62, 72, 70,  1,  0,  0,  0, 49, 70, 64, 69,  1, 46, 62, 65,  1, 47, 76,\n",
      "        76, 79,  1, 46, 62, 65,  1,  0,  0,  0, 32, 82, 81, 69, 76, 79, 26,  1,\n",
      "        49, 76, 63, 66, 79, 81,  1, 51, 14,  1, 42, 70, 86, 76, 80, 62, 72, 70,\n",
      "         1,  0, 34, 62, 81, 66, 68, 76, 79, 86])\n",
      "Rich Dad Poor Dad \n",
      "\n",
      "\n",
      "Robert T. Kiyosaki \n",
      "\n",
      "\n",
      "Rich Oad Poor Oad \n",
      "\n",
      "\n",
      "Author: Robert T. Kiyosaki \n",
      "Category\n",
      "['\\n', ' ', '!', '\"', '#', '$', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', '¬', '—', '‘', '’', '“', '”', '□']\n",
      "tensor([49, 70, 64, 69,  1, 35, 62, 65,  1, 47, 76, 76, 79,  1, 35, 62, 65,  1,\n",
      "         0,  0,  0, 49, 76, 63, 66, 79, 81,  1, 51, 14,  1, 42, 70, 86, 76, 80,\n",
      "        62, 72, 70,  1,  0,  0,  0, 49, 70, 64, 69,  1, 46, 62, 65,  1, 47, 76,\n",
      "        76, 79,  1, 46, 62, 65,  1,  0,  0,  0, 32, 82, 81, 69, 76, 79, 26,  1,\n",
      "        49, 76, 63, 66, 79, 81,  1, 51, 14,  1, 42, 70, 86, 76, 80, 62, 72, 70,\n",
      "         1,  0, 34, 62, 81, 66, 68, 76, 79, 86, 26,  1, 32, 79, 81,  1, 76, 67,\n",
      "         1, 43, 70, 83, 70, 75, 68,  1,  0, 54, 66, 63, 80, 70, 81, 66, 26,  1,\n",
      "        69, 81, 81, 77, 26, 15, 15, 74, 76, 81, 80, 62, 64, 69, 14, 70, 75, 67,\n",
      "        76,  1,  0, 35, 62, 81, 66, 26,  1, 18, 24, 13, 46, 64, 81, 76, 63, 66,\n",
      "        79, 13, 18, 16, 17, 18,  1,  0,  0,  0, 47, 62, 68, 66,  1, 17, 15, 17,\n",
      "        17, 20,  1,  0,  0,  0, 69, 81, 81, 77,  1, 26,  1, 15,  1, 15,  1, 74,\n",
      "        76, 81, 80, 62, 64, 69,  1, 14,  1, 70, 75, 67, 76,  1,  0,  0,  0,  0,\n",
      "         0, 49, 70, 64, 69,  1, 35, 62, 65,  1, 47, 76, 76, 79,  1, 35, 62, 65,\n",
      "         1,  0,  0,  0, 49, 76, 63, 66, 79, 81,  1, 51, 14,  1, 42, 70, 86, 76,\n",
      "        80, 62, 72, 70,  1])\n",
      "Rich Dad Poor Dad \n",
      "\n",
      "\n",
      "Robert T. Kiyosaki \n",
      "\n",
      "\n",
      "Rich Oad Poor Oad \n",
      "\n",
      "\n",
      "Author: Robert T. Kiyosaki \n",
      "Category: Art of Living \n",
      "Website: http://motsach.info \n",
      "Date: 28-October-2012 \n",
      "\n",
      "\n",
      "Page 1/114 \n",
      "\n",
      "\n",
      "http : / / motsach . info \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rich Dad Poor Dad \n",
      "\n",
      "\n",
      "Robert T. Kiyosaki \n"
     ]
    }
   ],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(train_data[0:100])\n",
    "print(text[0:100])\n",
    "print(sorted(list(set(text))))\n",
    "\n",
    "\n",
    "#block_size = 8\n",
    "print(train_data[0:block_size+1])\n",
    "print(text[0:block_size+1])\n",
    "\n",
    "\n",
    "x = train_data[0:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "  context = x[0:t+1]\n",
    "  target = y[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e4056d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "torch.Size([8, 256])\n",
      "tensor([[62, 75, 65,  ..., 75, 70, 75],\n",
      "        [66, 75, 65,  ..., 86, 76, 82],\n",
      "        [70, 80,  1,  ..., 62, 86, 64],\n",
      "        ...,\n",
      "        [80,  1, 70,  ..., 81, 69, 66],\n",
      "        [75, 68,  1,  ..., 64, 70, 66],\n",
      "        [65,  1, 73,  ..., 75, 62, 81]], device='cuda:0')\n",
      "target:\n",
      "torch.Size([8, 256])\n",
      "tensor([[75, 65,  1,  ..., 70, 75, 68],\n",
      "        [75, 65, 66,  ..., 76, 82, 12],\n",
      "        [80,  1, 64,  ..., 86, 64, 69],\n",
      "        ...,\n",
      "        [ 1, 70, 75,  ..., 69, 66, 86],\n",
      "        [68,  1, 81,  ..., 70, 66, 81],\n",
      "        [ 1, 73, 62,  ..., 62, 81, 70]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "#torch.manual_seed(1337)\n",
    "#batch_size = 4\n",
    "#block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"input:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"target:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "  for t in range(block_size):\n",
    "    context = xb[ b, 0:t+1 ]\n",
    "    target = yb[ b, t ]\n",
    "    #print(context,\"->\",target)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e3aad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GateLayer, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, n_embedding)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, encoder_output):\n",
    "        # Compute gating vector\n",
    "        gating_vector = self.linear(encoder_output)\n",
    "        gating_vector = self.sigmoid(gating_vector)\n",
    "        # Apply gating vector to encoder output\n",
    "        gated_encoder_output = encoder_output * gating_vector\n",
    "        \n",
    "        return gated_encoder_output\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key  = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gate = GateLayer( n_embedding, head_size )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        #print(\"====x=====\")\n",
    "        #print(x.shape)\n",
    "        #x = self.gate(x)\n",
    "\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        #print(k.shape)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        attention_part = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        attention_part = attention_part.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        attention_part = F.softmax(attention_part, dim=-1) # (B, T, T)\n",
    "        attention_part = self.dropout(attention_part)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = attention_part @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(n_embedding, n_embedding)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.linear(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embedding):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embedding, 4 * n_embedding),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embedding, n_embedding),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embedding, n_head):\n",
    "        # n_embedding: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embedding // n_head\n",
    "        self.multi_head_attention = MultiHeadAttention(n_head, head_size)\n",
    "        self.feed_forward = FeedFoward(n_embedding)\n",
    "        self.layer_norm_1 = nn.LayerNorm(n_embedding)\n",
    "        self.layer_norm_2 = nn.LayerNorm(n_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.multi_head_attention(self.layer_norm_1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class GPT2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_into_embedding = nn.Embedding(vocab_size, n_embedding)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embedding)\n",
    "        \n",
    "        #Input: (∗), IntTensor or LongTensor of arbitrary shape containing the indices to extract\n",
    "        #Output: (∗,H), where * is the input shape and H=embedding_dim\n",
    "        \n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block(n_embedding, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.layer_norm = nn.LayerNorm(n_embedding) # final layer norm\n",
    "        #self.ffwd = FeedFoward(n_embedding)\n",
    "        #self.sa_head = MultiHeadAttention(4,n_embedding//4)\n",
    "        self.linear_head = nn.Linear(n_embedding, vocab_size)\n",
    "\n",
    "    def forward(self, id_number_of_vector_x, targets=None):#target (B,T)\n",
    "        #B,T =id_number_of_vector_x.shape\n",
    "        #tok_embed = self.token_into_embedding(id_number_of_vector_x) #(B,T,C) (batch,Time,Channel)\n",
    "        #logits = self.lm_head(tok_embed) #(B,T,vocab_size)\n",
    "\n",
    "        B, T = id_number_of_vector_x.shape\n",
    "\n",
    "        # id_number_of_vector_x and targets are both (B,T) tensor of integers\n",
    "        #tok_emb = self.token_into_embedding(id_number_of_vector_x) # (B,T,C)\n",
    "        #pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        #x = tok_emb + pos_emb # (B,T,C) + pos_emb across the batch\n",
    "        #x = self.blocks(x) # (B,T,C)\n",
    "        #x = self.ln_f(x) # (B,T,C)\n",
    "        \n",
    "        #x = self.sa_head(x) # (B,T,vocab_size)\n",
    "        #logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        # id_number_of_vector_x and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_into_embedding(id_number_of_vector_x) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        #x = tok_emb + pos_emb # (B,T,C)\n",
    "        #x = self.blocks(x) # (B,T,C)\n",
    "        #x = self.ln_f(x) # (B,T,C)\n",
    "        #x = self.sa_head(x) # (B,T,vocab_size)\n",
    "        #x = self.ffwd(x)\n",
    "        #logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,T,C) + pos_emb across the batch\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.layer_norm(x) # (B,T,C)\n",
    "        \n",
    "        #x = self.sa_head(x) # (B,T,vocab_size)\n",
    "        logits = self.linear_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets == None:\n",
    "          loss = None\n",
    "        else:\n",
    "          B, T, C = logits.shape # logit(p) = ln( p / ( 1 - p ) )\n",
    "          logits = logits.view(B*T,C)\n",
    "          targets = targets.view(B*T)\n",
    "          loss = F.cross_entropy(logits,targets) # H( P, Q ) = -0.9 * log( 0.8 ) - 0.1 * log( 0.2 ) = 0.311, the lower the better matching\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, id_number_of_vector_x, max_new_tokens):\n",
    "        # id_number_of_vector_x is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop id_number_of_vector_x to the last block_size tokens\n",
    "            id_number_of_vector_x_cut = id_number_of_vector_x[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(id_number_of_vector_x_cut)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            id_number_of_vector_x_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            id_number_of_vector_x = torch.cat((id_number_of_vector_x, id_number_of_vector_x_next), dim=1) # (B, T+1)\n",
    "        return id_number_of_vector_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "109d8731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.888993 M parameters\n",
      "torch.Size([2048, 97]) tensor(4.6676, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "model.load_state_dict(torch.load(\"./model\"))\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "'''\n",
    "\n",
    "model = GPT2()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "logits, loss = m(xb,yb)\n",
    "print(logits.shape,loss) # -ln(1/65) = 4.174387269896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a47f00fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.888993 M parameters\n",
      "tensor([[0]])\n",
      "tensor([[32]], device='cuda:0')\n",
      "Athos, as we recognize the autobiography \n",
      "of Anwer Sadatan production, of all the evits of the very p\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GPT2()\n",
    "#model.load_state_dict(torch.load(\"./GPT2_Shakespeare\"))\n",
    "model.load_state_dict(torch.load(\"./GPT2_rich_dad_poor_dad\"))\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "id_number_of_vector_x = torch.zeros((1,1), dtype=torch.long)\n",
    "print(id_number_of_vector_x)\n",
    "print(torch.tensor([[32]]).to(device))\n",
    "#print(decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=10)[0].tolist()))\n",
    "\n",
    "print(decode(m.generate(torch.tensor([[32]]).to(device), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be071f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALE \n",
      "\n",
      "\n",
      "Brcu^t toyu tyH^Hairt \n",
      "\n",
      "\n",
      "practical and useful of giving. Both of my vision and the curactive a\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.tensor([[32]]).to(device), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ef93a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_name', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'bias', 'buffers', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'half', 'in_features', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'out_features', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'requires_grad_', 'reset_parameters', 'share_memory', 'state_dict', 'to', 'train', 'training', 'type', 'weight', 'zero_grad']\n",
      "<bound method Module.type of Linear(in_features=384, out_features=16, bias=False)>\n",
      "384\n",
      "16\n",
      "torch.Size([16, 384])\n",
      "torch.Size([384])\n",
      "['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_name', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'buffers', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'embedding_dim', 'eval', 'extra_repr', 'float', 'forward', 'from_pretrained', 'half', 'load_state_dict', 'max_norm', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'norm_type', 'num_embeddings', 'padding_idx', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'requires_grad_', 'reset_parameters', 'scale_grad_by_freq', 'share_memory', 'sparse', 'state_dict', 'to', 'train', 'training', 'type', 'weight', 'zero_grad']\n",
      "['T', '__abs__', '__add__', '__and__', '__array__', '__array_priority__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', '__cuda_array_interface__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__idiv__', '__ifloordiv__', '__ilshift__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__long__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rfloordiv__', '__rmul__', '__rpow__', '__rshift__', '__rsub__', '__rtruediv__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__torch_function__', '__truediv__', '__weakref__', '__xor__', '_backward_hooks', '_base', '_cdata', '_coalesced_', '_dimI', '_dimV', '_grad', '_grad_fn', '_indices', '_is_view', '_make_subclass', '_nnz', '_update_names', '_values', '_version', 'abs', 'abs_', 'absolute', 'absolute_', 'acos', 'acos_', 'acosh', 'acosh_', 'add', 'add_', 'addbmm', 'addbmm_', 'addcdiv', 'addcdiv_', 'addcmul', 'addcmul_', 'addmm', 'addmm_', 'addmv', 'addmv_', 'addr', 'addr_', 'align_as', 'align_to', 'all', 'allclose', 'amax', 'amin', 'angle', 'any', 'apply_', 'arccos', 'arccos_', 'arccosh', 'arccosh_', 'arcsin', 'arcsin_', 'arcsinh', 'arcsinh_', 'arctan', 'arctan_', 'arctanh', 'arctanh_', 'argmax', 'argmin', 'argsort', 'as_strided', 'as_strided_', 'as_subclass', 'asin', 'asin_', 'asinh', 'asinh_', 'atan', 'atan2', 'atan2_', 'atan_', 'atanh', 'atanh_', 'backward', 'baddbmm', 'baddbmm_', 'bernoulli', 'bernoulli_', 'bfloat16', 'bincount', 'bitwise_and', 'bitwise_and_', 'bitwise_not', 'bitwise_not_', 'bitwise_or', 'bitwise_or_', 'bitwise_xor', 'bitwise_xor_', 'bmm', 'bool', 'byte', 'cauchy_', 'ceil', 'ceil_', 'char', 'cholesky', 'cholesky_inverse', 'cholesky_solve', 'chunk', 'clamp', 'clamp_', 'clamp_max', 'clamp_max_', 'clamp_min', 'clamp_min_', 'clip', 'clip_', 'clone', 'coalesce', 'conj', 'contiguous', 'copy_', 'cos', 'cos_', 'cosh', 'cosh_', 'count_nonzero', 'cpu', 'cross', 'cuda', 'cummax', 'cummin', 'cumprod', 'cumsum', 'data', 'data_ptr', 'deg2rad', 'deg2rad_', 'dense_dim', 'dequantize', 'det', 'detach', 'detach_', 'device', 'diag', 'diag_embed', 'diagflat', 'diagonal', 'digamma', 'digamma_', 'dim', 'dist', 'div', 'div_', 'divide', 'divide_', 'dot', 'double', 'dtype', 'eig', 'element_size', 'eq', 'eq_', 'equal', 'erf', 'erf_', 'erfc', 'erfc_', 'erfinv', 'erfinv_', 'exp', 'exp2', 'exp2_', 'exp_', 'expand', 'expand_as', 'expm1', 'expm1_', 'exponential_', 'fft', 'fill_', 'fill_diagonal_', 'fix', 'fix_', 'flatten', 'flip', 'fliplr', 'flipud', 'float', 'floor', 'floor_', 'floor_divide', 'floor_divide_', 'fmod', 'fmod_', 'frac', 'frac_', 'gather', 'gcd', 'gcd_', 'ge', 'ge_', 'geometric_', 'geqrf', 'ger', 'get_device', 'grad', 'grad_fn', 'greater', 'greater_', 'greater_equal', 'greater_equal_', 'gt', 'gt_', 'half', 'hardshrink', 'has_names', 'heaviside', 'heaviside_', 'histc', 'hypot', 'hypot_', 'i0', 'i0_', 'ifft', 'imag', 'index_add', 'index_add_', 'index_copy', 'index_copy_', 'index_fill', 'index_fill_', 'index_put', 'index_put_', 'index_select', 'indices', 'int', 'int_repr', 'inverse', 'irfft', 'is_coalesced', 'is_complex', 'is_contiguous', 'is_cuda', 'is_distributed', 'is_floating_point', 'is_leaf', 'is_meta', 'is_mkldnn', 'is_nonzero', 'is_pinned', 'is_quantized', 'is_same_size', 'is_set_to', 'is_shared', 'is_signed', 'is_sparse', 'isclose', 'isfinite', 'isinf', 'isnan', 'isneginf', 'isposinf', 'isreal', 'istft', 'item', 'kthvalue', 'layout', 'lcm', 'lcm_', 'le', 'le_', 'lerp', 'lerp_', 'less', 'less_', 'less_equal', 'less_equal_', 'lgamma', 'lgamma_', 'log', 'log10', 'log10_', 'log1p', 'log1p_', 'log2', 'log2_', 'log_', 'log_normal_', 'log_softmax', 'logaddexp', 'logaddexp2', 'logcumsumexp', 'logdet', 'logical_and', 'logical_and_', 'logical_not', 'logical_not_', 'logical_or', 'logical_or_', 'logical_xor', 'logical_xor_', 'logit', 'logit_', 'logsumexp', 'long', 'lstsq', 'lt', 'lt_', 'lu', 'lu_solve', 'map2_', 'map_', 'masked_fill', 'masked_fill_', 'masked_scatter', 'masked_scatter_', 'masked_select', 'matmul', 'matrix_exp', 'matrix_power', 'max', 'maximum', 'mean', 'median', 'min', 'minimum', 'mm', 'mode', 'movedim', 'mul', 'mul_', 'multinomial', 'multiply', 'multiply_', 'mv', 'mvlgamma', 'mvlgamma_', 'name', 'names', 'nanquantile', 'nansum', 'narrow', 'narrow_copy', 'ndim', 'ndimension', 'ne', 'ne_', 'neg', 'neg_', 'negative', 'negative_', 'nelement', 'new', 'new_empty', 'new_full', 'new_ones', 'new_tensor', 'new_zeros', 'nextafter', 'nextafter_', 'nonzero', 'norm', 'normal_', 'not_equal', 'not_equal_', 'numel', 'numpy', 'orgqr', 'ormqr', 'outer', 'output_nr', 'permute', 'pin_memory', 'pinverse', 'polygamma', 'polygamma_', 'pow', 'pow_', 'prelu', 'prod', 'put_', 'q_per_channel_axis', 'q_per_channel_scales', 'q_per_channel_zero_points', 'q_scale', 'q_zero_point', 'qr', 'qscheme', 'quantile', 'rad2deg', 'rad2deg_', 'random_', 'real', 'reciprocal', 'reciprocal_', 'record_stream', 'refine_names', 'register_hook', 'reinforce', 'relu', 'relu_', 'remainder', 'remainder_', 'rename', 'rename_', 'renorm', 'renorm_', 'repeat', 'repeat_interleave', 'requires_grad', 'requires_grad_', 'reshape', 'reshape_as', 'resize', 'resize_', 'resize_as', 'resize_as_', 'retain_grad', 'rfft', 'roll', 'rot90', 'round', 'round_', 'rsqrt', 'rsqrt_', 'scatter', 'scatter_', 'scatter_add', 'scatter_add_', 'select', 'set_', 'sgn', 'sgn_', 'shape', 'share_memory_', 'short', 'sigmoid', 'sigmoid_', 'sign', 'sign_', 'signbit', 'sin', 'sin_', 'sinh', 'sinh_', 'size', 'slogdet', 'smm', 'softmax', 'solve', 'sort', 'sparse_dim', 'sparse_mask', 'sparse_resize_', 'sparse_resize_and_clear_', 'split', 'split_with_sizes', 'sqrt', 'sqrt_', 'square', 'square_', 'squeeze', 'squeeze_', 'sspaddmm', 'std', 'stft', 'storage', 'storage_offset', 'storage_type', 'stride', 'sub', 'sub_', 'subtract', 'subtract_', 'sum', 'sum_to_size', 'svd', 'symeig', 't', 't_', 'take', 'tan', 'tan_', 'tanh', 'tanh_', 'to', 'to_dense', 'to_mkldnn', 'to_sparse', 'tolist', 'topk', 'trace', 'transpose', 'transpose_', 'triangular_solve', 'tril', 'tril_', 'triu', 'triu_', 'true_divide', 'true_divide_', 'trunc', 'trunc_', 'type', 'type_as', 'unbind', 'unflatten', 'unfold', 'uniform_', 'unique', 'unique_consecutive', 'unsafe_chunk', 'unsafe_split', 'unsafe_split_with_sizes', 'unsqueeze', 'unsqueeze_', 'values', 'var', 'vdot', 'view', 'view_as', 'where', 'zero_']\n",
      "torch.Size([97, 384])\n",
      "tensor([-1.3664e-01, -4.0590e-01, -2.2219e-01,  5.3633e-01,  1.3847e+00,\n",
      "         6.0940e-01,  1.0424e+00,  1.1116e+00, -2.6177e-01, -1.1758e-01,\n",
      "        -8.0097e-01,  3.9556e-01,  1.8125e-01,  4.5105e-01,  2.7599e-01,\n",
      "         5.1245e-01,  5.1657e-01,  1.4031e+00, -6.5616e-01,  3.6901e-01,\n",
      "         7.3739e-01, -2.9283e-01, -1.2868e+00, -7.9836e-01, -2.4348e-01,\n",
      "        -4.3813e-01,  3.0676e-01,  8.2756e-01, -1.9864e-01, -4.6908e-01,\n",
      "         4.0065e-01,  2.0083e+00, -5.0702e-01,  7.8803e-01, -9.9201e-01,\n",
      "         2.9955e-01, -1.2661e-01,  6.3483e-02,  5.6666e-01,  5.4232e-01,\n",
      "         9.1668e-01, -1.0347e-01,  1.1669e+00,  1.8649e+00,  5.9467e-02,\n",
      "         8.1702e-01, -4.9200e-01,  8.2695e-01, -9.3656e-01, -7.9871e-01,\n",
      "         1.0738e+00, -4.7494e-01, -7.2439e-01, -2.2799e-01,  3.0416e-01,\n",
      "         1.1063e+00, -4.2677e-01,  6.1318e-01,  1.3603e+00, -1.1408e+00,\n",
      "        -7.3598e-01, -7.5957e-01, -6.9394e-02, -2.5286e-01,  4.9853e-01,\n",
      "         4.6131e-01, -6.1774e-01,  7.9585e-01,  4.2797e-01, -1.7809e-01,\n",
      "        -1.9401e+00, -5.5100e-01,  2.6329e-01,  1.1866e+00, -5.6975e-01,\n",
      "        -1.1178e-01, -5.2068e-02,  1.3399e+00,  1.2692e-01, -1.9369e+00,\n",
      "        -1.5599e-01, -3.0305e-01, -2.8277e+00,  1.5071e-02,  2.3361e+00,\n",
      "        -3.8259e-01, -3.2923e-01, -5.6894e-01,  7.5279e-01, -1.4578e+00,\n",
      "        -1.9438e+00, -2.3516e+00, -4.1028e-02, -3.2798e-02, -5.7765e-01,\n",
      "        -3.2116e-01, -2.5557e-01,  1.0580e-01,  4.3000e-01,  3.2293e-01,\n",
      "         2.7354e-01,  6.9304e-01,  1.8769e+00, -3.9564e-01, -1.3200e+00,\n",
      "         2.2713e-01,  4.8751e-01, -1.3580e+00, -3.8609e-01, -1.0239e+00,\n",
      "        -1.4512e-01, -2.3120e-01,  4.3180e-01, -5.5675e-01, -7.9909e-01,\n",
      "        -7.5038e-01,  8.4322e-01, -3.1666e-01,  3.4654e-01,  7.7588e-01,\n",
      "         3.8141e-01, -2.0563e-01,  3.1469e-01, -8.0654e-01,  4.7067e-01,\n",
      "         1.0281e+00, -4.8603e-01, -2.9031e+00,  4.1955e-01,  1.6954e+00,\n",
      "        -9.4966e-01, -2.0211e-01, -5.6915e-02, -7.5391e-01,  5.8532e-01,\n",
      "         1.3990e+00, -8.8408e-02,  1.9064e-01,  7.5883e-01,  2.4874e-03,\n",
      "        -2.5647e-02,  2.1273e-01,  1.1395e+00, -5.0308e-01,  8.5667e-01,\n",
      "         8.8261e-01,  8.0532e-01, -8.7556e-01, -1.4758e-01, -1.4905e-01,\n",
      "         5.1957e-02, -9.1003e-01,  6.8148e-01, -5.4598e-01,  5.3370e-01,\n",
      "         3.8573e-01,  3.3385e-02,  1.6973e-01, -1.8141e-01, -7.0944e-01,\n",
      "        -1.2175e+00, -1.1221e+00,  1.2512e-01,  7.7090e-01, -5.4690e-01,\n",
      "         9.2581e-01, -1.1107e-01,  7.5230e-01,  1.5444e+00, -1.1679e+00,\n",
      "        -5.8597e-01,  1.0054e+00,  5.6257e-02,  4.0462e-01,  3.9183e-01,\n",
      "         6.4785e-01,  1.2511e-02,  7.7656e-01,  1.0831e+00, -7.0587e-01,\n",
      "        -1.8596e-01, -1.8027e+00, -6.2220e-01,  1.2311e+00, -1.7799e+00,\n",
      "        -1.0808e+00, -6.7453e-01, -1.1141e-01,  8.4733e-02,  1.9948e-02,\n",
      "        -4.0087e-01,  3.3899e-01,  8.1522e-01,  1.5621e+00, -7.0142e-01,\n",
      "        -4.8223e-01, -6.6697e-01, -1.8925e-01,  1.1880e+00, -1.2273e+00,\n",
      "         8.9999e-01,  5.9492e-01,  8.1438e-01,  1.5666e-02, -8.8533e-01,\n",
      "         4.1776e-01, -6.4685e-01,  4.5221e-01,  2.5957e-02,  3.9510e-01,\n",
      "        -8.5163e-01,  8.2882e-01,  9.5060e-01, -4.4255e-01, -9.2770e-01,\n",
      "        -1.5975e+00,  2.9371e-01, -1.2315e+00, -8.7343e-01,  8.9428e-02,\n",
      "         1.7647e-01, -7.4679e-01,  2.8629e+00, -4.5990e-01, -1.6549e-01,\n",
      "         1.1021e+00, -1.6458e+00, -4.1318e-02,  2.1957e+00,  1.1366e+00,\n",
      "         1.5442e-01, -1.3475e+00, -1.0809e+00,  1.2546e+00, -8.4330e-01,\n",
      "         4.5855e-01, -4.8445e-02, -6.4026e-01, -2.1022e+00, -1.0389e-01,\n",
      "        -1.2566e+00,  1.0140e+00,  4.5058e-01, -6.1631e-01, -8.6026e-01,\n",
      "         1.2391e+00, -6.6977e-01,  7.2814e-01,  6.4342e-01, -7.0971e-01,\n",
      "        -9.4501e-01,  7.4483e-01,  2.8113e-01,  1.3425e+00, -1.0013e+00,\n",
      "        -9.8773e-01,  1.6508e+00,  1.0094e+00, -1.3246e+00,  4.7838e-02,\n",
      "        -5.5132e-01,  1.8356e+00,  4.9750e-01,  8.4641e-01, -6.9083e-01,\n",
      "        -4.2841e-01, -4.1163e-01,  6.7305e-01, -7.4259e-01, -3.8063e-01,\n",
      "         1.4060e-01, -1.2704e-01, -4.7451e-01,  2.1874e-01,  1.1581e-01,\n",
      "         4.9308e-01,  1.5480e-01,  4.7753e-01, -1.5080e+00,  1.0588e+00,\n",
      "        -1.9669e+00,  2.1036e+00,  7.2395e-01, -6.3684e-01, -7.8241e-01,\n",
      "         6.4944e-01,  2.8066e-03,  1.3208e+00, -1.5772e+00,  1.0723e+00,\n",
      "        -8.2203e-01,  1.4328e+00, -2.3067e-01,  7.0783e-01, -4.4902e-01,\n",
      "        -1.3776e+00, -4.4486e-01, -6.3538e-01,  2.1044e-01, -1.1066e+00,\n",
      "         1.2276e+00, -1.0044e+00,  7.8033e-01, -1.0372e-01, -7.7795e-01,\n",
      "        -8.6399e-01,  1.1657e+00, -7.7011e-01,  6.8821e-01, -2.3464e+00,\n",
      "        -1.0903e+00,  1.0466e+00, -2.4827e-01,  8.8626e-01, -1.4259e+00,\n",
      "        -1.1011e+00, -5.7526e-01,  8.8903e-01,  1.0949e+00, -8.0354e-01,\n",
      "        -3.5087e-01, -5.7875e-02,  6.7720e-01, -8.5290e-02, -6.5205e-02,\n",
      "        -3.7540e-01, -5.5354e-01, -2.8629e-01,  3.4709e-01, -8.3628e-01,\n",
      "        -1.1885e+00,  1.1538e+00,  2.3160e-01, -1.4859e-01, -1.4890e+00,\n",
      "         5.2647e-01,  6.6733e-01, -8.2302e-01,  3.9545e-01, -2.9798e-01,\n",
      "        -7.5579e-01,  7.2871e-01, -3.2641e-01, -9.1883e-01, -4.0755e-01,\n",
      "         1.5744e-01, -5.8806e-02, -6.9044e-01,  4.2294e-01, -1.4729e+00,\n",
      "        -4.8012e-01, -1.0481e+00, -4.4776e-01, -2.7175e-01,  2.2290e-02,\n",
      "        -2.1795e+00,  1.0271e+00, -7.6579e-01,  3.9064e-01,  7.7011e-01,\n",
      "        -6.5713e-01,  2.2870e-01,  6.1872e-01, -2.0098e-01,  2.5962e-02,\n",
      "        -1.5894e+00, -8.6020e-01, -1.8814e+00, -7.1663e-02,  9.1778e-01,\n",
      "         9.2523e-02, -1.2062e+00, -1.1386e+00, -8.6978e-01,  1.0440e+00,\n",
      "        -2.7735e+00,  1.2994e+00,  1.2769e+00, -2.4975e-01, -1.2188e+00,\n",
      "        -5.0826e-01,  8.4310e-01,  8.4368e-01, -7.9602e-02], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_name', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'bias', 'buffers', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'half', 'in_features', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'out_features', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'requires_grad_', 'reset_parameters', 'share_memory', 'state_dict', 'to', 'train', 'training', 'type', 'weight', 'zero_grad']\\n<bound method Module.type of Linear(in_features=384, out_features=16, bias=False)>\\n384\\n16\\ntorch.Size([16, 384])\\ntorch.Size([384])\\n['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_name', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'buffers', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'embedding_dim', 'eval', 'extra_repr', 'float', 'forward', 'from_pretrained', 'half', 'load_state_dict', 'max_norm', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'norm_type', 'num_embeddings', 'padding_idx', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'requires_grad_', 'reset_parameters', 'scale_grad_by_freq', 'share_memory', 'sparse', 'state_dict', 'to', 'train', 'training', 'type', 'weight', 'zero_grad']\\n['T', '__abs__', '__add__', '__and__', '__array__', '__array_priority__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', '__cuda_array_interface__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__idiv__', '__ifloordiv__', '__ilshift__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__long__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rfloordiv__', '__rmul__', '__rpow__', '__rshift__', '__rsub__', '__rtruediv__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__torch_function__', '__truediv__', '__weakref__', '__xor__', '_backward_hooks', '_base', '_cdata', '_coalesced_', '_dimI', '_dimV', '_grad', '_grad_fn', '_indices', '_is_view', '_make_subclass', '_nnz', '_update_names', '_values', '_version', 'abs', 'abs_', 'absolute', 'absolute_', 'acos', 'acos_', 'acosh', 'acosh_', 'add', 'add_', 'addbmm', 'addbmm_', 'addcdiv', 'addcdiv_', 'addcmul', 'addcmul_', 'addmm', 'addmm_', 'addmv', 'addmv_', 'addr', 'addr_', 'align_as', 'align_to', 'all', 'allclose', 'amax', 'amin', 'angle', 'any', 'apply_', 'arccos', 'arccos_', 'arccosh', 'arccosh_', 'arcsin', 'arcsin_', 'arcsinh', 'arcsinh_', 'arctan', 'arctan_', 'arctanh', 'arctanh_', 'argmax', 'argmin', 'argsort', 'as_strided', 'as_strided_', 'as_subclass', 'asin', 'asin_', 'asinh', 'asinh_', 'atan', 'atan2', 'atan2_', 'atan_', 'atanh', 'atanh_', 'backward', 'baddbmm', 'baddbmm_', 'bernoulli', 'bernoulli_', 'bfloat16', 'bincount', 'bitwise_and', 'bitwise_and_', 'bitwise_not', 'bitwise_not_', 'bitwise_or', 'bitwise_or_', 'bitwise_xor', 'bitwise_xor_', 'bmm', 'bool', 'byte', 'cauchy_', 'ceil', 'ceil_', 'char', 'cholesky', 'cholesky_inverse', 'cholesky_solve', 'chunk', 'clamp', 'clamp_', 'clamp_max', 'clamp_max_', 'clamp_min', 'clamp_min_', 'clip', 'clip_', 'clone', 'coalesce', 'conj', 'contiguous', 'copy_', 'cos', 'cos_', 'cosh', 'cosh_', 'count_nonzero', 'cpu', 'cross', 'cuda', 'cummax', 'cummin', 'cumprod', 'cumsum', 'data', 'data_ptr', 'deg2rad', 'deg2rad_', 'dense_dim', 'dequantize', 'det', 'detach', 'detach_', 'device', 'diag', 'diag_embed', 'diagflat', 'diagonal', 'digamma', 'digamma_', 'dim', 'dist', 'div', 'div_', 'divide', 'divide_', 'dot', 'double', 'dtype', 'eig', 'element_size', 'eq', 'eq_', 'equal', 'erf', 'erf_', 'erfc', 'erfc_', 'erfinv', 'erfinv_', 'exp', 'exp2', 'exp2_', 'exp_', 'expand', 'expand_as', 'expm1', 'expm1_', 'exponential_', 'fft', 'fill_', 'fill_diagonal_', 'fix', 'fix_', 'flatten', 'flip', 'fliplr', 'flipud', 'float', 'floor', 'floor_', 'floor_divide', 'floor_divide_', 'fmod', 'fmod_', 'frac', 'frac_', 'gather', 'gcd', 'gcd_', 'ge', 'ge_', 'geometric_', 'geqrf', 'ger', 'get_device', 'grad', 'grad_fn', 'greater', 'greater_', 'greater_equal', 'greater_equal_', 'gt', 'gt_', 'half', 'hardshrink', 'has_names', 'heaviside', 'heaviside_', 'histc', 'hypot', 'hypot_', 'i0', 'i0_', 'ifft', 'imag', 'index_add', 'index_add_', 'index_copy', 'index_copy_', 'index_fill', 'index_fill_', 'index_put', 'index_put_', 'index_select', 'indices', 'int', 'int_repr', 'inverse', 'irfft', 'is_coalesced', 'is_complex', 'is_contiguous', 'is_cuda', 'is_distributed', 'is_floating_point', 'is_leaf', 'is_meta', 'is_mkldnn', 'is_nonzero', 'is_pinned', 'is_quantized', 'is_same_size', 'is_set_to', 'is_shared', 'is_signed', 'is_sparse', 'isclose', 'isfinite', 'isinf', 'isnan', 'isneginf', 'isposinf', 'isreal', 'istft', 'item', 'kthvalue', 'layout', 'lcm', 'lcm_', 'le', 'le_', 'lerp', 'lerp_', 'less', 'less_', 'less_equal', 'less_equal_', 'lgamma', 'lgamma_', 'log', 'log10', 'log10_', 'log1p', 'log1p_', 'log2', 'log2_', 'log_', 'log_normal_', 'log_softmax', 'logaddexp', 'logaddexp2', 'logcumsumexp', 'logdet', 'logical_and', 'logical_and_', 'logical_not', 'logical_not_', 'logical_or', 'logical_or_', 'logical_xor', 'logical_xor_', 'logit', 'logit_', 'logsumexp', 'long', 'lstsq', 'lt', 'lt_', 'lu', 'lu_solve', 'map2_', 'map_', 'masked_fill', 'masked_fill_', 'masked_scatter', 'masked_scatter_', 'masked_select', 'matmul', 'matrix_exp', 'matrix_power', 'max', 'maximum', 'mean', 'median', 'min', 'minimum', 'mm', 'mode', 'movedim', 'mul', 'mul_', 'multinomial', 'multiply', 'multiply_', 'mv', 'mvlgamma', 'mvlgamma_', 'name', 'names', 'nanquantile', 'nansum', 'narrow', 'narrow_copy', 'ndim', 'ndimension', 'ne', 'ne_', 'neg', 'neg_', 'negative', 'negative_', 'nelement', 'new', 'new_empty', 'new_full', 'new_ones', 'new_tensor', 'new_zeros', 'nextafter', 'nextafter_', 'nonzero', 'norm', 'normal_', 'not_equal', 'not_equal_', 'numel', 'numpy', 'orgqr', 'ormqr', 'outer', 'output_nr', 'permute', 'pin_memory', 'pinverse', 'polygamma', 'polygamma_', 'pow', 'pow_', 'prelu', 'prod', 'put_', 'q_per_channel_axis', 'q_per_channel_scales', 'q_per_channel_zero_points', 'q_scale', 'q_zero_point', 'qr', 'qscheme', 'quantile', 'rad2deg', 'rad2deg_', 'random_', 'real', 'reciprocal', 'reciprocal_', 'record_stream', 'refine_names', 'register_hook', 'reinforce', 'relu', 'relu_', 'remainder', 'remainder_', 'rename', 'rename_', 'renorm', 'renorm_', 'repeat', 'repeat_interleave', 'requires_grad', 'requires_grad_', 'reshape', 'reshape_as', 'resize', 'resize_', 'resize_as', 'resize_as_', 'retain_grad', 'rfft', 'roll', 'rot90', 'round', 'round_', 'rsqrt', 'rsqrt_', 'scatter', 'scatter_', 'scatter_add', 'scatter_add_', 'select', 'set_', 'sgn', 'sgn_', 'shape', 'share_memory_', 'short', 'sigmoid', 'sigmoid_', 'sign', 'sign_', 'signbit', 'sin', 'sin_', 'sinh', 'sinh_', 'size', 'slogdet', 'smm', 'softmax', 'solve', 'sort', 'sparse_dim', 'sparse_mask', 'sparse_resize_', 'sparse_resize_and_clear_', 'split', 'split_with_sizes', 'sqrt', 'sqrt_', 'square', 'square_', 'squeeze', 'squeeze_', 'sspaddmm', 'std', 'stft', 'storage', 'storage_offset', 'storage_type', 'stride', 'sub', 'sub_', 'subtract', 'subtract_', 'sum', 'sum_to_size', 'svd', 'symeig', 't', 't_', 'take', 'tan', 'tan_', 'tanh', 'tanh_', 'to', 'to_dense', 'to_mkldnn', 'to_sparse', 'tolist', 'topk', 'trace', 'transpose', 'transpose_', 'triangular_solve', 'tril', 'tril_', 'triu', 'triu_', 'true_divide', 'true_divide_', 'trunc', 'trunc_', 'type', 'type_as', 'unbind', 'unflatten', 'unfold', 'uniform_', 'unique', 'unique_consecutive', 'unsafe_chunk', 'unsafe_split', 'unsafe_split_with_sizes', 'unsqueeze', 'unsqueeze_', 'values', 'var', 'vdot', 'view', 'view_as', 'where', 'zero_']\\ntorch.Size([97, 384])\\ntensor([-1.3664e-01, -4.0590e-01, -2.2219e-01,  5.3633e-01,  1.3847e+00,\\n         6.0940e-01,  1.0424e+00,  1.1116e+00, -2.6177e-01, -1.1758e-01,\\n        -8.0097e-01,  3.9556e-01,  1.8125e-01,  4.5105e-01,  2.7599e-01,\\n         5.1245e-01,  5.1657e-01,  1.4031e+00, -6.5616e-01,  3.6901e-01,\\n         7.3739e-01, -2.9283e-01, -1.2868e+00, -7.9836e-01, -2.4348e-01,\\n        -4.3813e-01,  3.0676e-01,  8.2756e-01, -1.9864e-01, -4.6908e-01,\\n         4.0065e-01,  2.0083e+00, -5.0702e-01,  7.8803e-01, -9.9201e-01,\\n         2.9955e-01, -1.2661e-01,  6.3483e-02,  5.6666e-01,  5.4232e-01,\\n         9.1668e-01, -1.0347e-01,  1.1669e+00,  1.8649e+00,  5.9467e-02,\\n         8.1702e-01, -4.9200e-01,  8.2695e-01, -9.3656e-01, -7.9871e-01,\\n         1.0738e+00, -4.7494e-01, -7.2439e-01, -2.2799e-01,  3.0416e-01,\\n         1.1063e+00, -4.2677e-01,  6.1318e-01,  1.3603e+00, -1.1408e+00,\\n        -7.3598e-01, -7.5957e-01, -6.9394e-02, -2.5286e-01,  4.9853e-01,\\n         4.6131e-01, -6.1774e-01,  7.9585e-01,  4.2797e-01, -1.7809e-01,\\n        -1.9401e+00, -5.5100e-01,  2.6329e-01,  1.1866e+00, -5.6975e-01,\\n        -1.1178e-01, -5.2068e-02,  1.3399e+00,  1.2692e-01, -1.9369e+00,\\n        -1.5599e-01, -3.0305e-01, -2.8277e+00,  1.5071e-02,  2.3361e+00,\\n        -3.8259e-01, -3.2923e-01, -5.6894e-01,  7.5279e-01, -1.4578e+00,\\n        -1.9438e+00, -2.3516e+00, -4.1028e-02, -3.2798e-02, -5.7765e-01,\\n        -3.2116e-01, -2.5557e-01,  1.0580e-01,  4.3000e-01,  3.2293e-01,\\n         2.7354e-01,  6.9304e-01,  1.8769e+00, -3.9564e-01, -1.3200e+00,\\n         2.2713e-01,  4.8751e-01, -1.3580e+00, -3.8609e-01, -1.0239e+00,\\n        -1.4512e-01, -2.3120e-01,  4.3180e-01, -5.5675e-01, -7.9909e-01,\\n        -7.5038e-01,  8.4322e-01, -3.1666e-01,  3.4654e-01,  7.7588e-01,\\n         3.8141e-01, -2.0563e-01,  3.1469e-01, -8.0654e-01,  4.7067e-01,\\n         1.0281e+00, -4.8603e-01, -2.9031e+00,  4.1955e-01,  1.6954e+00,\\n        -9.4966e-01, -2.0211e-01, -5.6915e-02, -7.5391e-01,  5.8532e-01,\\n         1.3990e+00, -8.8408e-02,  1.9064e-01,  7.5883e-01,  2.4874e-03,\\n        -2.5647e-02,  2.1273e-01,  1.1395e+00, -5.0308e-01,  8.5667e-01,\\n         8.8261e-01,  8.0532e-01, -8.7556e-01, -1.4758e-01, -1.4905e-01,\\n         5.1957e-02, -9.1003e-01,  6.8148e-01, -5.4598e-01,  5.3370e-01,\\n         3.8573e-01,  3.3385e-02,  1.6973e-01, -1.8141e-01, -7.0944e-01,\\n        -1.2175e+00, -1.1221e+00,  1.2512e-01,  7.7090e-01, -5.4690e-01,\\n         9.2581e-01, -1.1107e-01,  7.5230e-01,  1.5444e+00, -1.1679e+00,\\n        -5.8597e-01,  1.0054e+00,  5.6257e-02,  4.0462e-01,  3.9183e-01,\\n         6.4785e-01,  1.2511e-02,  7.7656e-01,  1.0831e+00, -7.0587e-01,\\n        -1.8596e-01, -1.8027e+00, -6.2220e-01,  1.2311e+00, -1.7799e+00,\\n        -1.0808e+00, -6.7453e-01, -1.1141e-01,  8.4733e-02,  1.9948e-02,\\n        -4.0087e-01,  3.3899e-01,  8.1522e-01,  1.5621e+00, -7.0142e-01,\\n        -4.8223e-01, -6.6697e-01, -1.8925e-01,  1.1880e+00, -1.2273e+00,\\n         8.9999e-01,  5.9492e-01,  8.1438e-01,  1.5666e-02, -8.8533e-01,\\n         4.1776e-01, -6.4685e-01,  4.5221e-01,  2.5957e-02,  3.9510e-01,\\n        -8.5163e-01,  8.2882e-01,  9.5060e-01, -4.4255e-01, -9.2770e-01,\\n        -1.5975e+00,  2.9371e-01, -1.2315e+00, -8.7343e-01,  8.9428e-02,\\n         1.7647e-01, -7.4679e-01,  2.8629e+00, -4.5990e-01, -1.6549e-01,\\n         1.1021e+00, -1.6458e+00, -4.1318e-02,  2.1957e+00,  1.1366e+00,\\n         1.5442e-01, -1.3475e+00, -1.0809e+00,  1.2546e+00, -8.4330e-01,\\n         4.5855e-01, -4.8445e-02, -6.4026e-01, -2.1022e+00, -1.0389e-01,\\n        -1.2566e+00,  1.0140e+00,  4.5058e-01, -6.1631e-01, -8.6026e-01,\\n         1.2391e+00, -6.6977e-01,  7.2814e-01,  6.4342e-01, -7.0971e-01,\\n        -9.4501e-01,  7.4483e-01,  2.8113e-01,  1.3425e+00, -1.0013e+00,\\n        -9.8773e-01,  1.6508e+00,  1.0094e+00, -1.3246e+00,  4.7838e-02,\\n        -5.5132e-01,  1.8356e+00,  4.9750e-01,  8.4641e-01, -6.9083e-01,\\n        -4.2841e-01, -4.1163e-01,  6.7305e-01, -7.4259e-01, -3.8063e-01,\\n         1.4060e-01, -1.2704e-01, -4.7451e-01,  2.1874e-01,  1.1581e-01,\\n         4.9308e-01,  1.5480e-01,  4.7753e-01, -1.5080e+00,  1.0588e+00,\\n        -1.9669e+00,  2.1036e+00,  7.2395e-01, -6.3684e-01, -7.8241e-01,\\n         6.4944e-01,  2.8066e-03,  1.3208e+00, -1.5772e+00,  1.0723e+00,\\n        -8.2203e-01,  1.4328e+00, -2.3067e-01,  7.0783e-01, -4.4902e-01,\\n        -1.3776e+00, -4.4486e-01, -6.3538e-01,  2.1044e-01, -1.1066e+00,\\n         1.2276e+00, -1.0044e+00,  7.8033e-01, -1.0372e-01, -7.7795e-01,\\n        -8.6399e-01,  1.1657e+00, -7.7011e-01,  6.8821e-01, -2.3464e+00,\\n        -1.0903e+00,  1.0466e+00, -2.4827e-01,  8.8626e-01, -1.4259e+00,\\n        -1.1011e+00, -5.7526e-01,  8.8903e-01,  1.0949e+00, -8.0354e-01,\\n        -3.5087e-01, -5.7875e-02,  6.7720e-01, -8.5290e-02, -6.5205e-02,\\n        -3.7540e-01, -5.5354e-01, -2.8629e-01,  3.4709e-01, -8.3628e-01,\\n        -1.1885e+00,  1.1538e+00,  2.3160e-01, -1.4859e-01, -1.4890e+00,\\n         5.2647e-01,  6.6733e-01, -8.2302e-01,  3.9545e-01, -2.9798e-01,\\n        -7.5579e-01,  7.2871e-01, -3.2641e-01, -9.1883e-01, -4.0755e-01,\\n         1.5744e-01, -5.8806e-02, -6.9044e-01,  4.2294e-01, -1.4729e+00,\\n        -4.8012e-01, -1.0481e+00, -4.4776e-01, -2.7175e-01,  2.2290e-02,\\n        -2.1795e+00,  1.0271e+00, -7.6579e-01,  3.9064e-01,  7.7011e-01,\\n        -6.5713e-01,  2.2870e-01,  6.1872e-01, -2.0098e-01,  2.5962e-02,\\n        -1.5894e+00, -8.6020e-01, -1.8814e+00, -7.1663e-02,  9.1778e-01,\\n         9.2523e-02, -1.2062e+00, -1.1386e+00, -8.6978e-01,  1.0440e+00,\\n        -2.7735e+00,  1.2994e+00,  1.2769e+00, -2.4975e-01, -1.2188e+00,\\n        -5.0826e-01,  8.4310e-01,  8.4368e-01, -7.9602e-02], device='cuda:0',\\n       grad_fn=<SelectBackward>)\\n       \\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dir(m.blocks[0].multi_head_attention.heads[0].key))\n",
    "print(m.blocks[0].multi_head_attention.heads[0].key.type)\n",
    "print(m.blocks[0].multi_head_attention.heads[0].key.in_features)\n",
    "print(m.blocks[0].multi_head_attention.heads[0].key.out_features)\n",
    "print(m.blocks[0].multi_head_attention.heads[0].key.weight.shape)\n",
    "print(m.blocks[0].multi_head_attention.heads[0].key.weight[0].shape)\n",
    "#print(m.blocks[0].multi_head_attention.heads[0].key.weight[0])\n",
    "print(dir(m.token_into_embedding))\n",
    "print(dir(m.token_into_embedding.weight))\n",
    "print(m.token_into_embedding.weight.shape)\n",
    "print(m.token_into_embedding.weight[96])\n",
    "\n",
    "#Input: (∗), IntTensor or LongTensor of arbitrary shape containing the indices to extract\n",
    "\n",
    "#Output: (∗,H), where * is the input shape and =embedding_dim H=embedding_dim\n",
    "\n",
    "'''\n",
    "['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_name', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'bias', 'buffers', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'half', 'in_features', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'out_features', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'requires_grad_', 'reset_parameters', 'share_memory', 'state_dict', 'to', 'train', 'training', 'type', 'weight', 'zero_grad']\n",
    "<bound method Module.type of Linear(in_features=384, out_features=16, bias=False)>\n",
    "384\n",
    "16\n",
    "torch.Size([16, 384])\n",
    "torch.Size([384])\n",
    "['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_name', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'buffers', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'embedding_dim', 'eval', 'extra_repr', 'float', 'forward', 'from_pretrained', 'half', 'load_state_dict', 'max_norm', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'norm_type', 'num_embeddings', 'padding_idx', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'requires_grad_', 'reset_parameters', 'scale_grad_by_freq', 'share_memory', 'sparse', 'state_dict', 'to', 'train', 'training', 'type', 'weight', 'zero_grad']\n",
    "['T', '__abs__', '__add__', '__and__', '__array__', '__array_priority__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', '__cuda_array_interface__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__idiv__', '__ifloordiv__', '__ilshift__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__long__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rfloordiv__', '__rmul__', '__rpow__', '__rshift__', '__rsub__', '__rtruediv__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__torch_function__', '__truediv__', '__weakref__', '__xor__', '_backward_hooks', '_base', '_cdata', '_coalesced_', '_dimI', '_dimV', '_grad', '_grad_fn', '_indices', '_is_view', '_make_subclass', '_nnz', '_update_names', '_values', '_version', 'abs', 'abs_', 'absolute', 'absolute_', 'acos', 'acos_', 'acosh', 'acosh_', 'add', 'add_', 'addbmm', 'addbmm_', 'addcdiv', 'addcdiv_', 'addcmul', 'addcmul_', 'addmm', 'addmm_', 'addmv', 'addmv_', 'addr', 'addr_', 'align_as', 'align_to', 'all', 'allclose', 'amax', 'amin', 'angle', 'any', 'apply_', 'arccos', 'arccos_', 'arccosh', 'arccosh_', 'arcsin', 'arcsin_', 'arcsinh', 'arcsinh_', 'arctan', 'arctan_', 'arctanh', 'arctanh_', 'argmax', 'argmin', 'argsort', 'as_strided', 'as_strided_', 'as_subclass', 'asin', 'asin_', 'asinh', 'asinh_', 'atan', 'atan2', 'atan2_', 'atan_', 'atanh', 'atanh_', 'backward', 'baddbmm', 'baddbmm_', 'bernoulli', 'bernoulli_', 'bfloat16', 'bincount', 'bitwise_and', 'bitwise_and_', 'bitwise_not', 'bitwise_not_', 'bitwise_or', 'bitwise_or_', 'bitwise_xor', 'bitwise_xor_', 'bmm', 'bool', 'byte', 'cauchy_', 'ceil', 'ceil_', 'char', 'cholesky', 'cholesky_inverse', 'cholesky_solve', 'chunk', 'clamp', 'clamp_', 'clamp_max', 'clamp_max_', 'clamp_min', 'clamp_min_', 'clip', 'clip_', 'clone', 'coalesce', 'conj', 'contiguous', 'copy_', 'cos', 'cos_', 'cosh', 'cosh_', 'count_nonzero', 'cpu', 'cross', 'cuda', 'cummax', 'cummin', 'cumprod', 'cumsum', 'data', 'data_ptr', 'deg2rad', 'deg2rad_', 'dense_dim', 'dequantize', 'det', 'detach', 'detach_', 'device', 'diag', 'diag_embed', 'diagflat', 'diagonal', 'digamma', 'digamma_', 'dim', 'dist', 'div', 'div_', 'divide', 'divide_', 'dot', 'double', 'dtype', 'eig', 'element_size', 'eq', 'eq_', 'equal', 'erf', 'erf_', 'erfc', 'erfc_', 'erfinv', 'erfinv_', 'exp', 'exp2', 'exp2_', 'exp_', 'expand', 'expand_as', 'expm1', 'expm1_', 'exponential_', 'fft', 'fill_', 'fill_diagonal_', 'fix', 'fix_', 'flatten', 'flip', 'fliplr', 'flipud', 'float', 'floor', 'floor_', 'floor_divide', 'floor_divide_', 'fmod', 'fmod_', 'frac', 'frac_', 'gather', 'gcd', 'gcd_', 'ge', 'ge_', 'geometric_', 'geqrf', 'ger', 'get_device', 'grad', 'grad_fn', 'greater', 'greater_', 'greater_equal', 'greater_equal_', 'gt', 'gt_', 'half', 'hardshrink', 'has_names', 'heaviside', 'heaviside_', 'histc', 'hypot', 'hypot_', 'i0', 'i0_', 'ifft', 'imag', 'index_add', 'index_add_', 'index_copy', 'index_copy_', 'index_fill', 'index_fill_', 'index_put', 'index_put_', 'index_select', 'indices', 'int', 'int_repr', 'inverse', 'irfft', 'is_coalesced', 'is_complex', 'is_contiguous', 'is_cuda', 'is_distributed', 'is_floating_point', 'is_leaf', 'is_meta', 'is_mkldnn', 'is_nonzero', 'is_pinned', 'is_quantized', 'is_same_size', 'is_set_to', 'is_shared', 'is_signed', 'is_sparse', 'isclose', 'isfinite', 'isinf', 'isnan', 'isneginf', 'isposinf', 'isreal', 'istft', 'item', 'kthvalue', 'layout', 'lcm', 'lcm_', 'le', 'le_', 'lerp', 'lerp_', 'less', 'less_', 'less_equal', 'less_equal_', 'lgamma', 'lgamma_', 'log', 'log10', 'log10_', 'log1p', 'log1p_', 'log2', 'log2_', 'log_', 'log_normal_', 'log_softmax', 'logaddexp', 'logaddexp2', 'logcumsumexp', 'logdet', 'logical_and', 'logical_and_', 'logical_not', 'logical_not_', 'logical_or', 'logical_or_', 'logical_xor', 'logical_xor_', 'logit', 'logit_', 'logsumexp', 'long', 'lstsq', 'lt', 'lt_', 'lu', 'lu_solve', 'map2_', 'map_', 'masked_fill', 'masked_fill_', 'masked_scatter', 'masked_scatter_', 'masked_select', 'matmul', 'matrix_exp', 'matrix_power', 'max', 'maximum', 'mean', 'median', 'min', 'minimum', 'mm', 'mode', 'movedim', 'mul', 'mul_', 'multinomial', 'multiply', 'multiply_', 'mv', 'mvlgamma', 'mvlgamma_', 'name', 'names', 'nanquantile', 'nansum', 'narrow', 'narrow_copy', 'ndim', 'ndimension', 'ne', 'ne_', 'neg', 'neg_', 'negative', 'negative_', 'nelement', 'new', 'new_empty', 'new_full', 'new_ones', 'new_tensor', 'new_zeros', 'nextafter', 'nextafter_', 'nonzero', 'norm', 'normal_', 'not_equal', 'not_equal_', 'numel', 'numpy', 'orgqr', 'ormqr', 'outer', 'output_nr', 'permute', 'pin_memory', 'pinverse', 'polygamma', 'polygamma_', 'pow', 'pow_', 'prelu', 'prod', 'put_', 'q_per_channel_axis', 'q_per_channel_scales', 'q_per_channel_zero_points', 'q_scale', 'q_zero_point', 'qr', 'qscheme', 'quantile', 'rad2deg', 'rad2deg_', 'random_', 'real', 'reciprocal', 'reciprocal_', 'record_stream', 'refine_names', 'register_hook', 'reinforce', 'relu', 'relu_', 'remainder', 'remainder_', 'rename', 'rename_', 'renorm', 'renorm_', 'repeat', 'repeat_interleave', 'requires_grad', 'requires_grad_', 'reshape', 'reshape_as', 'resize', 'resize_', 'resize_as', 'resize_as_', 'retain_grad', 'rfft', 'roll', 'rot90', 'round', 'round_', 'rsqrt', 'rsqrt_', 'scatter', 'scatter_', 'scatter_add', 'scatter_add_', 'select', 'set_', 'sgn', 'sgn_', 'shape', 'share_memory_', 'short', 'sigmoid', 'sigmoid_', 'sign', 'sign_', 'signbit', 'sin', 'sin_', 'sinh', 'sinh_', 'size', 'slogdet', 'smm', 'softmax', 'solve', 'sort', 'sparse_dim', 'sparse_mask', 'sparse_resize_', 'sparse_resize_and_clear_', 'split', 'split_with_sizes', 'sqrt', 'sqrt_', 'square', 'square_', 'squeeze', 'squeeze_', 'sspaddmm', 'std', 'stft', 'storage', 'storage_offset', 'storage_type', 'stride', 'sub', 'sub_', 'subtract', 'subtract_', 'sum', 'sum_to_size', 'svd', 'symeig', 't', 't_', 'take', 'tan', 'tan_', 'tanh', 'tanh_', 'to', 'to_dense', 'to_mkldnn', 'to_sparse', 'tolist', 'topk', 'trace', 'transpose', 'transpose_', 'triangular_solve', 'tril', 'tril_', 'triu', 'triu_', 'true_divide', 'true_divide_', 'trunc', 'trunc_', 'type', 'type_as', 'unbind', 'unflatten', 'unfold', 'uniform_', 'unique', 'unique_consecutive', 'unsafe_chunk', 'unsafe_split', 'unsafe_split_with_sizes', 'unsqueeze', 'unsqueeze_', 'values', 'var', 'vdot', 'view', 'view_as', 'where', 'zero_']\n",
    "torch.Size([97, 384])\n",
    "tensor([-1.3664e-01, -4.0590e-01, -2.2219e-01,  5.3633e-01,  1.3847e+00,\n",
    "         6.0940e-01,  1.0424e+00,  1.1116e+00, -2.6177e-01, -1.1758e-01,\n",
    "        -8.0097e-01,  3.9556e-01,  1.8125e-01,  4.5105e-01,  2.7599e-01,\n",
    "         5.1245e-01,  5.1657e-01,  1.4031e+00, -6.5616e-01,  3.6901e-01,\n",
    "         7.3739e-01, -2.9283e-01, -1.2868e+00, -7.9836e-01, -2.4348e-01,\n",
    "        -4.3813e-01,  3.0676e-01,  8.2756e-01, -1.9864e-01, -4.6908e-01,\n",
    "         4.0065e-01,  2.0083e+00, -5.0702e-01,  7.8803e-01, -9.9201e-01,\n",
    "         2.9955e-01, -1.2661e-01,  6.3483e-02,  5.6666e-01,  5.4232e-01,\n",
    "         9.1668e-01, -1.0347e-01,  1.1669e+00,  1.8649e+00,  5.9467e-02,\n",
    "         8.1702e-01, -4.9200e-01,  8.2695e-01, -9.3656e-01, -7.9871e-01,\n",
    "         1.0738e+00, -4.7494e-01, -7.2439e-01, -2.2799e-01,  3.0416e-01,\n",
    "         1.1063e+00, -4.2677e-01,  6.1318e-01,  1.3603e+00, -1.1408e+00,\n",
    "        -7.3598e-01, -7.5957e-01, -6.9394e-02, -2.5286e-01,  4.9853e-01,\n",
    "         4.6131e-01, -6.1774e-01,  7.9585e-01,  4.2797e-01, -1.7809e-01,\n",
    "        -1.9401e+00, -5.5100e-01,  2.6329e-01,  1.1866e+00, -5.6975e-01,\n",
    "        -1.1178e-01, -5.2068e-02,  1.3399e+00,  1.2692e-01, -1.9369e+00,\n",
    "        -1.5599e-01, -3.0305e-01, -2.8277e+00,  1.5071e-02,  2.3361e+00,\n",
    "        -3.8259e-01, -3.2923e-01, -5.6894e-01,  7.5279e-01, -1.4578e+00,\n",
    "        -1.9438e+00, -2.3516e+00, -4.1028e-02, -3.2798e-02, -5.7765e-01,\n",
    "        -3.2116e-01, -2.5557e-01,  1.0580e-01,  4.3000e-01,  3.2293e-01,\n",
    "         2.7354e-01,  6.9304e-01,  1.8769e+00, -3.9564e-01, -1.3200e+00,\n",
    "         2.2713e-01,  4.8751e-01, -1.3580e+00, -3.8609e-01, -1.0239e+00,\n",
    "        -1.4512e-01, -2.3120e-01,  4.3180e-01, -5.5675e-01, -7.9909e-01,\n",
    "        -7.5038e-01,  8.4322e-01, -3.1666e-01,  3.4654e-01,  7.7588e-01,\n",
    "         3.8141e-01, -2.0563e-01,  3.1469e-01, -8.0654e-01,  4.7067e-01,\n",
    "         1.0281e+00, -4.8603e-01, -2.9031e+00,  4.1955e-01,  1.6954e+00,\n",
    "        -9.4966e-01, -2.0211e-01, -5.6915e-02, -7.5391e-01,  5.8532e-01,\n",
    "         1.3990e+00, -8.8408e-02,  1.9064e-01,  7.5883e-01,  2.4874e-03,\n",
    "        -2.5647e-02,  2.1273e-01,  1.1395e+00, -5.0308e-01,  8.5667e-01,\n",
    "         8.8261e-01,  8.0532e-01, -8.7556e-01, -1.4758e-01, -1.4905e-01,\n",
    "         5.1957e-02, -9.1003e-01,  6.8148e-01, -5.4598e-01,  5.3370e-01,\n",
    "         3.8573e-01,  3.3385e-02,  1.6973e-01, -1.8141e-01, -7.0944e-01,\n",
    "        -1.2175e+00, -1.1221e+00,  1.2512e-01,  7.7090e-01, -5.4690e-01,\n",
    "         9.2581e-01, -1.1107e-01,  7.5230e-01,  1.5444e+00, -1.1679e+00,\n",
    "        -5.8597e-01,  1.0054e+00,  5.6257e-02,  4.0462e-01,  3.9183e-01,\n",
    "         6.4785e-01,  1.2511e-02,  7.7656e-01,  1.0831e+00, -7.0587e-01,\n",
    "        -1.8596e-01, -1.8027e+00, -6.2220e-01,  1.2311e+00, -1.7799e+00,\n",
    "        -1.0808e+00, -6.7453e-01, -1.1141e-01,  8.4733e-02,  1.9948e-02,\n",
    "        -4.0087e-01,  3.3899e-01,  8.1522e-01,  1.5621e+00, -7.0142e-01,\n",
    "        -4.8223e-01, -6.6697e-01, -1.8925e-01,  1.1880e+00, -1.2273e+00,\n",
    "         8.9999e-01,  5.9492e-01,  8.1438e-01,  1.5666e-02, -8.8533e-01,\n",
    "         4.1776e-01, -6.4685e-01,  4.5221e-01,  2.5957e-02,  3.9510e-01,\n",
    "        -8.5163e-01,  8.2882e-01,  9.5060e-01, -4.4255e-01, -9.2770e-01,\n",
    "        -1.5975e+00,  2.9371e-01, -1.2315e+00, -8.7343e-01,  8.9428e-02,\n",
    "         1.7647e-01, -7.4679e-01,  2.8629e+00, -4.5990e-01, -1.6549e-01,\n",
    "         1.1021e+00, -1.6458e+00, -4.1318e-02,  2.1957e+00,  1.1366e+00,\n",
    "         1.5442e-01, -1.3475e+00, -1.0809e+00,  1.2546e+00, -8.4330e-01,\n",
    "         4.5855e-01, -4.8445e-02, -6.4026e-01, -2.1022e+00, -1.0389e-01,\n",
    "        -1.2566e+00,  1.0140e+00,  4.5058e-01, -6.1631e-01, -8.6026e-01,\n",
    "         1.2391e+00, -6.6977e-01,  7.2814e-01,  6.4342e-01, -7.0971e-01,\n",
    "        -9.4501e-01,  7.4483e-01,  2.8113e-01,  1.3425e+00, -1.0013e+00,\n",
    "        -9.8773e-01,  1.6508e+00,  1.0094e+00, -1.3246e+00,  4.7838e-02,\n",
    "        -5.5132e-01,  1.8356e+00,  4.9750e-01,  8.4641e-01, -6.9083e-01,\n",
    "        -4.2841e-01, -4.1163e-01,  6.7305e-01, -7.4259e-01, -3.8063e-01,\n",
    "         1.4060e-01, -1.2704e-01, -4.7451e-01,  2.1874e-01,  1.1581e-01,\n",
    "         4.9308e-01,  1.5480e-01,  4.7753e-01, -1.5080e+00,  1.0588e+00,\n",
    "        -1.9669e+00,  2.1036e+00,  7.2395e-01, -6.3684e-01, -7.8241e-01,\n",
    "         6.4944e-01,  2.8066e-03,  1.3208e+00, -1.5772e+00,  1.0723e+00,\n",
    "        -8.2203e-01,  1.4328e+00, -2.3067e-01,  7.0783e-01, -4.4902e-01,\n",
    "        -1.3776e+00, -4.4486e-01, -6.3538e-01,  2.1044e-01, -1.1066e+00,\n",
    "         1.2276e+00, -1.0044e+00,  7.8033e-01, -1.0372e-01, -7.7795e-01,\n",
    "        -8.6399e-01,  1.1657e+00, -7.7011e-01,  6.8821e-01, -2.3464e+00,\n",
    "        -1.0903e+00,  1.0466e+00, -2.4827e-01,  8.8626e-01, -1.4259e+00,\n",
    "        -1.1011e+00, -5.7526e-01,  8.8903e-01,  1.0949e+00, -8.0354e-01,\n",
    "        -3.5087e-01, -5.7875e-02,  6.7720e-01, -8.5290e-02, -6.5205e-02,\n",
    "        -3.7540e-01, -5.5354e-01, -2.8629e-01,  3.4709e-01, -8.3628e-01,\n",
    "        -1.1885e+00,  1.1538e+00,  2.3160e-01, -1.4859e-01, -1.4890e+00,\n",
    "         5.2647e-01,  6.6733e-01, -8.2302e-01,  3.9545e-01, -2.9798e-01,\n",
    "        -7.5579e-01,  7.2871e-01, -3.2641e-01, -9.1883e-01, -4.0755e-01,\n",
    "         1.5744e-01, -5.8806e-02, -6.9044e-01,  4.2294e-01, -1.4729e+00,\n",
    "        -4.8012e-01, -1.0481e+00, -4.4776e-01, -2.7175e-01,  2.2290e-02,\n",
    "        -2.1795e+00,  1.0271e+00, -7.6579e-01,  3.9064e-01,  7.7011e-01,\n",
    "        -6.5713e-01,  2.2870e-01,  6.1872e-01, -2.0098e-01,  2.5962e-02,\n",
    "        -1.5894e+00, -8.6020e-01, -1.8814e+00, -7.1663e-02,  9.1778e-01,\n",
    "         9.2523e-02, -1.2062e+00, -1.1386e+00, -8.6978e-01,  1.0440e+00,\n",
    "        -2.7735e+00,  1.2994e+00,  1.2769e+00, -2.4975e-01, -1.2188e+00,\n",
    "        -5.0826e-01,  8.4310e-01,  8.4368e-01, -7.9602e-02], device='cuda:0',\n",
    "       grad_fn=<SelectBackward>)\n",
    "       \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40a83412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# low 6.689373 M parameters\\n\\n# hyperparameters\\nbatch_size = 8 # how many independent sequences will we process in parallel?\\nblock_size = 512 # what is the maximum input context length for predictions?\\nmax_iters = 5000\\neval_interval = 500\\nlearning_rate = 3e-4\\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\\neval_iters = 200\\nn_embedding = 128 # 32 =  head_size = n_embedding / n_head\\nn_head = 8\\nn_layer = 20\\ndropout = 0.2\\n# ------------\\n\\n#torch.manual_seed(1337)\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# low 6.689373 M parameters\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 8 # how many independent sequences will we process in parallel?\n",
    "block_size = 512 # what is the maximum input context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embedding = 128 # 32 =  head_size = n_embedding / n_head\n",
    "n_head = 8\n",
    "n_layer = 20\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "#torch.manual_seed(1337)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f6d3e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.888993 M parameters\n",
      "step 0: train loss 0.0834, val loss 2.8108\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.89 GiB total capacity; 9.87 GiB already allocated; 39.12 MiB free; 9.87 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# evaluate the loss\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "Cell \u001b[0;32mIn[10], line 145\u001b[0m, in \u001b[0;36mGPT2.forward\u001b[0;34m(self, id_number_of_vector_x, targets)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m#x = tok_emb + pos_emb # (B,T,C)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m#x = self.blocks(x) # (B,T,C)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m#x = self.ln_f(x) # (B,T,C)\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m#x = self.sa_head(x) # (B,T,vocab_size)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m#x = self.ffwd(x)\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m#logits = self.lm_head(x) # (B,T,vocab_size)\u001b[39;00m\n\u001b[1;32m    144\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \u001b[38;5;66;03m# (B,T,C) + pos_emb across the batch\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m    146\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m#x = self.sa_head(x) # (B,T,vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:117\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "Cell \u001b[0;32mIn[10], line 94\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 94\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm_2(x))\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "Cell \u001b[0;32mIn[10], line 62\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 62\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h(x) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(out))\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[10], line 62\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 62\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(out))\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "Cell \u001b[0;32mIn[10], line 46\u001b[0m, in \u001b[0;36mHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m attention_part \u001b[38;5;241m=\u001b[39m attention_part\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtril[:T, :T] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m attention_part \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attention_part, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m attention_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_part\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# perform the weighted aggregation of the values\u001b[39;00m\n\u001b[1;32m     48\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:983\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.\u001b[39m:\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    980\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (_VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training)\n\u001b[1;32m    982\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inplace\n\u001b[0;32m--> 983\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.89 GiB total capacity; 9.87 GiB already allocated; 39.12 MiB free; 9.87 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=17e-5)\n",
    "\n",
    "for iter in range(50000):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"====result_loss====\")\n",
    "print(loss.item())\n",
    "\n",
    "#torch.save(m.state_dict(), \"./GPT2_Shakespeare\")\n",
    "#torch.save(m.state_dict(), \"./GPT2_7_habits\")\n",
    "torch.save(m.state_dict(), \"./GPT2_rich_dad_poor_dad_Fine-tuning_with_Custom_Datasets\")\n",
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long,device=device), max_new_tokens=1000)[0].tolist()))\n",
    "\n",
    "'''\n",
    "#torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key  = nn.Linear(C,head_size,bias=False)\n",
    "query = nn.Linear(C,head_size,bias=False)\n",
    "value = nn.Linear(C,head_size,bias=False)\n",
    "\n",
    "k = key(x) #(B,T,16)\n",
    "q = query(x) #(B,T,16)\n",
    "v = value(x)\n",
    "\n",
    "wei = q @ k.transpose(-2,-1) # (B,T,16) @ (B,16,T) => (B,T,T)\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0,float('-inf'))\n",
    "wei = F.softmax(wei,dim=-1)\n",
    "\n",
    "output = wei @ v\n",
    "output.shape\n",
    "wei[0]\n",
    "\n",
    "\n",
    "\n",
    "torch.softmax(torch.tensor([0.1,-0.2,0.3,-0.2,0.5]),dim=-1)\n",
    "torch.softmax(torch.tensor([0.1,-0.2,0.3,-0.2,0.5])*8,dim=-1)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long,device=device), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eecff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"How to get rich fast\"\n",
    "context = torch.tensor( [ encode(text) ], dtype=torch.long, device=device )\n",
    "print(decode(m.generate( context, max_new_tokens=1000 )[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a55d768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to get rich faster, I suggested that the interpretation of what my highest was born and \n",
      "pause the problem. My flow dad I could not have the credit with the bank to buy another house, since he was \n",
      "almost skills. They need $7,000 at the difference between way. \n",
      "\n",
      "The extent of the problem was the 60 years of the three generally thried with his \n",
      "6 country went to enjoy in the 303es. \n",
      "\n",
      "\"Look, there is days always gone?\" I asked myself goals to a start they listening nothing to manage. \n",
      "\n",
      "Rich dad guestors for a pattern. But I was the other and his life went to riding good grades, and they not only to \n",
      "line out. In other words, most people because they would encourage to money to go because they \n",
      "want to love them with the most pain forward. They were created, dealing long \n",
      "his neither words. \n",
      "\n",
      "The wall today, there were antimal panic is most important. Give memore than our \n",
      "moods and the weaknesses of other people. They think simply genuinely \n",
      "happy for the masses being getting some and soon often working\n"
     ]
    }
   ],
   "source": [
    "text = \"How to get rich fast\"\n",
    "context = torch.tensor( [ encode(text) ], dtype=torch.long, device=device )\n",
    "print(decode(m.generate( context, max_new_tokens=1000 )[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2f779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa8d7e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is what world famous speaker and author Anthony Robbins says about Robert’s work. \n",
      "\n",
      "“Robert Kiyosaki’s work in education he’s not payer,” I said with a sly grin. \n",
      "\n",
      "“Look, I guess for you to have to be job, better smarter. Then you are the money, instead of being an \n",
      "little changes that there are no recal today. There are more and the things \n",
      "\n",
      "Power \n",
      "\n",
      "\"Isea, we walk you to think school. You something something kind of the second doing to these \n",
      "committed-the boy.\" \n",
      "\n",
      "\"Id yes?\" \n",
      "\n",
      "\"No, not really. Dad, I don't turn you with me to the schoolists trainked in the happy marriage,\" I say. \n",
      "\"Sound you panice the game off the things first go \n",
      "net into opening. You cannot pretend your standard of listening in your life and to be \n",
      "influenced. \n",
      "\n",
      "When you listen, you have a profession and will be wealthy promoted law. But, as a result, \n",
      "communication is problemed, correctly inspired or over and interpreted in loyalty to old these \n",
      "methods may be limited in purpose and even body the subtial force. It may be very \n",
      "different frequential, areas if your friends would be involved in your mind, in your \n",
      "both the marriage job and the regions of the three goose. \n",
      "\n",
      "My friend was asked him when he explored his working hard all his been a goor had to play it safe. \n",
      "The real estate is too only and torture differently. The boys I worked with me into \n",
      "my heart was weakness, and the course, I think my attention can ready for better them. Besides, my \n",
      "instead of thinking, the world is foound that encountry you and seek first to understand. That's Pale It \n",
      "was before. A great pressure to get by pieces. Although I paid myself turned \n",
      "on from admitMas, and they had been taught to pay it to the \n",
      "principles of cons her. S\n",
      "\n",
      "Pend it give it some ambiguity self. There was no sense to both getting more frundamental \n",
      "than sources of any particular groups of interdependent in their employer. \n",
      "\n",
      "The Constitution worked in the middle of the other person, there are friends who are not a driven \n",
      "because between involved in a principle center. It is also a distatement, a child an employee with a \n",
      "source of irritation, a stumbling block, and wishing it comes to money. Matterion is \n",
      "like going to the store managers of that trust in the subject. \n",
      "\n",
      "The middle class is trying to read with the synergistic conversation. \n",
      "\n",
      "Most people have learned or an loving the real key to investment it. By the appying job, I was like every \n",
      "day to walk me in his office and spending. While he had well hammered look out, and why we read the \n",
      "supermarket house or a small company expenses and self-worth solutions and \n",
      "feelings. The common occasionally have you not the power to experience your problems and \n",
      "concerns. That process are a safer making greater, writer do some script. \n",
      "\n",
      "When you Begin with the End in Mind often determines your days are often a regular basis, \n",
      "which making relationships in the content of independent will, your limage, and would benefit \n",
      "and substane because are and management in each other in \n",
      "parts and aids. \n",
      "\n",
      "There are the change that to our chases for me to out there and methods of influence that \n",
      "matters, they have reflect with some of us hard many. It's suggestionstication, it's my office \n",
      "to method of happy and to inte take the construction of a home with a big problem. \n",
      "\n",
      "I considered the forundation of a year-win being that the man's problem circumstances, and there's form a \n",
      "\"Boy, and that experience.\" \n",
      "\n",
      "I had intrined View Vietnam president that made a president of 80 percent of the population, and \n",
      "straight to addiving the four life-suppoilor tring toward the Indian force-upting getting some and \n",
      "asked them Balance Sheet. \n",
      "\n",
      "His patent of First obst creation, and then you live the seasoning has you not greatered the \n",
      "golden egg. \n",
      "\n",
      "I this norrowing all that time, increased it was to get out of the “Rat Race,” for the other stacket \n",
      "on the “Fast Track”. As I have said, so much people about the source of power, it became other \n",
      "environment. I say that the confession I have way to live. \n",
      "\n",
      "I don’t want to pay the mortgage, the rich almost every poor on in the stock. The \n",
      "problem was that Bank on thinking and the other people never see the way we see other woman. I \n",
      "had a new problem on the one-win particular unisting the office in management is at the office \n",
      "will, beyond unanticipated productive, conability, and positive and huge integrity you and work together \n",
      "to see yourself for structure, and then to be doing something in your boss's \n",
      "Circleof Influence is logical/verb. \n",
      "\n",
      "Clearing the money will solve all you have that income definition of working four things you may make \n",
      "because heroes it create the fear of small. \n",
      "\n",
      "The Centers In the short new, suppose to are win for most people to avoid the funeral win-lose \n",
      "does not knowing the difference between an asset and a liability. \n",
      "\n",
      "Moreover, not major people and the technical, skills and get up and skill, but they can \n",
      "fail to trust them back to disappear. They \n",
      "look at their dreamatoches in perception as a child and they work for them be, a day come and to the \n",
      "government. The next time sheet prove in their home. \n",
      "\n",
      "What is the character? They searching form their motives taxes, and increase, They become defensive, \n",
      "and the seizing of thinking that advented the school. They instead of taking the time to admit \n",
      "payments with their first livejumn, take those who have allow to done I make my offer. I think I \n",
      "know there was a 90 percent who run to work with have your family, and then I sure that they \n",
      "fear of them right and fear and develop their sense of power. Or you are ever destiny \n",
      "that something elsements outside found in the funeral process of written family, compassion, and \n",
      "help you understand the materials, the will be able to truly everything in your life. \n",
      "\n",
      "An important meeti ng when the suburbs, or a yonge employees keep grades up, go to work, people became \n",
      "like buck they want to achieve them. Problems would do not be are interested in getting a different \n",
      "are just to work and get on more money. So they can afford their sons that average form only \n",
      "in real ways. \n",
      "\n",
      "Page 78/114 \n",
      "\n",
      "\n",
      "http : / motsach . info \n",
      "\n",
      "\n",
      "Rich Dad Poor Dad \n",
      "\n",
      "\n",
      "Robert T. Kiyosaki \n",
      "\n",
      "\n",
      "“Choosing our thoughts?” Mike asked, put the different spingle way were the old woman \n",
      "I understand the simple leadership. The script was the Joney and Madericy was a man home of the \n",
      "time Management in the middle of the month. First about and told him we agreed that shown these \n",
      "characters at top foten their mouth. We seem to interpret in some conduction to and help a person \n",
      "groups of people to achieve results. \n",
      "\n",
      "Because seeking to understand how we are going to throw the exercise present of working on the \n",
      "stomes agon, asked them when I was the only received to keep a book, I did not say the \n",
      "promise of the greater fundamental mate, the income increased your spouse, \n",
      "on the context of the weekly board with the employees go up. There things where there eroging \n",
      "\n",
      "\n",
      "\n",
      "THE SEVEN HABHS OF HIGHLY EFFECTIVE PEOPLE \n",
      "\n",
      "\n",
      "Brcu^t toyu tyH^Havt \n",
      "\n",
      "\n",
      "empowers us to create customer or against whicher, the immediate reposed concires \n",
      "that are by the underlying shopping incope is full to creational ambity. There's a bigger house \n",
      "come extended and the sixth tree stack. The course of study too increase it, the tool extent that \n",
      "muture their metaphy, writing and families and insights that are illegal. \n",
      "\n",
      "I wonderful personal stages are courage, as the standard thought counterproductive represents. \n",
      "\n",
      "I have had to get the problem in managing a prioristy skyle of like programs, bringing and continued \n",
      "experiences, there is a transformer where never quate to draw for short of help. \n",
      "\n",
      "Pe seemed at the leaves of experience room, contains heroes for me, and there is also the whole is \n",
      "applicable. There is a great habit of success in the “Forment Georden Process?” The only woman \n",
      "have experienced on their synergy -- how, in really makes their deducity and \n",
      "in working with their may involvement with simply changing their jobs and desire of \n",
      "investment in their groups. It is also begin to realize this to ouch first seeking to understand \n",
      "of each home is uniqueto another person. A swing manager, or a paradigm of attempt. The traditional \n",
      "conscience, it has been a combinanced of developing an effective presentation and the other is \n",
      "problem. \n",
      "\n",
      "This Perception Centeredness. It was a cold saying, \"if only to learn somewhere this golden \n",
      "grades that count.\" In the mids of human interaction, management, and the \"security or independent \n",
      "personal lives, which give meaning to timeleve and to livether integrity and commitments \n",
      "of interdependency and effectiveness te life-one. It's talents, and it's a territ of Principle-Centered, it based on \n",
      "control principles and interpersonal growth. \n",
      "\n",
      "- M arge Public Victory \n",
      "\n",
      "Third, author of Peak Performan \n",
      "\n",
      "The Seven Habits of Higher Egypt and People \n",
      "Communicate rein Habits 4, asked them if every qualner had “give me and those it.\" They won't be alive the \n",
      "stephn -- getting and then from line or when they really listen with it be? What they can look at things it \n",
      "\n",
      "\n",
      "\n",
      "THE SEVEN HABHS OF HIGHLY EFFECTIVE PEOPLE \n",
      "\n",
      "\n",
      "Brwcft to you tyHyHsnt \n",
      "\n",
      "\n",
      "application of \"Begin with them.\" With the beginning and reality to other person as well. \n",
      "\n",
      "Now in other people are by the Creation of it. Accountability is really the other weakness, or \n",
      "acqui instead of the first creation, the territory is a dramatic choice for may were ready to buy \n",
      "also. Mutual funds are the foundation of trust and defits just the first, the family thing underlying creative \n",
      "problems and is what the Choice of actord enation. That third generation do not change, the \n",
      "challes becomes hour constant, and the build of cash, graduates. \n",
      "\n",
      "In the words of accountability, such as many problems began the truthful in the vision you \n",
      "necessary for other ego i n the suburbs. \n",
      "\n",
      "I often each of these encourroged adults to compensate for the line: \"What happens to us go understand \n",
      "how the makes will admit,\" he told him that the highest paid soft parents will go on forever. \n",
      "\n",
      "So He was\n"
     ]
    }
   ],
   "source": [
    "# not working because of the small size of the LLM\n",
    "text = \"This is what world famous speaker and author Anthony Robbins says about Robert’s work. \"\n",
    "context = torch.tensor( [ encode(text) ], dtype=torch.long, device=device )\n",
    "print(decode(m.generate( context, max_new_tokens=10000 )[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b435fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. linear neural network learning the price, w is the price, x is the amount, the loss is the total price\n",
    "what is the GPT learning\n",
    "w is [not known, maybe key, query, value], x is the word chain, the loss is the next char of the word chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff50fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
