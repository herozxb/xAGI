{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6086c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import imageio\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from run_nerf_helpers import *\n",
    "\n",
    "from load_llff import load_llff_data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "np.random.seed(0)\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9816287f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2097152"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32768*  64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef60a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takeaway function and not decompose it\n",
    "def batchify(fn, chunk):\n",
    "    \"\"\"Constructs a version of 'fn' that applies to smaller batches.\n",
    "    \"\"\"\n",
    "    if chunk is None:\n",
    "        return fn\n",
    "    def ret(inputs):\n",
    "        #print(chunk) #65536\n",
    "        #print(inputs.shape[0]) #4194304\n",
    "        #print(range(0, inputs.shape[0], chunk)) #range(0, 4194304, 65536)  4194304/65536 = 64\n",
    "        \n",
    "        #65536\n",
    "        #2097152\n",
    "        #range(0, 2097152, 65536) 2097152/65536 = 32\n",
    "        \n",
    "        #fn(inputs[i:i+chunk]).shape = torch.Size([65536, 4])\n",
    "        #print(fn)\n",
    "        #NeRF(\n",
    "        #  (pts_linears): ModuleList(\n",
    "        #    (0): Linear(in_features=63, out_features=256, bias=True)\n",
    "        #    (1): Linear(in_features=256, out_features=256, bias=True)\n",
    "        #    (2): Linear(in_features=256, out_features=256, bias=True)\n",
    "        #    (3): Linear(in_features=256, out_features=256, bias=True)\n",
    "        #    (4): Linear(in_features=256, out_features=256, bias=True)\n",
    "        #    (5): Linear(in_features=319, out_features=256, bias=True)\n",
    "        #    (6): Linear(in_features=256, out_features=256, bias=True)\n",
    "        #    (7): Linear(in_features=256, out_features=256, bias=True)\n",
    "        #  )\n",
    "        #  (views_linears): ModuleList(\n",
    "        #    (0): Linear(in_features=283, out_features=128, bias=True)\n",
    "        #  )\n",
    "        #  (feature_linear): Linear(in_features=256, out_features=256, bias=True)\n",
    "        #  (alpha_linear): Linear(in_features=256, out_features=1, bias=True)\n",
    "        #  (rgb_linear): Linear(in_features=128, out_features=3, bias=True)\n",
    "        #)\n",
    "        \n",
    "        #print([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)])\n",
    "        \n",
    "        return torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
    "    return ret\n",
    "\n",
    "def run_network(inputs, view_direction, fn, embed_fn, embeddirs_fn, netchunk=1024*64):\n",
    "    \"\"\"Prepares inputs and applies network 'fn'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # [DONE] fn = model\n",
    "    # [NOT] pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples, 3]\n",
    "    \n",
    "    #3\n",
    "    #[-1, 3]\n",
    "    # print(inputs.shape[-1]) # 3\n",
    "    # print([-1, inputs.shape[-1]]) # [-1, 3]\n",
    "    # print(inputs.shape) #     torch.Size([32768, 64, 3])\n",
    "    # print(inputs_flat.shape) #torch.Size([2097152, 3])    \n",
    "\n",
    "\n",
    "    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]]) # -1 is the last one\n",
    "\n",
    "    # [NOT]  embad the position\n",
    "    \n",
    "    # print(embed_fn) #[torch.sin, torch.cos]\n",
    "    embedded = embed_fn(inputs_flat)\n",
    "    # print(embedded.shape) #torch.Size([2097152, 63]) torch.Size([4194304, 63])\n",
    "    #\n",
    "    \n",
    "    \n",
    "    if view_direction is not None:\n",
    "        # print(view_direction.shape)         # torch.Size([32768, 3])\n",
    "        # print(view_direction[:,None].shape) # torch.Size([32768, 1, 3])\n",
    "        # print(inputs.shape)                 # torch.Size([32768, 64, 3])\n",
    "        input_dirs = view_direction[:,None].expand(inputs.shape)\n",
    "        #print(input_dirs.shape)              # torch.Size([32768, 64, 3])\n",
    "        input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n",
    "        #print(input_dirs_flat.shape)         # torch.Size([2097152, 3])  32768 * 64 = 2097152\n",
    "        # print(embeddirs_fn)                 # [torch.sin, torch.cos]\n",
    "        embedded_dirs = embeddirs_fn(input_dirs_flat)\n",
    "        # print(embedded.shape)               # torch.Size([2097152, 63])\n",
    "        # print(embedded_dirs.shape)          # torch.Size([65536, 27])\n",
    "        embedded = torch.cat([embedded, embedded_dirs], -1)\n",
    "        # print(embedded.shape)               # torch.Size([65536, 90])\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    outputs_flat = batchify(fn, netchunk)(embedded)\n",
    "    # print(outputs_flat.shape)               # torch.Size([2097152, 4]) 4 is rgb + alpha\n",
    "    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n",
    "    # print(list(inputs.shape))                                    # [32768, 64, 3]\n",
    "    # print(list(inputs.shape[:-1]))                               # [32768, 64]\n",
    "    # print([outputs_flat.shape[-1]])                              # [4]\n",
    "    # print(list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])    # [32768, 64, 4]\n",
    "    # print(outputs.shape)                                         # torch.Size([32768, 64, 4])\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def batchify_rays(rays_flat, chunk=1024*32, **kwargs):\n",
    "    \"\"\"Render rays in smaller minibatches to avoid OOM.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_ret = {}\n",
    "    # range(0, 190512, 32768)\n",
    "    print(range(0, rays_flat.shape[0], chunk))\n",
    "    for i in range(0, rays_flat.shape[0], chunk):\n",
    "        ret = render_rays(rays_flat[i:i+chunk], **kwargs)\n",
    "        # print(ret)\n",
    "        # {\n",
    "        # 'rgb_map': tensor([[0.1997, 0.2570, 0.1611],\n",
    "        # [0.0614, 0.0617, 0.0618],\n",
    "        # [0.2061, 0.2731, 0.1685],\n",
    "        # ...,\n",
    "        # [0.7932, 0.7912, 0.7810],\n",
    "        # [0.6166, 0.6201, 0.5730],\n",
    "        # [0.3050, 0.3551, 0.2093]], device='cuda:0', grad_fn=<SumBackward1>), \n",
    "        # \n",
    "        # 'disp_map': tensor([1.7129, 1.7855, 1.6072,  ..., 1.8940, 1.6734, 1.6624], device='cuda:0', grad_fn=<MulBackward0>), \n",
    "        # \n",
    "        # 'acc_map': tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000], device='cuda:0', grad_fn=<SumBackward1>), \n",
    "        # \n",
    "        # 'raw': tensor([[[-1.9319e-01, -4.2380e-02, -8.0331e-01, -1.4389e+01],\n",
    "        # [ 9.3089e-02,  8.9779e-02, -9.1683e-01, -2.2910e+01],\n",
    "        # [ 4.2853e-01,  3.8774e-01, -8.5693e-01, -2.6790e+01],\n",
    "        # ...,\n",
    "        # [-6.9547e-02, -1.2431e-01, -9.7465e-01,  5.1020e+01],\n",
    "        # [-4.1269e-01, -3.2929e-01, -1.0338e+00,  9.0478e+01],\n",
    "        # [-2.8733e-01, -8.4059e-02, -8.7000e-01,  2.4445e+02]],\n",
    "\n",
    "        # [[ 7.6820e-01,  1.2545e+00,  2.8074e-01, -1.4041e+01],\n",
    "        # [ 1.0468e+00,  1.6502e+00, -2.7239e-03, -1.8463e+01],\n",
    "        # [ 2.0594e+00,  2.3907e+00,  3.1019e-01, -2.8290e+01],\n",
    "        # ...,\n",
    "        # [-5.4305e-01, -5.7906e-01, -8.0245e-01,  3.2258e+01],\n",
    "        # [-1.0495e+00, -1.0278e+00, -1.0844e+00,  2.2359e+01],\n",
    "        # [-1.0510e+00, -9.6312e-01, -1.0124e+00,  4.2875e+01]],\n",
    "\n",
    "        # [[-6.1452e-01, -3.0069e-01, -1.3510e+00, -1.6736e+01],\n",
    "        # [-5.0800e-01, -2.1953e-01, -1.4639e+00, -1.6490e+01],\n",
    "        # [-5.9743e-01, -3.3871e-01, -1.5218e+00, -1.6432e+01],\n",
    "        # ...,\n",
    "        # [-1.5179e+00, -1.2576e+00, -1.7958e+00,  1.1900e+02],\n",
    "        # [-1.4549e+00, -1.3467e+00, -1.7947e+00,  2.3185e+01],\n",
    "        # [-1.4572e+00, -1.3552e+00, -1.8064e+00,  2.3385e+01]],\n",
    "\n",
    "        # ...,\n",
    "\n",
    "        # [[ 6.5511e-01,  4.8005e-01, -6.8248e-01, -2.8155e+01],\n",
    "        # [ 6.5423e-01,  5.9683e-01, -5.8341e-01, -2.5869e+01],\n",
    "        # [ 8.5920e-01,  7.2798e-01, -5.1905e-01, -2.8843e+01],\n",
    "        # ...,\n",
    "        # [-1.4973e+00, -1.5925e+00, -1.6391e+00,  4.7125e+02],\n",
    "        # [-2.4364e+00, -2.4179e+00, -2.4181e+00,  1.1233e+02],\n",
    "        # [-2.2584e+00, -2.2210e+00, -2.2316e+00,  1.7644e+02]],\n",
    "\n",
    "        # [[ 4.9670e-01,  7.7980e-01, -8.1408e-01, -2.0221e+01],\n",
    "        # [ 4.6471e-01,  4.5477e-01, -8.6296e-01, -2.0946e+01],\n",
    "        # [ 5.8376e-01,  7.6404e-01, -6.3700e-01, -2.5659e+01],\n",
    "        # ...,\n",
    "        # [ 3.0117e+00,  4.8709e+00,  4.3206e+00, -3.5615e+00],\n",
    "        # [ 3.3435e+00,  5.6523e+00,  5.2936e+00, -2.9616e+00],\n",
    "        # [ 2.6037e+00,  5.9075e+00,  6.9644e+00,  3.0141e+00]],\n",
    "\n",
    "        # [[-4.6111e-01, -5.3944e-01, -1.6045e+00, -1.5485e+01],\n",
    "        # [-6.8579e-01, -7.7304e-01, -1.8440e+00, -1.4753e+01],\n",
    "        # [-5.5825e-01, -6.8661e-01, -1.7472e+00, -1.9158e+01],\n",
    "        # ...,\n",
    "        # [-1.0059e+00, -1.3362e+00, -1.9315e+00,  4.7464e+00],\n",
    "        # [-1.0327e+00, -1.2731e+00, -1.8308e+00,  1.7083e+00],\n",
    "        # [-9.5244e-01, -1.1675e+00, -1.5974e+00,  1.9663e+00]]],\n",
    "        # device='cuda:0', grad_fn=<ReshapeAliasBackward0>), \n",
    "        # \n",
    "        # 'rgb0': tensor([[0.1879, 0.2435, 0.1474],\n",
    "        # [0.0499, 0.0502, 0.0511],\n",
    "        # [0.2260, 0.2895, 0.1910],\n",
    "        # ...,\n",
    "        # [0.7747, 0.7790, 0.7720],\n",
    "        # [0.6317, 0.6374, 0.5979],\n",
    "        # [0.3091, 0.3362, 0.1943]], device='cuda:0', grad_fn=<SumBackward1>), \n",
    "        # \n",
    "        # 'disp0': tensor([1.7112, 1.8140, 1.6950,  ..., 1.8942, 1.6637, 1.6136], device='cuda:0', grad_fn=<MulBackward0>), \n",
    "        #\n",
    "        # 'acc0': tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000], device='cuda:0', grad_fn=<SumBackward1>), \n",
    "        #\n",
    "        # 'z_std': tensor([0.0225, 0.0156, 0.0120,  ..., 0.0108, 0.0400, 0.1336], device='cuda:0')}\n",
    "        \n",
    "        for k in ret:\n",
    "            if k not in all_ret:\n",
    "                all_ret[k] = []\n",
    "            all_ret[k].append(ret[k])\n",
    "        # print(all_ret)\n",
    "        # {\n",
    "        # 'rgb_map': [tensor([[0.3596, 0.3708, 0.2484],\n",
    "        # [0.4484, 0.4415, 0.3111],\n",
    "        # [0.3928, 0.3898, 0.2720],\n",
    "        # ...,\n",
    "        # [0.5030, 0.4285, 0.3517],\n",
    "        # [0.4962, 0.4172, 0.3334],\n",
    "        # [0.4887, 0.4095, 0.3234]], device='cuda:0'), tensor([[0.4374, 0.3868, 0.3077],\n",
    "        # [0.3152, 0.2962, 0.2380],\n",
    "        # [0.3303, 0.3047, 0.2461],\n",
    "        # ...,\n",
    "        # [0.3095, 0.2413, 0.1847],\n",
    "        # [0.3425, 0.2649, 0.2020],\n",
    "        # [0.3390, 0.2649, 0.2017]], device='cuda:0'), tensor([[0.3280, 0.2559, 0.1973],\n",
    "        # [0.3296, 0.2548, 0.1944],\n",
    "        # [0.3282, 0.2526, 0.1935],\n",
    "        # ...,\n",
    "        # [0.6551, 0.6464, 0.5171],\n",
    "        # [0.6250, 0.6028, 0.4798],\n",
    "        # [0.5882, 0.5469, 0.4264]], device='cuda:0')], 'disp_map': [tensor([1.6642, 1.6570, 1.6521,  ..., 1.5733, 1.5742, 1.5784], device='cuda:0'), tensor([1.6753, 1.7885, 1.7782,  ..., 1.2587, 1.2471, 1.2447], device='cuda:0'), tensor([1.2399, 1.2391, 1.2425,  ..., 1.8311, 1.6835, 1.5569], device='cuda:0')], 'acc_map': [tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000], device='cuda:0'), tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000], device='cuda:0'), tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000], device='cuda:0')], 'rgb0': [tensor([[0.4536, 0.4364, 0.3346],\n",
    "        # [0.5424, 0.5290, 0.4109],\n",
    "        # [0.4064, 0.4065, 0.3093],\n",
    "        # ...,\n",
    "        # [0.5558, 0.4727, 0.3682],\n",
    "        # [0.5424, 0.4564, 0.3573],\n",
    "        # [0.5501, 0.4731, 0.3673]], device='cuda:0'), tensor([[0.4619, 0.4234, 0.3358],\n",
    "        # [0.3041, 0.3021, 0.2474],\n",
    "        # [0.3874, 0.3397, 0.2682],\n",
    "        # ...,\n",
    "        # [0.3025, 0.2407, 0.1764],\n",
    "        # [0.3031, 0.2447, 0.1801],\n",
    "        # [0.2946, 0.2330, 0.1723]], device='cuda:0'), tensor([[0.3052, 0.2408, 0.1792],\n",
    "        # [0.3163, 0.2515, 0.1894],\n",
    "        # [0.3279, 0.2622, 0.1952],\n",
    "        # ...,\n",
    "        # [0.5881, 0.6169, 0.4851],\n",
    "        # [0.5934, 0.6035, 0.4814],\n",
    "        # [0.5528, 0.5513, 0.4327]], device='cuda:0')], \n",
    "        # 'disp0': [tensor([1.6387, 1.6301, 1.6558,  ..., 1.5708, 1.5701, 1.5913], device='cuda:0'), tensor([1.7420, 1.7823, 1.6791,  ..., 1.2635, 1.2633, 1.2224], device='cuda:0'), tensor([1.2286, 1.2143, 1.2203,  ..., 1.9050, 1.8043, 1.7108], device='cuda:0')], \n",
    "        # 'acc0': [tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000], device='cuda:0'), tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000], device='cuda:0'), tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000], device='cuda:0')], \n",
    "        # 'z_std': [tensor([0.0900, 0.0902, 0.0901,  ..., 0.0915, 0.0911, 0.0938], device='cuda:0'), tensor([0.0941, 0.0896, 0.0965,  ..., 0.1283, 0.1263, 0.1056], device='cuda:0'), tensor([0.1051, 0.1050, 0.1063,  ..., 0.1518, 0.1549, 0.1401], device='cuda:0')]}\n",
    "    all_ret = {k : torch.cat(all_ret[k], 0) for k in all_ret}\n",
    "    # print(all_ret)\n",
    "    # {\n",
    "    # 'rgb_map': tensor([[0.3581, 0.3689, 0.2477],\n",
    "    #    [0.4479, 0.4408, 0.3112],\n",
    "    #    [0.3914, 0.3884, 0.2717],\n",
    "    #    ...,\n",
    "    #    [0.8166, 0.7137, 0.5876],\n",
    "    #    [0.8147, 0.7113, 0.5886],\n",
    "    #    [0.8124, 0.7104, 0.5837]], device='cuda:0'), \n",
    "    #\n",
    "    # 'disp_map': tensor([1.6642, 1.6571, 1.6522,  ..., 1.9530, 1.9497, 1.9521], device='cuda:0'), \n",
    "    # 'acc_map': tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000], device='cuda:0'), \n",
    "    # 'rgb0': tensor([[0.4541, 0.4368, 0.3353],\n",
    "    #    [0.5436, 0.5301, 0.4126],\n",
    "    #    [0.4064, 0.4066, 0.3099],\n",
    "    #    ...,\n",
    "    #    [0.8254, 0.7126, 0.6027],\n",
    "    #    [0.8204, 0.7077, 0.5972],\n",
    "    #    [0.8221, 0.7101, 0.5964]], device='cuda:0'), \n",
    "    # 'disp0': tensor([1.6389, 1.6301, 1.6560,  ..., 1.7748, 1.7937, 1.8374], device='cuda:0'), \n",
    "    # 'acc0': tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000], device='cuda:0'), \n",
    "    # 'z_std': tensor([0.0900, 0.0902, 0.0901,  ..., 0.1195, 0.1143, 0.1107], device='cuda:0')}\n",
    "    \n",
    "    # torch.Size([378, 504, 3]) torch.Size([378, 504])\n",
    "    return all_ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def render(H, W, K, chunk=1024*32, rays=None, c2w=None, ndc=True,\n",
    "                  near=0., far=1.,\n",
    "                  use_viewdirs=False, c2w_staticcam=None,\n",
    "                  **kwargs):\n",
    "    \"\"\"Render rays\n",
    "    Args:\n",
    "      H: int. Height of image in pixels.\n",
    "      W: int. Width of image in pixels.\n",
    "      focal: float. Focal length of pinhole camera.\n",
    "      chunk: int. Maximum number of rays to process simultaneously. Used to\n",
    "        control maximum memory usage. Does not affect final results.\n",
    "      rays: array of shape [2, batch_size, 3]. Ray origin and direction for\n",
    "        each example in batch.\n",
    "      c2w: array of shape [3, 4]. Camera-to-world transformation matrix.\n",
    "      ndc: bool. If True, represent ray origin, direction in NDC coordinates.\n",
    "      near: float or array of shape [batch_size]. Nearest distance for a ray.\n",
    "      far: float or array of shape [batch_size]. Farthest distance for a ray.\n",
    "      use_viewdirs: bool. If True, use viewing direction of a point in space in model.\n",
    "      c2w_staticcam: array of shape [3, 4]. If not None, use this transformation matrix for \n",
    "       camera while using other c2w argument for viewing directions.\n",
    "    Returns:\n",
    "      rgb_map: [batch_size, 3]. Predicted RGB values for rays.\n",
    "      disp_map: [batch_size]. Disparity map. Inverse of depth.\n",
    "      acc_map: [batch_size]. Accumulated opacity (alpha) along a ray.\n",
    "      extras: dict with everything returned by render_rays().\n",
    "    \"\"\"\n",
    "    if c2w is not None:\n",
    "        # special case to render full image\n",
    "        rays_o, rays_d = get_rays(H, W, K, c2w)\n",
    "        #print(H)  # 378\n",
    "        #print(W)  # 504\n",
    "        \n",
    "        #print(K)\n",
    "        \n",
    "        #[[407.5657959   0.        252.       ]\n",
    "        # [  0.        407.5657959 189.       ]\n",
    "        # [  0.          0.          1.       ]]\n",
    "        \n",
    "        # print(c2w)\n",
    "\n",
    "        # tensor([[ 9.9583e-01, -3.7973e-05,  9.1231e-02,  3.9451e-01],\n",
    "        #        [ 3.8132e-05,  1.0000e+00, -1.8653e-09, -9.6858e-09],\n",
    "        #        [-9.1231e-02,  3.4807e-06,  9.9583e-01,  0.0000e+00]], device='cuda:0')\n",
    "\n",
    "        # print(rays_o.shape) # torch.Size([378, 504, 3])\n",
    "        # print(rays_d.shape) # torch.Size([378, 504, 3])\n",
    "    else:\n",
    "        # use provided ray batch\n",
    "        rays_o, rays_d = rays\n",
    "\n",
    "    if use_viewdirs:\n",
    "        # provide ray directions as input\n",
    "        viewdirs = rays_d\n",
    "        if c2w_staticcam is not None:\n",
    "            # special case to visualize effect of viewdirs\n",
    "            rays_o, rays_d = get_rays(H, W, K, c2w_staticcam)\n",
    "        viewdirs = viewdirs / torch.norm(viewdirs, dim=-1, keepdim=True)\n",
    "        viewdirs = torch.reshape(viewdirs, [-1,3]).float()\n",
    "        # print(viewdirs)\n",
    "        \n",
    "        # tensor([[ 0.3432,  0.1160, -0.9321],\n",
    "        #        [-0.1477,  0.2889, -0.9459],\n",
    "        #        [ 0.3782, -0.3090, -0.8726],\n",
    "        #        ...,\n",
    "        #        [-0.5139, -0.2868, -0.8085],\n",
    "        #        [ 0.5251,  0.2238, -0.8211],\n",
    "        #        [-0.2700, -0.2209, -0.9372]], device='cuda:0')\n",
    "        \n",
    "        # print(viewdirs.shape)\n",
    "        # torch.Size([1024, 3])\n",
    "        \n",
    "\n",
    "    sh = rays_d.shape # [..., 3] \n",
    "    # print(sh) # torch.Size([378, 504, 3])\n",
    "    if ndc:\n",
    "        # for forward facing scenes\n",
    "        rays_o, rays_d = ndc_rays(H, W, K[0][0], 1., rays_o, rays_d)\n",
    "\n",
    "        \n",
    "\n",
    "    # Create ray batch\n",
    "    # print(rays_o.shape) # torch.Size([378, 504, 3])\n",
    "    rays_o = torch.reshape(rays_o, [-1,3]).float()\n",
    "    # print(rays_o.shape) # torch.Size([190512, 3])\n",
    "    rays_d = torch.reshape(rays_d, [-1,3]).float()\n",
    "\n",
    "    # print(torch.ones_like(rays_d[...,:1]))\n",
    "    \n",
    "    # tensor([[1.],\n",
    "    #    [1.],\n",
    "    #    [1.],\n",
    "    #    ...,\n",
    "    #    [1.],\n",
    "    #    [1.],\n",
    "    #    [1.]], device='cuda:0')\n",
    "    \n",
    "    # print(torch.ones_like(rays_d[...,:1]).shape)\n",
    "    # torch.Size([190512, 1])\n",
    "    \n",
    "    near, far = near * torch.ones_like(rays_d[...,:1]), far * torch.ones_like(rays_d[...,:1])\n",
    "    \n",
    "    # print(near)\n",
    "    \n",
    "    # tensor([[0.],\n",
    "    #    [0.],\n",
    "    #    [0.],\n",
    "    #    ...,\n",
    "    #    [0.],\n",
    "    #    [0.],\n",
    "    #    [0.]], device='cuda:0')\n",
    "    \n",
    "    # print(far)\n",
    "    \n",
    "    # tensor([[1.],\n",
    "    #    [1.],\n",
    "    #    [1.],\n",
    "    #    ...,\n",
    "    #    [1.],\n",
    "    #    [1.],\n",
    "    #    [1.]], device='cuda:0')\n",
    "    \n",
    "    rays = torch.cat([rays_o, rays_d, near, far], -1)\n",
    "    \n",
    "    #print(rays)\n",
    "    \n",
    "    #tensor([[ 0.3704,  0.0923, -1.0000,  ...,  2.0000,  0.0000,  1.0000],\n",
    "    #    [-0.9372,  0.0905, -1.0000,  ...,  2.0000,  0.0000,  1.0000],\n",
    "    #    [ 0.8355, -0.1676, -1.0000,  ...,  2.0000,  0.0000,  1.0000],\n",
    "    #    ...,\n",
    "    #    [-0.3754, -0.1484, -1.0000,  ...,  2.0000,  0.0000,  1.0000],\n",
    "    #    [ 0.0914,  1.0717, -1.0000,  ...,  2.0000,  0.0000,  1.0000],\n",
    "    #    [ 1.0489,  0.5523, -1.0000,  ...,  2.0000,  0.0000,  1.0000]],\n",
    "    #   device='cuda:0')\n",
    "    \n",
    "    # print(rays.shape) # torch.Size([1024, 8])\n",
    "    \n",
    "\n",
    "    if use_viewdirs:\n",
    "        rays = torch.cat([rays, viewdirs], -1)\n",
    "\n",
    "    #print(rays)\n",
    "    \n",
    "    # tensor([[-0.7948, -0.9399, -1.0000,  ..., -0.2841, -0.3516, -0.8920],\n",
    "    #    [-0.2115, -0.0843, -1.0000,  ..., -0.2699, -0.0656, -0.9607],\n",
    "    #    [-0.5973,  0.3672, -1.0000,  ..., -0.1928,  0.1862, -0.9634],\n",
    "    #    ...,\n",
    "    #    [-0.7046,  0.4731, -1.0000,  ..., -0.2490,  0.2268, -0.9416],\n",
    "    #    [ 0.0953,  0.9137, -1.0000,  ...,  0.3694,  0.3839, -0.8462],\n",
    "    #    [-0.3576,  0.9673, -1.0000,  ..., -0.4698,  0.3516, -0.8097]],\n",
    "    #   device='cuda:0')\n",
    "    \n",
    "    # print(rays.shape) # torch.Size([1024, 11])\n",
    "    \n",
    "    # Render and reshape\n",
    "    all_ret = batchify_rays(rays, chunk, **kwargs)\n",
    "    for k in all_ret:\n",
    "        print(k)\n",
    "        print(list(sh[:-1]))\n",
    "        print(list(all_ret[k].shape[1:]))\n",
    "        k_sh = list(sh[:-1]) + list(all_ret[k].shape[1:])\n",
    "        print(k_sh)\n",
    "        all_ret[k] = torch.reshape(all_ret[k], k_sh)\n",
    "        \n",
    "        #rgb_map\n",
    "        #[378, 504]\n",
    "        #[3]\n",
    "        #[378, 504, 3]\n",
    "        #disp_map\n",
    "        #[378, 504]\n",
    "        #[]\n",
    "        #[378, 504]\n",
    "        #acc_map\n",
    "        #[378, 504]\n",
    "        #[]\n",
    "        #[378, 504]\n",
    "        #rgb0\n",
    "        #[378, 504]\n",
    "        #[3]\n",
    "        #[378, 504, 3]\n",
    "        #disp0\n",
    "        #[378, 504]\n",
    "        #[]\n",
    "        #[378, 504]\n",
    "        #acc0\n",
    "        #[378, 504]\n",
    "        #[]\n",
    "        #[378, 504]\n",
    "        #z_std\n",
    "        #[378, 504]\n",
    "        #[]\n",
    "        #[378, 504]\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    k_extract = ['rgb_map', 'disp_map', 'acc_map']\n",
    "    ret_list = [all_ret[k] for k in k_extract]\n",
    "    ret_dict = {k : all_ret[k] for k in all_ret if k not in k_extract}\n",
    "    return ret_list + [ret_dict]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def render_rays(ray_batch,\n",
    "                network_fn,\n",
    "                network_query_fn,\n",
    "                N_samples,\n",
    "                return_raw=False,\n",
    "                linear_depend_inverse_depth=False,\n",
    "                perturb=0.,\n",
    "                N_importance=0,\n",
    "                network_fine=None,\n",
    "                white_background=False,\n",
    "                raw_noise_std=0.,\n",
    "                verbose=False,\n",
    "                pytest=False):\n",
    "    \"\"\"Volumetric rendering.\n",
    "    Args:\n",
    "      ray_batch: array of shape [batch_size, ...]. All information necessary\n",
    "        for sampling along a ray, including: ray origin, ray direction, min\n",
    "        dist, max dist, and unit-magnitude viewing direction.\n",
    "      network_fn: function. Model for predicting RGB and density at each point\n",
    "        in space.\n",
    "      network_query_fn: function used for passing queries to network_fn.\n",
    "      N_samples: int. Number of different times to sample along each ray.\n",
    "      return_raw: bool. If True, include model's raw, unprocessed predictions.\n",
    "      linear_depend_inverse: bool. If True, sample linearly in inverse depth rather than in depth.\n",
    "      perturb: float, 0 or 1. If non-zero, each ray is sampled at stratified\n",
    "        random points in time.\n",
    "      N_importance: int. Number of additional times to sample along each ray.\n",
    "        These samples are only passed to network_fine.\n",
    "      network_fine: \"fine\" network with same spec as network_fn.\n",
    "      white_background: bool. If True, assume a white background.\n",
    "      raw_noise_std: ...\n",
    "      verbose: bool. If True, print more debugging info.\n",
    "    Returns:\n",
    "      rgb_map: [num_rays, 3]. Estimated RGB color of a ray. Comes from fine model.\n",
    "      disparity_map: [num_rays]. Disparity map. 1 / depth.\n",
    "      accumulated_opacity_map: [num_rays]. Accumulated opacity along each ray. Comes from fine model.\n",
    "      raw: [num_rays, num_samples, 4]. Raw predictions from model.\n",
    "      rgb0: See rgb_map. Output for coarse model.\n",
    "      disp0: See disparity_map. Output for coarse model.\n",
    "      acc0: See accumulated_opacity_map. Output for coarse model.\n",
    "      z_std: [num_rays]. Standard deviation of distances along ray for each\n",
    "        sample.\n",
    "    \"\"\"\n",
    "    N_rays = ray_batch.shape[0]\n",
    "    rays_origin, rays_direction = ray_batch[:,0:3], ray_batch[:,3:6] # [N_rays, 3] each\n",
    "    view_direction = ray_batch[:,-3:] if ray_batch.shape[-1] > 8 else None\n",
    "    bounds = torch.reshape(ray_batch[...,6:8], [-1,1,2])\n",
    "    near, far = bounds[...,0], bounds[...,1] # [-1,1]\n",
    "\n",
    "    t_vals = torch.linspace(0., 1., steps=N_samples).to(device)\n",
    "    \n",
    "    if not linear_depend_inverse_depth:\n",
    "        z_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "    else:\n",
    "        z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "\n",
    "    z_vals = z_vals.expand([N_rays, N_samples]).to(device)\n",
    "\n",
    "    if perturb > 0.:\n",
    "        # get intervals between samples\n",
    "        mids = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n",
    "        upper = torch.cat([mids, z_vals[...,-1:]], -1)\n",
    "        lower = torch.cat([z_vals[...,:1], mids], -1)\n",
    "        # stratified samples in those intervals\n",
    "        t_rand = torch.rand(z_vals.shape).to(device)\n",
    "\n",
    "        # Pytest, overwrite u with numpy's fixed random numbers\n",
    "        if pytest:\n",
    "            np.random.seed(0)\n",
    "            t_rand = np.random.rand(*list(z_vals.shape))\n",
    "            t_rand = torch.Tensor(t_rand).to(device)\n",
    "\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "\n",
    "    # form the origin to the end of the ray, in the ray direction to form pts\n",
    "    pts = rays_origin[...,None,:] + rays_direction[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples, 3]\n",
    "    \n",
    "\n",
    "\n",
    "#     raw = run_network(pts)\n",
    "    raw = network_query_fn(pts, view_direction, network_fn)\n",
    "    rgb_map, disparity_map, accumulated_opacity_map, weights, depth_map = raw2outputs(raw, z_vals, rays_direction, raw_noise_std, white_background, pytest=pytest)\n",
    "\n",
    "    if N_importance > 0:\n",
    "\n",
    "        rgb_map_0, disp_map_0, acc_map_0 = rgb_map, disparity_map, accumulated_opacity_map\n",
    "\n",
    "        z_vals_mid = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n",
    "        z_samples = sample_pdf(z_vals_mid, weights[...,1:-1], N_importance, det=(perturb==0.), pytest=pytest)\n",
    "        z_samples = z_samples.detach()\n",
    "\n",
    "        z_vals, _ = torch.sort(torch.cat([z_vals, z_samples], -1), -1)\n",
    "        pts = rays_origin[...,None,:] + rays_direction[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples + N_importance, 3]\n",
    "\n",
    "        run_fn = network_fn if network_fine is None else network_fine\n",
    "#         raw = run_network(pts, fn=run_fn)\n",
    "        raw = network_query_fn(pts, view_direction, run_fn)\n",
    "\n",
    "        rgb_map, disparity_map, accumulated_opacity_map, weights, depth_map = raw2outputs(raw, z_vals, rays_direction, raw_noise_std, white_background, pytest=pytest)\n",
    "\n",
    "    ret = {'rgb_map' : rgb_map, 'disp_map' : disparity_map, 'acc_map' : accumulated_opacity_map}\n",
    "    if return_raw:\n",
    "        ret['raw'] = raw\n",
    "    if N_importance > 0:\n",
    "        ret['rgb0'] = rgb_map_0\n",
    "        ret['disp0'] = disp_map_0\n",
    "        ret['acc0'] = acc_map_0\n",
    "        ret['z_std'] = torch.std(z_samples, dim=-1, unbiased=False)  # [N_rays]\n",
    "\n",
    "    for k in ret:\n",
    "        if (torch.isnan(ret[k]).any() or torch.isinf(ret[k]).any()) and DEBUG:\n",
    "            print(f\"! [Numerical Error] {k} contains nan or inf.\")\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "def raw2outputs(raw, z_vals, rays_direction, raw_noise_std=0, white_background=False, pytest=False):\n",
    "    \"\"\"Transforms model's predictions to semantically meaningful values.\n",
    "    Args:\n",
    "        raw: [num_rays, num_samples along ray, 4]. Prediction from model.\n",
    "        z_vals: [num_rays, num_samples along ray]. Integration time.\n",
    "        rays_direction: [num_rays, 3]. Direction of each ray.\n",
    "    Returns:\n",
    "        rgb_map: [num_rays, 3]. Estimated RGB color of a ray.\n",
    "        disparity_map: [num_rays]. Disparity map. Inverse of depth map.\n",
    "        accumulated_opacity_map: [num_rays]. Sum of weights along each ray.\n",
    "        weights: [num_rays, num_samples]. Weights assigned to each sampled color.\n",
    "        depth_map: [num_rays]. Estimated distance to object.\n",
    "    \"\"\"\n",
    "    raw2alpha = lambda raw, distances, act_fn=F.relu: 1.-torch.exp(-act_fn(raw)*distances)\n",
    "\n",
    "    distances = z_vals[...,1:] - z_vals[...,:-1]\n",
    "    distances = torch.cat([distances, torch.Tensor([1e10]).to(device).expand(distances[...,:1].shape)], -1)  # [N_rays, N_samples]\n",
    "\n",
    "    distances = distances * torch.norm(rays_direction[...,None,:], dim=-1)\n",
    "\n",
    "    rgb = torch.sigmoid(raw[...,:3])  # [N_rays, N_samples, 3]\n",
    "    noise = 0.\n",
    "    if raw_noise_std > 0.:\n",
    "        noise = torch.randn(raw[...,3].shape).to(device) * raw_noise_std\n",
    "\n",
    "        # Overwrite randomly sampled data if pytest\n",
    "        if pytest:\n",
    "            np.random.seed(0)\n",
    "            noise = np.random.rand(*list(raw[...,3].shape)) * raw_noise_std\n",
    "            noise = torch.Tensor(noise).to(device)\n",
    "\n",
    "    alpha = raw2alpha(raw[...,3] + noise, distances)  # [N_rays, N_samples]\n",
    "    # weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n",
    "    # tensor = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float)\n",
    "    # cumprod_tensor = torch.cumprod(tensor, dim=0)\n",
    "    # tensor([1., 2., 6., 24., 120.])\n",
    "    # torch.cumprod() is Ti of all the point\n",
    "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1)).to(device), 1.-alpha + 1e-10], -1), -1)[:, :-1]\n",
    "    rgb_map = torch.sum(weights[...,None] * rgb, -2)  # [N_rays, 3]\n",
    "\n",
    "    depth_map = torch.sum(weights * z_vals, -1)\n",
    "    disparity_map = 1./torch.max(1e-10 * torch.ones_like(depth_map).to(device), depth_map / torch.sum(weights, -1))\n",
    "    accumulated_opacity_map = torch.sum(weights, -1)\n",
    "\n",
    "    if white_background:\n",
    "        rgb_map = rgb_map + (1.-accumulated_opacity_map[...,None])\n",
    "\n",
    "    return rgb_map, disparity_map, accumulated_opacity_map, weights, depth_map\n",
    "\n",
    "\n",
    "def render_path(render_poses, hwf, K, chunk, render_kwargs, gt_imgs=None, savedir=None, render_factor=0):\n",
    "\n",
    "    H, W, focal = hwf\n",
    "\n",
    "    if render_factor!=0:\n",
    "        # Render downsampled for speed\n",
    "        H = H//render_factor\n",
    "        W = W//render_factor\n",
    "        focal = focal/render_factor\n",
    "\n",
    "    rgbs = []\n",
    "    disps = []\n",
    "\n",
    "    t = time.time()\n",
    "    for i, c2w in enumerate(tqdm(render_poses)):\n",
    "        print(i, time.time() - t)\n",
    "        t = time.time()\n",
    "        rgb, disp, acc, _ = render(H, W, K, chunk=chunk, c2w=c2w[:3,:4], **render_kwargs)\n",
    "        rgbs.append(rgb.cpu().numpy())\n",
    "        disps.append(disp.cpu().numpy())\n",
    "        if i==0:\n",
    "            print(rgb.shape, disp.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        if gt_imgs is not None and render_factor==0:\n",
    "            p = -10. * np.log10(np.mean(np.square(rgb.cpu().numpy() - gt_imgs[i])))\n",
    "            print(p)\n",
    "        \"\"\"\n",
    "\n",
    "        if savedir is not None:\n",
    "            rgb8 = to8b(rgbs[-1])\n",
    "            filename = os.path.join(savedir, '{:03d}.png'.format(i))\n",
    "            imageio.imwrite(filename, rgb8)\n",
    "\n",
    "\n",
    "    rgbs = np.stack(rgbs, 0)\n",
    "    disps = np.stack(disps, 0)\n",
    "\n",
    "    return rgbs, disps\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002a2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    \n",
    "    #########################################################################\n",
    "    # python run_nerf.py --config configs/fern.txt\n",
    "    N_importance = 64\n",
    "    N_rand = 1024\n",
    "    N_samples = 64\n",
    "    basedir = './logs'\n",
    "    chunk = 32768\n",
    "    config = 'configs/fern.txt'\n",
    "    datadir = './data/nerf_llff_data/fern'\n",
    "    dataset_type = 'llff'\n",
    "    expname = 'fern_test'\n",
    "    factor = 8\n",
    "    ft_path = None\n",
    "    half_res = False\n",
    "    i_embed = 0\n",
    "    i_img = 500\n",
    "    i_print = 100\n",
    "    i_testset = 5000\n",
    "    i_video = 5000\n",
    "    i_weights = 10000\n",
    "    lindisp = False\n",
    "    llffhold = 8\n",
    "    lrate = 0.0005\n",
    "    lrate_decay = 250\n",
    "    multires = 10\n",
    "    multires_views = 4\n",
    "    netchunk = 65536\n",
    "    netdepth = 8\n",
    "    netdepth_fine = 8\n",
    "    netwidth = 256\n",
    "    netwidth_fine = 256\n",
    "    no_batching = False\n",
    "    no_ndc = False\n",
    "    no_reload = False\n",
    "    perturb = 1.0\n",
    "    precrop_frac = 0.5\n",
    "    precrop_iters = 0\n",
    "    raw_noise_std = 1.0\n",
    "    render_factor = 0\n",
    "    render_only = False\n",
    "    render_test = False\n",
    "    shape = 'greek'\n",
    "    spherify = False\n",
    "    testskip = 8\n",
    "    use_viewdirs = True\n",
    "    white_background = False\n",
    "    ################################################################################\n",
    "\n",
    "    images, poses, bds, render_poses, i_test = load_llff_data(datadir, factor, recenter=True, bd_factor=.75, spherify=spherify)\n",
    "    print(\"================[0][load_data]=================\")\n",
    "    #print(images.shape)   (20, 378, 504, 3)\n",
    "    #print(poses.shape)    (20, 3, 5)\n",
    "    #print(bds.shape)      (20, 2)\n",
    "    #print(render_poses.shape)  (120, 3, 5)   # Generate poses for spiral path\n",
    "    #print(i_test) 12           print('HOLDOUT view is', i_test)\n",
    "    #print(i_test.shape)\n",
    "\n",
    "    #print(bds) [0.5500126  2.4253333 ]\n",
    "    # print(bds.shape)  (20, 2)\n",
    "    # []\n",
    "\n",
    "\n",
    "    # what is the (20, 3, 5), 3 is for what, 5 is for what\n",
    "    hwf = poses[0,:3,-1]\n",
    "    poses = poses[:,:3,:4]\n",
    "    #print(poses.shape) (20, 3, 4)\n",
    "    print(poses[0])\n",
    "    # (20, 378, 504, 3) (120, 3, 5) [378.     504.     407.5658] ./data/nerf_llff_data/fern\n",
    "    print('Loaded llff', images.shape, render_poses.shape, hwf, datadir)\n",
    "\n",
    "    if not isinstance(i_test, list):\n",
    "        i_test = [i_test]\n",
    "\n",
    "    #print(i_test) [12]\n",
    "\n",
    "\n",
    "    if llffhold > 0:\n",
    "        print('Auto LLFF holdout,', llffhold)\n",
    "        i_test = np.arange(images.shape[0])[::llffhold]\n",
    "    #print(i_test) [ 0  8 16]\n",
    "\n",
    "    i_val = i_test\n",
    "    i_train = np.array([i for i in np.arange(int(images.shape[0])) if\n",
    "                    (i not in i_test and i not in i_val)])\n",
    "    #print(i_train) [ 1  2  3  4  5  6  7  9 10 11 12 13 14 15 17 18 19]\n",
    "\n",
    "\n",
    "    print('DEFINING BOUNDS')\n",
    "    if no_ndc:\n",
    "        near = np.ndarray.min(bds) * .9\n",
    "        far = np.ndarray.max(bds) * 1.\n",
    "\n",
    "    else:\n",
    "        near = 0.\n",
    "        far = 1.\n",
    "    print('NEAR FAR', near, far) # 0.4737630307674408 2.4794018268585205\n",
    "\n",
    "\n",
    "    # 1 Cast intrinsics to right types\n",
    "    H, W, focal = hwf\n",
    "    #print(hwf)  [378.     504.     407.5658]\n",
    "    H, W = int(H), int(W)\n",
    "    hwf = [H, W, focal]\n",
    "    \n",
    "    #print(hwf) [378, 504, 407.5658]\n",
    "\n",
    "    # Load data\n",
    "\n",
    "    K = np.array([\n",
    "        [focal, 0, 0.5*W],\n",
    "        [0, focal, 0.5*H],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    print(K)    \n",
    "    \n",
    "    basedir = basedir\n",
    "    expname = expname\n",
    "    os.makedirs(os.path.join(basedir, expname), exist_ok=True)    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    # create_nerf()\n",
    "    \n",
    "    print(\"================[1][create_nerf()]=================\")\n",
    "    # Positional encoding\n",
    "    \n",
    "    embed_fn, input_ch = get_embedder(multires, i_embed)\n",
    "\n",
    "    input_ch_views = 0\n",
    "    embeddirs_fn = None\n",
    "    if use_viewdirs:\n",
    "        embeddirs_fn, input_ch_views = get_embedder(multires_views, i_embed)\n",
    "    output_ch = 5 if N_importance > 0 else 4\n",
    "    skips = [4]\n",
    "    \n",
    "    \n",
    "    model = NeRF(D=netdepth, W=netwidth,\n",
    "                 input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                 input_ch_views=input_ch_views, use_viewdirs=use_viewdirs).to(device)\n",
    "    grad_vars = list(model.parameters())\n",
    "\n",
    "    \n",
    "    model_fine = None\n",
    "    if N_importance > 0:\n",
    "        model_fine = NeRF(D=netdepth_fine, W=netwidth_fine,\n",
    "                          input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                          input_ch_views=input_ch_views, use_viewdirs=use_viewdirs).to(device)\n",
    "        grad_vars += list(model_fine.parameters())\n",
    "\n",
    "    network_query_fn = lambda inputs, viewdirs, network_fn : run_network(inputs, viewdirs, network_fn,\n",
    "                                                                embed_fn=embed_fn,\n",
    "                                                                embeddirs_fn=embeddirs_fn,\n",
    "                                                                netchunk=netchunk)\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.Adam(params=grad_vars, lr=lrate, betas=(0.9, 0.999))\n",
    "\n",
    "    start = 0\n",
    "    basedir = basedir\n",
    "    expname = expname\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    # Load checkpoints\n",
    "    if ft_path is not None and ft_path!='None':\n",
    "        ckpts = [ft_path]\n",
    "    else:\n",
    "        ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if 'tar' in f]\n",
    "\n",
    "    print('Found ckpts', ckpts)\n",
    "    if len(ckpts) > 0 and not no_reload:\n",
    "        ckpt_path = ckpts[-1]\n",
    "        print('Reloading from', ckpt_path)\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "\n",
    "        start = ckpt['global_step']\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "\n",
    "        # Load model\n",
    "        model.load_state_dict(ckpt['network_fn_state_dict'])\n",
    "        if model_fine is not None:\n",
    "            model_fine.load_state_dict(ckpt['network_fine_state_dict'])\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    render_kwargs_train = {\n",
    "        'network_query_fn' : network_query_fn,\n",
    "        'perturb' : perturb,\n",
    "        'N_importance' : N_importance,\n",
    "        'network_fine' : model_fine,\n",
    "        'N_samples' : N_samples,\n",
    "        'network_fn' : model,\n",
    "        'use_viewdirs' : use_viewdirs,\n",
    "        'white_background' : white_background,\n",
    "        'raw_noise_std' : raw_noise_std,\n",
    "    }\n",
    "\n",
    "    # NDC only good for LLFF-style forward facing data\n",
    "    if dataset_type != 'llff' or no_ndc:\n",
    "        print('Not ndc!')\n",
    "        render_kwargs_train['ndc'] = False\n",
    "        render_kwargs_train['lindisp'] = lindisp\n",
    "\n",
    "    render_kwargs_test = {k : render_kwargs_train[k] for k in render_kwargs_train}\n",
    "    render_kwargs_test['perturb'] = False\n",
    "    render_kwargs_test['raw_noise_std'] = 0.\n",
    "    \n",
    "    \n",
    "    print(\"================[2][get training data, validation data]=================\")\n",
    "\n",
    "    global_step = start\n",
    "\n",
    "    bds_dict = {\n",
    "        'near' : near,\n",
    "        'far' : far,\n",
    "    }\n",
    "\n",
    "    # near, far to dictionary\n",
    "    render_kwargs_train.update(bds_dict)\n",
    "    render_kwargs_test.update(bds_dict)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Move testing data to GPU\n",
    "    render_poses = torch.Tensor(render_poses).to(device)\n",
    "    #print(render_poses)\n",
    "    #print(render_poses.shape) torch.Size([3, 3, 4])    \n",
    "    \n",
    "    # Prepare raybatch tensor if batching random rays\n",
    "    N_rand = N_rand # 1024\n",
    "    use_batching = not no_batching # no_batching = True\n",
    "    \n",
    "    \n",
    "    \n",
    "    if use_batching:\n",
    "        # For random ray batching\n",
    "        print('get rays')\n",
    "        rays = np.stack([get_rays_np(H, W, K, p) for p in poses[:,:3,:4]], 0) # [N, ro+rd, H, W, 3]\n",
    "        print('done, concats')\n",
    "        rays_rgb = np.concatenate([rays, images[:,None]], 1) # [N, ro+rd+rgb, H, W, 3]\n",
    "        rays_rgb = np.transpose(rays_rgb, [0,2,3,1,4]) # [N, H, W, ro+rd+rgb, 3]\n",
    "        rays_rgb = np.stack([rays_rgb[i] for i in i_train], 0) # train images only\n",
    "        rays_rgb = np.reshape(rays_rgb, [-1,3,3]) # [(N-1)*H*W, ro+rd+rgb, 3]\n",
    "        rays_rgb = rays_rgb.astype(np.float32)\n",
    "        print('shuffle rays')\n",
    "        np.random.shuffle(rays_rgb)\n",
    "\n",
    "        print('done')\n",
    "        i_batch = 0\n",
    "\n",
    "    # Move training data to GPU\n",
    "    if use_batching:\n",
    "        images = torch.Tensor(images).to(device)\n",
    "    poses = torch.Tensor(poses).to(device)\n",
    "    \n",
    "    #print(poses)\n",
    "    #print(poses.shape) #torch.Size([20, 3, 4])\n",
    "    \n",
    "    if use_batching:\n",
    "        rays_rgb = torch.Tensor(rays_rgb).to(device)    \n",
    "\n",
    "    N_iters = 100000 + 1\n",
    "    print('Begin')\n",
    "    print('TRAIN views are', i_train)\n",
    "    print('TEST views are', i_test)\n",
    "    print('VAL views are', i_val)\n",
    "\n",
    "    start = start + 1   \n",
    "    for i in trange(start, N_iters):\n",
    "        time0 = time.time()\n",
    "        \n",
    "                # Sample random ray batch\n",
    "        if use_batching:\n",
    "            # Random over all images\n",
    "            batch = rays_rgb[i_batch:i_batch+N_rand] # [B, 2+1, 3*?]\n",
    "            batch = torch.transpose(batch, 0, 1)\n",
    "            batch_rays, target_s = batch[:2], batch[2]\n",
    "\n",
    "            i_batch += N_rand\n",
    "            if i_batch >= rays_rgb.shape[0]:\n",
    "                print(\"Shuffle data after an epoch!\")\n",
    "                rand_idx = torch.randperm(rays_rgb.shape[0])\n",
    "                rays_rgb = rays_rgb[rand_idx]\n",
    "                i_batch = 0\n",
    "                \n",
    "                \n",
    "        #####  Core optimization loop  #####\n",
    "        rgb, disp, acc, extras = render(H, W, K, chunk=chunk, rays=batch_rays, verbose=i < 10, return_raw=True, **render_kwargs_train)   \n",
    "        \n",
    "        #print(\"================[3][loss]=================\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # get the loss of model prediction and the target image\n",
    "        img_loss = img2mse(rgb, target_s)\n",
    "        trans = extras['raw'][...,-1]\n",
    "        loss = img_loss\n",
    "        psnr = mse2psnr(img_loss)\n",
    "\n",
    "        #input(\"Press Enter to continue...\")\n",
    "        \n",
    "        if 'rgb0' in extras:\n",
    "            img_loss0 = img2mse(extras['rgb0'], target_s)\n",
    "            loss = loss + img_loss0\n",
    "            psnr0 = mse2psnr(img_loss0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # NOTE: IMPORTANT!\n",
    "        ###   update learning rate   ###\n",
    "        decay_rate = 0.1\n",
    "        decay_steps = lrate_decay * 1000\n",
    "        new_lrate = lrate * (decay_rate ** (global_step / decay_steps))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = new_lrate\n",
    "        ################################\n",
    "\n",
    "        dt = time.time()-time0\n",
    "\n",
    "        # Rest is logging\n",
    "        if i%i_weights==0:\n",
    "            path = os.path.join(basedir, expname, '{:06d}.tar'.format(i))\n",
    "            torch.save({\n",
    "                'global_step': global_step,\n",
    "                'network_fn_state_dict': render_kwargs_train['network_fn'].state_dict(),\n",
    "                'network_fine_state_dict': render_kwargs_train['network_fine'].state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, path)\n",
    "            print('Saved checkpoints at', path)\n",
    "\n",
    "        if i%i_video==0 and i > 0:\n",
    "            # Turn on testing mode\n",
    "            with torch.no_grad():\n",
    "                rgbs, disps = render_path(render_poses, hwf, K, chunk, render_kwargs_test)\n",
    "            print('Done, saving', rgbs.shape, disps.shape)\n",
    "            moviebase = os.path.join(basedir, expname, '{}_spiral_{:06d}_'.format(expname, i))\n",
    "            imageio.mimwrite(moviebase + 'rgb.mp4', to8b(rgbs), fps=30, quality=8)\n",
    "            imageio.mimwrite(moviebase + 'disp.mp4', to8b(disps / np.max(disps)), fps=30, quality=8)\n",
    "\n",
    "        if i%i_testset==0 and i > 0:\n",
    "            testsavedir = os.path.join(basedir, expname, 'testset_{:06d}'.format(i))\n",
    "            os.makedirs(testsavedir, exist_ok=True)\n",
    "            print('test poses shape', poses[i_test].shape)\n",
    "            with torch.no_grad():\n",
    "                render_path(torch.Tensor(poses[i_test]).to(device), hwf, K, chunk, render_kwargs_test, gt_imgs=images[i_test], savedir=testsavedir)\n",
    "            print('Saved test set')\n",
    "\n",
    "\n",
    "    \n",
    "        if i%i_print==0:\n",
    "            tqdm.write(f\"[TRAIN] Iter: {i} Loss: {loss.item()}  PSNR: {psnr.item()}\")\n",
    "\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b64a16c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================[0][load_data]=================\n",
      "[[ 0.99569476 -0.02079598 -0.09033062 -0.3081002 ]\n",
      " [ 0.02503342  0.9986262   0.04603354  0.1346772 ]\n",
      " [ 0.0892492  -0.04809664  0.99484736  0.03989876]]\n",
      "Loaded llff (20, 378, 504, 3) (120, 3, 5) [378.     504.     407.5658] ./data/nerf_llff_data/fern\n",
      "Auto LLFF holdout, 8\n",
      "DEFINING BOUNDS\n",
      "NEAR FAR 0.0 1.0\n",
      "[[407.5657959   0.        252.       ]\n",
      " [  0.        407.5657959 189.       ]\n",
      " [  0.          0.          1.       ]]\n",
      "================[1][create_nerf()]=================\n",
      "Found ckpts ['./logs/fern_test/010000.tar', './logs/fern_test/020000.tar', './logs/fern_test/030000.tar', './logs/fern_test/040000.tar', './logs/fern_test/050000.tar', './logs/fern_test/060000.tar', './logs/fern_test/070000.tar', './logs/fern_test/080000.tar', './logs/fern_test/090000.tar', './logs/fern_test/100000.tar']\n",
      "Reloading from ./logs/fern_test/100000.tar\n",
      "================[2][get training data, validation data]=================\n",
      "get rays\n",
      "done, concats\n",
      "shuffle rays\n",
      "done\n",
      "Begin\n",
      "TRAIN views are [ 1  2  3  4  5  6  7  9 10 11 12 13 14 15 17 18 19]\n",
      "TEST views are [ 0  8 16]\n",
      "VAL views are [ 0  8 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 1024, 32768)\n",
      "rgb_map\n",
      "[1024]\n",
      "[3]\n",
      "[1024, 3]\n",
      "disp_map\n",
      "[1024]\n",
      "[]\n",
      "[1024]\n",
      "acc_map\n",
      "[1024]\n",
      "[]\n",
      "[1024]\n",
      "raw\n",
      "[1024]\n",
      "[128, 4]\n",
      "[1024, 128, 4]\n",
      "rgb0\n",
      "[1024]\n",
      "[3]\n",
      "[1024, 3]\n",
      "disp0\n",
      "[1024]\n",
      "[]\n",
      "[1024]\n",
      "acc0\n",
      "[1024]\n",
      "[]\n",
      "[1024]\n",
      "z_std\n",
      "[1024]\n",
      "[]\n",
      "[1024]\n",
      "Saved checkpoints at ./logs/fern_test/100000.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                          | 0/120 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0024423599243164062\n",
      "range(0, 190512, 32768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep/.local/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\n",
      "  1%|▎                                 | 1/120 [00:11<23:27, 11.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb_map\n",
      "[378, 504]\n",
      "[3]\n",
      "[378, 504, 3]\n",
      "disp_map\n",
      "[378, 504]\n",
      "[]\n",
      "[378, 504]\n",
      "acc_map\n",
      "[378, 504]\n",
      "[]\n",
      "[378, 504]\n",
      "rgb0\n",
      "[378, 504]\n",
      "[3]\n",
      "[378, 504, 3]\n",
      "disp0\n",
      "[378, 504]\n",
      "[]\n",
      "[378, 504]\n",
      "acc0\n",
      "[378, 504]\n",
      "[]\n",
      "[378, 504]\n",
      "z_std\n",
      "[378, 504]\n",
      "[]\n",
      "[378, 504]\n",
      "torch.Size([378, 504, 3]) torch.Size([378, 504])\n",
      "1 11.828275680541992\n",
      "range(0, 190512, 32768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▌                                 | 2/120 [00:23<23:20, 11.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb_map\n",
      "[378, 504]\n",
      "[3]\n",
      "[378, 504, 3]\n",
      "disp_map\n",
      "[378, 504]\n",
      "[]\n",
      "[378, 504]\n",
      "acc_map\n",
      "[378, 504]\n",
      "[]\n",
      "[378, 504]\n",
      "rgb0\n",
      "[378, 504]\n",
      "[3]\n",
      "[378, 504, 3]\n",
      "disp0\n",
      "[378, 504]\n",
      "[]\n",
      "[378, 504]\n",
      "acc0\n",
      "[378, 504]\n",
      "[]\n",
      "[378, 504]\n",
      "z_std\n",
      "[378, 504]\n",
      "[]\n",
      "[378, 504]\n",
      "2 11.890933513641357\n",
      "range(0, 190512, 32768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▌                                 | 2/120 [00:34<33:56, 17.26s/it]\n",
      "  0%|                                            | 0/1 [00:34<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 339\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39mi_video\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# Turn on testing mode\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 339\u001b[0m         rgbs, disps \u001b[38;5;241m=\u001b[39m \u001b[43mrender_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender_poses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhwf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_kwargs_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone, saving\u001b[39m\u001b[38;5;124m'\u001b[39m, rgbs\u001b[38;5;241m.\u001b[39mshape, disps\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    341\u001b[0m     moviebase \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(basedir, expname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_spiral_\u001b[39m\u001b[38;5;132;01m{:06d}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(expname, i))\n",
      "Cell \u001b[0;32mIn[3], line 616\u001b[0m, in \u001b[0;36mrender_path\u001b[0;34m(render_poses, hwf, K, chunk, render_kwargs, gt_imgs, savedir, render_factor)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28mprint\u001b[39m(i, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t)\n\u001b[1;32m    615\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 616\u001b[0m rgb, disp, acc, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2w\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc2w\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrender_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m rgbs\u001b[38;5;241m.\u001b[39mappend(rgb\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    618\u001b[0m disps\u001b[38;5;241m.\u001b[39mappend(disp\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[0;32mIn[3], line 415\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(H, W, K, chunk, rays, c2w, ndc, near, far, use_viewdirs, c2w_staticcam, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m     rays \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([rays, viewdirs], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m#print(rays)\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# tensor([[-0.7948, -0.9399, -1.0000,  ..., -0.2841, -0.3516, -0.8920],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    413\u001b[0m \n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# Render and reshape\u001b[39;00m\n\u001b[0;32m--> 415\u001b[0m all_ret \u001b[38;5;241m=\u001b[39m \u001b[43mbatchify_rays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m all_ret:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28mprint\u001b[39m(k)\n",
      "Cell \u001b[0;32mIn[3], line 105\u001b[0m, in \u001b[0;36mbatchify_rays\u001b[0;34m(rays_flat, chunk, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, rays_flat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], chunk))\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, rays_flat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], chunk):\n\u001b[0;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mrender_rays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrays_flat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# print(ret)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# {\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# 'rgb_map': tensor([[0.1997, 0.2570, 0.1611],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# 'z_std': tensor([0.0225, 0.0156, 0.0120,  ..., 0.0108, 0.0400, 0.1336], device='cuda:0')}\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m ret:\n",
      "Cell \u001b[0;32mIn[3], line 513\u001b[0m, in \u001b[0;36mrender_rays\u001b[0;34m(ray_batch, network_fn, network_query_fn, N_samples, return_raw, linear_depend_inverse_depth, perturb, N_importance, network_fine, white_background, raw_noise_std, verbose, pytest)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m#     raw = run_network(pts)\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     raw \u001b[38;5;241m=\u001b[39m network_query_fn(pts, view_direction, network_fn)\n\u001b[0;32m--> 513\u001b[0m     rgb_map, disparity_map, accumulated_opacity_map, weights, depth_map \u001b[38;5;241m=\u001b[39m \u001b[43mraw2outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrays_direction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_noise_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhite_background\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpytest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpytest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m N_importance \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    517\u001b[0m         rgb_map_0, disp_map_0, acc_map_0 \u001b[38;5;241m=\u001b[39m rgb_map, disparity_map, accumulated_opacity_map\n",
      "Cell \u001b[0;32mIn[3], line 565\u001b[0m, in \u001b[0;36mraw2outputs\u001b[0;34m(raw, z_vals, rays_direction, raw_noise_std, white_background, pytest)\u001b[0m\n\u001b[1;32m    562\u001b[0m raw2alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m raw, distances, act_fn\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu: \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mact_fn(raw)\u001b[38;5;241m*\u001b[39mdistances)\n\u001b[1;32m    564\u001b[0m distances \u001b[38;5;241m=\u001b[39m z_vals[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m-\u001b[39m z_vals[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 565\u001b[0m distances \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([distances, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1e10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpand(distances[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,:\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [N_rays, N_samples]\u001b[39;00m\n\u001b[1;32m    567\u001b[0m distances \u001b[38;5;241m=\u001b[39m distances \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(rays_direction[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m,:], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    569\u001b[0m rgb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(raw[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,:\u001b[38;5;241m3\u001b[39m])  \u001b[38;5;66;03m# [N_rays, N_samples, 3]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d01132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1fa0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
