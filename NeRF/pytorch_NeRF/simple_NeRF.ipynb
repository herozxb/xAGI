{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6086c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import imageio\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from run_nerf_helpers import *\n",
    "\n",
    "from load_llff import load_llff_data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "np.random.seed(0)\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef60a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takeaway function and not decompose it\n",
    "def batchify(fn, chunk):\n",
    "    \"\"\"Constructs a version of 'fn' that applies to smaller batches.\n",
    "    \"\"\"\n",
    "    if chunk is None:\n",
    "        return fn\n",
    "    def ret(inputs):\n",
    "        return torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
    "    return ret\n",
    "\n",
    "def run_network(inputs, view_direction, fn, embed_fn, embeddirs_fn, netchunk=1024*64):\n",
    "    \"\"\"Prepares inputs and applies network 'fn'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # [DONE] fn = model\n",
    "    # [NOT] pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples, 3]\n",
    "    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]]) # -1 is the last one\n",
    "    \n",
    "    # [NOT]  embad the position\n",
    "    embedded = embed_fn(inputs_flat)\n",
    "\n",
    "    if view_direction is not None:\n",
    "        input_dirs = view_direction[:,None].expand(inputs.shape)\n",
    "        input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n",
    "        embedded_dirs = embeddirs_fn(input_dirs_flat)\n",
    "        embedded = torch.cat([embedded, embedded_dirs], -1)\n",
    "\n",
    "    outputs_flat = batchify(fn, netchunk)(embedded)\n",
    "    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def batchify_rays(rays_flat, chunk=1024*32, **kwargs):\n",
    "    \"\"\"Render rays in smaller minibatches to avoid OOM.\n",
    "    \"\"\"\n",
    "    all_ret = {}\n",
    "    for i in range(0, rays_flat.shape[0], chunk):\n",
    "        ret = render_rays(rays_flat[i:i+chunk], **kwargs)\n",
    "        for k in ret:\n",
    "            if k not in all_ret:\n",
    "                all_ret[k] = []\n",
    "            all_ret[k].append(ret[k])\n",
    "\n",
    "    all_ret = {k : torch.cat(all_ret[k], 0) for k in all_ret}\n",
    "    return all_ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def render(H, W, K, chunk=1024*32, rays=None, c2w=None, ndc=True,\n",
    "                  near=0., far=1.,\n",
    "                  use_viewdirs=False, c2w_staticcam=None,\n",
    "                  **kwargs):\n",
    "    \"\"\"Render rays\n",
    "    Args:\n",
    "      H: int. Height of image in pixels.\n",
    "      W: int. Width of image in pixels.\n",
    "      focal: float. Focal length of pinhole camera.\n",
    "      chunk: int. Maximum number of rays to process simultaneously. Used to\n",
    "        control maximum memory usage. Does not affect final results.\n",
    "      rays: array of shape [2, batch_size, 3]. Ray origin and direction for\n",
    "        each example in batch.\n",
    "      c2w: array of shape [3, 4]. Camera-to-world transformation matrix.\n",
    "      ndc: bool. If True, represent ray origin, direction in NDC coordinates.\n",
    "      near: float or array of shape [batch_size]. Nearest distance for a ray.\n",
    "      far: float or array of shape [batch_size]. Farthest distance for a ray.\n",
    "      use_viewdirs: bool. If True, use viewing direction of a point in space in model.\n",
    "      c2w_staticcam: array of shape [3, 4]. If not None, use this transformation matrix for \n",
    "       camera while using other c2w argument for viewing directions.\n",
    "    Returns:\n",
    "      rgb_map: [batch_size, 3]. Predicted RGB values for rays.\n",
    "      disp_map: [batch_size]. Disparity map. Inverse of depth.\n",
    "      acc_map: [batch_size]. Accumulated opacity (alpha) along a ray.\n",
    "      extras: dict with everything returned by render_rays().\n",
    "    \"\"\"\n",
    "    if c2w is not None:\n",
    "        # special case to render full image\n",
    "        rays_o, rays_d = get_rays(H, W, K, c2w)\n",
    "    else:\n",
    "        # use provided ray batch\n",
    "        rays_o, rays_d = rays\n",
    "\n",
    "    if use_viewdirs:\n",
    "        # provide ray directions as input\n",
    "        viewdirs = rays_d\n",
    "        if c2w_staticcam is not None:\n",
    "            # special case to visualize effect of viewdirs\n",
    "            rays_o, rays_d = get_rays(H, W, K, c2w_staticcam)\n",
    "        viewdirs = viewdirs / torch.norm(viewdirs, dim=-1, keepdim=True)\n",
    "        viewdirs = torch.reshape(viewdirs, [-1,3]).float()\n",
    "\n",
    "    sh = rays_d.shape # [..., 3]\n",
    "    if ndc:\n",
    "        # for forward facing scenes\n",
    "        rays_o, rays_d = ndc_rays(H, W, K[0][0], 1., rays_o, rays_d)\n",
    "\n",
    "    # Create ray batch\n",
    "    rays_o = torch.reshape(rays_o, [-1,3]).float()\n",
    "    rays_d = torch.reshape(rays_d, [-1,3]).float()\n",
    "\n",
    "    near, far = near * torch.ones_like(rays_d[...,:1]), far * torch.ones_like(rays_d[...,:1])\n",
    "    rays = torch.cat([rays_o, rays_d, near, far], -1)\n",
    "    if use_viewdirs:\n",
    "        rays = torch.cat([rays, viewdirs], -1)\n",
    "\n",
    "    # Render and reshape\n",
    "    all_ret = batchify_rays(rays, chunk, **kwargs)\n",
    "    for k in all_ret:\n",
    "        k_sh = list(sh[:-1]) + list(all_ret[k].shape[1:])\n",
    "        all_ret[k] = torch.reshape(all_ret[k], k_sh)\n",
    "\n",
    "    k_extract = ['rgb_map', 'disp_map', 'acc_map']\n",
    "    ret_list = [all_ret[k] for k in k_extract]\n",
    "    ret_dict = {k : all_ret[k] for k in all_ret if k not in k_extract}\n",
    "    return ret_list + [ret_dict]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def render_rays(ray_batch,\n",
    "                network_fn,\n",
    "                network_query_fn,\n",
    "                N_samples,\n",
    "                return_raw=False,\n",
    "                linear_depend_inverse_depth=False,\n",
    "                perturb=0.,\n",
    "                N_importance=0,\n",
    "                network_fine=None,\n",
    "                white_background=False,\n",
    "                raw_noise_std=0.,\n",
    "                verbose=False,\n",
    "                pytest=False):\n",
    "    \"\"\"Volumetric rendering.\n",
    "    Args:\n",
    "      ray_batch: array of shape [batch_size, ...]. All information necessary\n",
    "        for sampling along a ray, including: ray origin, ray direction, min\n",
    "        dist, max dist, and unit-magnitude viewing direction.\n",
    "      network_fn: function. Model for predicting RGB and density at each point\n",
    "        in space.\n",
    "      network_query_fn: function used for passing queries to network_fn.\n",
    "      N_samples: int. Number of different times to sample along each ray.\n",
    "      return_raw: bool. If True, include model's raw, unprocessed predictions.\n",
    "      linear_depend_inverse: bool. If True, sample linearly in inverse depth rather than in depth.\n",
    "      perturb: float, 0 or 1. If non-zero, each ray is sampled at stratified\n",
    "        random points in time.\n",
    "      N_importance: int. Number of additional times to sample along each ray.\n",
    "        These samples are only passed to network_fine.\n",
    "      network_fine: \"fine\" network with same spec as network_fn.\n",
    "      white_background: bool. If True, assume a white background.\n",
    "      raw_noise_std: ...\n",
    "      verbose: bool. If True, print more debugging info.\n",
    "    Returns:\n",
    "      rgb_map: [num_rays, 3]. Estimated RGB color of a ray. Comes from fine model.\n",
    "      disparity_map: [num_rays]. Disparity map. 1 / depth.\n",
    "      accumulated_opacity_map: [num_rays]. Accumulated opacity along each ray. Comes from fine model.\n",
    "      raw: [num_rays, num_samples, 4]. Raw predictions from model.\n",
    "      rgb0: See rgb_map. Output for coarse model.\n",
    "      disp0: See disparity_map. Output for coarse model.\n",
    "      acc0: See accumulated_opacity_map. Output for coarse model.\n",
    "      z_std: [num_rays]. Standard deviation of distances along ray for each\n",
    "        sample.\n",
    "    \"\"\"\n",
    "    N_rays = ray_batch.shape[0]\n",
    "    rays_origin, rays_direction = ray_batch[:,0:3], ray_batch[:,3:6] # [N_rays, 3] each\n",
    "    view_direction = ray_batch[:,-3:] if ray_batch.shape[-1] > 8 else None\n",
    "    bounds = torch.reshape(ray_batch[...,6:8], [-1,1,2])\n",
    "    near, far = bounds[...,0], bounds[...,1] # [-1,1]\n",
    "\n",
    "    t_vals = torch.linspace(0., 1., steps=N_samples).to(device)\n",
    "    \n",
    "    if not linear_depend_inverse_depth:\n",
    "        z_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "    else:\n",
    "        z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "\n",
    "    z_vals = z_vals.expand([N_rays, N_samples]).to(device)\n",
    "\n",
    "    if perturb > 0.:\n",
    "        # get intervals between samples\n",
    "        mids = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n",
    "        upper = torch.cat([mids, z_vals[...,-1:]], -1)\n",
    "        lower = torch.cat([z_vals[...,:1], mids], -1)\n",
    "        # stratified samples in those intervals\n",
    "        t_rand = torch.rand(z_vals.shape).to(device)\n",
    "\n",
    "        # Pytest, overwrite u with numpy's fixed random numbers\n",
    "        if pytest:\n",
    "            np.random.seed(0)\n",
    "            t_rand = np.random.rand(*list(z_vals.shape))\n",
    "            t_rand = torch.Tensor(t_rand).to(device)\n",
    "\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "\n",
    "    # form the origin to the end of the ray, in the ray direction to form pts\n",
    "    pts = rays_origin[...,None,:] + rays_direction[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples, 3]\n",
    "    \n",
    "\n",
    "\n",
    "#     raw = run_network(pts)\n",
    "    raw = network_query_fn(pts, view_direction, network_fn)\n",
    "    rgb_map, disparity_map, accumulated_opacity_map, weights, depth_map = raw2outputs(raw, z_vals, rays_direction, raw_noise_std, white_background, pytest=pytest)\n",
    "\n",
    "    if N_importance > 0:\n",
    "\n",
    "        rgb_map_0, disp_map_0, acc_map_0 = rgb_map, disparity_map, accumulated_opacity_map\n",
    "\n",
    "        z_vals_mid = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n",
    "        z_samples = sample_pdf(z_vals_mid, weights[...,1:-1], N_importance, det=(perturb==0.), pytest=pytest)\n",
    "        z_samples = z_samples.detach()\n",
    "\n",
    "        z_vals, _ = torch.sort(torch.cat([z_vals, z_samples], -1), -1)\n",
    "        pts = rays_origin[...,None,:] + rays_direction[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples + N_importance, 3]\n",
    "\n",
    "        run_fn = network_fn if network_fine is None else network_fine\n",
    "#         raw = run_network(pts, fn=run_fn)\n",
    "        raw = network_query_fn(pts, view_direction, run_fn)\n",
    "\n",
    "        rgb_map, disparity_map, accumulated_opacity_map, weights, depth_map = raw2outputs(raw, z_vals, rays_direction, raw_noise_std, white_background, pytest=pytest)\n",
    "\n",
    "    ret = {'rgb_map' : rgb_map, 'disp_map' : disparity_map, 'acc_map' : accumulated_opacity_map}\n",
    "    if return_raw:\n",
    "        ret['raw'] = raw\n",
    "    if N_importance > 0:\n",
    "        ret['rgb0'] = rgb_map_0\n",
    "        ret['disp0'] = disp_map_0\n",
    "        ret['acc0'] = acc_map_0\n",
    "        ret['z_std'] = torch.std(z_samples, dim=-1, unbiased=False)  # [N_rays]\n",
    "\n",
    "    for k in ret:\n",
    "        if (torch.isnan(ret[k]).any() or torch.isinf(ret[k]).any()) and DEBUG:\n",
    "            print(f\"! [Numerical Error] {k} contains nan or inf.\")\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "def raw2outputs(raw, z_vals, rays_direction, raw_noise_std=0, white_background=False, pytest=False):\n",
    "    \"\"\"Transforms model's predictions to semantically meaningful values.\n",
    "    Args:\n",
    "        raw: [num_rays, num_samples along ray, 4]. Prediction from model.\n",
    "        z_vals: [num_rays, num_samples along ray]. Integration time.\n",
    "        rays_direction: [num_rays, 3]. Direction of each ray.\n",
    "    Returns:\n",
    "        rgb_map: [num_rays, 3]. Estimated RGB color of a ray.\n",
    "        disparity_map: [num_rays]. Disparity map. Inverse of depth map.\n",
    "        accumulated_opacity_map: [num_rays]. Sum of weights along each ray.\n",
    "        weights: [num_rays, num_samples]. Weights assigned to each sampled color.\n",
    "        depth_map: [num_rays]. Estimated distance to object.\n",
    "    \"\"\"\n",
    "    raw2alpha = lambda raw, distances, act_fn=F.relu: 1.-torch.exp(-act_fn(raw)*distances)\n",
    "\n",
    "    distances = z_vals[...,1:] - z_vals[...,:-1]\n",
    "    distances = torch.cat([distances, torch.Tensor([1e10]).to(device).expand(distances[...,:1].shape)], -1)  # [N_rays, N_samples]\n",
    "\n",
    "    distances = distances * torch.norm(rays_direction[...,None,:], dim=-1)\n",
    "\n",
    "    rgb = torch.sigmoid(raw[...,:3])  # [N_rays, N_samples, 3]\n",
    "    noise = 0.\n",
    "    if raw_noise_std > 0.:\n",
    "        noise = torch.randn(raw[...,3].shape).to(device) * raw_noise_std\n",
    "\n",
    "        # Overwrite randomly sampled data if pytest\n",
    "        if pytest:\n",
    "            np.random.seed(0)\n",
    "            noise = np.random.rand(*list(raw[...,3].shape)) * raw_noise_std\n",
    "            noise = torch.Tensor(noise).to(device)\n",
    "\n",
    "    alpha = raw2alpha(raw[...,3] + noise, distances)  # [N_rays, N_samples]\n",
    "    # weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n",
    "    # tensor = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float)\n",
    "    # cumprod_tensor = torch.cumprod(tensor, dim=0)\n",
    "    # tensor([1., 2., 6., 24., 120.])\n",
    "    # torch.cumprod() is Ti of all the point\n",
    "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1)).to(device), 1.-alpha + 1e-10], -1), -1)[:, :-1]\n",
    "    rgb_map = torch.sum(weights[...,None] * rgb, -2)  # [N_rays, 3]\n",
    "\n",
    "    depth_map = torch.sum(weights * z_vals, -1)\n",
    "    disparity_map = 1./torch.max(1e-10 * torch.ones_like(depth_map).to(device), depth_map / torch.sum(weights, -1))\n",
    "    accumulated_opacity_map = torch.sum(weights, -1)\n",
    "\n",
    "    if white_background:\n",
    "        rgb_map = rgb_map + (1.-accumulated_opacity_map[...,None])\n",
    "\n",
    "    return rgb_map, disparity_map, accumulated_opacity_map, weights, depth_map\n",
    "\n",
    "\n",
    "def render_path(render_poses, hwf, K, chunk, render_kwargs, gt_imgs=None, savedir=None, render_factor=0):\n",
    "\n",
    "    H, W, focal = hwf\n",
    "\n",
    "    if render_factor!=0:\n",
    "        # Render downsampled for speed\n",
    "        H = H//render_factor\n",
    "        W = W//render_factor\n",
    "        focal = focal/render_factor\n",
    "\n",
    "    rgbs = []\n",
    "    disps = []\n",
    "\n",
    "    t = time.time()\n",
    "    for i, c2w in enumerate(tqdm(render_poses)):\n",
    "        print(i, time.time() - t)\n",
    "        t = time.time()\n",
    "        rgb, disp, acc, _ = render(H, W, K, chunk=chunk, c2w=c2w[:3,:4], **render_kwargs)\n",
    "        rgbs.append(rgb.cpu().numpy())\n",
    "        disps.append(disp.cpu().numpy())\n",
    "        if i==0:\n",
    "            print(rgb.shape, disp.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        if gt_imgs is not None and render_factor==0:\n",
    "            p = -10. * np.log10(np.mean(np.square(rgb.cpu().numpy() - gt_imgs[i])))\n",
    "            print(p)\n",
    "        \"\"\"\n",
    "\n",
    "        if savedir is not None:\n",
    "            rgb8 = to8b(rgbs[-1])\n",
    "            filename = os.path.join(savedir, '{:03d}.png'.format(i))\n",
    "            imageio.imwrite(filename, rgb8)\n",
    "\n",
    "\n",
    "    rgbs = np.stack(rgbs, 0)\n",
    "    disps = np.stack(disps, 0)\n",
    "\n",
    "    return rgbs, disps\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "002a2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    \n",
    "    #########################################################################\n",
    "    # python run_nerf.py --config configs/fern.txt\n",
    "    N_importance = 64\n",
    "    N_rand = 1024\n",
    "    N_samples = 64\n",
    "    basedir = './logs'\n",
    "    chunk = 32768\n",
    "    config = 'configs/fern.txt'\n",
    "    datadir = './data/nerf_llff_data/fern'\n",
    "    dataset_type = 'llff'\n",
    "    expname = 'fern_test'\n",
    "    factor = 8\n",
    "    ft_path = None\n",
    "    half_res = False\n",
    "    i_embed = 0\n",
    "    i_img = 500\n",
    "    i_print = 100\n",
    "    i_testset = 5000\n",
    "    i_video = 5000\n",
    "    i_weights = 10000\n",
    "    lindisp = False\n",
    "    llffhold = 8\n",
    "    lrate = 0.0005\n",
    "    lrate_decay = 250\n",
    "    multires = 10\n",
    "    multires_views = 4\n",
    "    netchunk = 65536\n",
    "    netdepth = 8\n",
    "    netdepth_fine = 8\n",
    "    netwidth = 256\n",
    "    netwidth_fine = 256\n",
    "    no_batching = False\n",
    "    no_ndc = False\n",
    "    no_reload = False\n",
    "    perturb = 1.0\n",
    "    precrop_frac = 0.5\n",
    "    precrop_iters = 0\n",
    "    raw_noise_std = 1.0\n",
    "    render_factor = 0\n",
    "    render_only = False\n",
    "    render_test = False\n",
    "    shape = 'greek'\n",
    "    spherify = False\n",
    "    testskip = 8\n",
    "    use_viewdirs = True\n",
    "    white_background = False\n",
    "    ################################################################################\n",
    "\n",
    "    images, poses, bds, render_poses, i_test = load_llff_data(datadir, factor, recenter=True, bd_factor=.75, spherify=spherify)\n",
    "    print(\"================[0][load_data]=================\")\n",
    "    #print(images.shape)   (20, 378, 504, 3)\n",
    "    #print(poses.shape)    (20, 3, 5)\n",
    "    #print(bds.shape)      (20, 2)\n",
    "    #print(render_poses.shape)  (120, 3, 5)   # Generate poses for spiral path\n",
    "    #print(i_test) 12           print('HOLDOUT view is', i_test)\n",
    "    #print(i_test.shape)\n",
    "\n",
    "    #print(bds) [0.5500126  2.4253333 ]\n",
    "    # print(bds.shape)  (20, 2)\n",
    "    # []\n",
    "\n",
    "\n",
    "    # what is the (20, 3, 5), 3 is for what, 5 is for what\n",
    "    hwf = poses[0,:3,-1]\n",
    "    poses = poses[:,:3,:4]\n",
    "    #print(poses.shape) (20, 3, 4)\n",
    "    print(poses[0])\n",
    "    # (20, 378, 504, 3) (120, 3, 5) [378.     504.     407.5658] ./data/nerf_llff_data/fern\n",
    "    print('Loaded llff', images.shape, render_poses.shape, hwf, datadir)\n",
    "\n",
    "    if not isinstance(i_test, list):\n",
    "        i_test = [i_test]\n",
    "\n",
    "    #print(i_test) [12]\n",
    "\n",
    "\n",
    "    if llffhold > 0:\n",
    "        print('Auto LLFF holdout,', llffhold)\n",
    "        i_test = np.arange(images.shape[0])[::llffhold]\n",
    "    #print(i_test) [ 0  8 16]\n",
    "\n",
    "    i_val = i_test\n",
    "    i_train = np.array([i for i in np.arange(int(images.shape[0])) if\n",
    "                    (i not in i_test and i not in i_val)])\n",
    "    #print(i_train) [ 1  2  3  4  5  6  7  9 10 11 12 13 14 15 17 18 19]\n",
    "\n",
    "\n",
    "    print('DEFINING BOUNDS')\n",
    "    if no_ndc:\n",
    "        near = np.ndarray.min(bds) * .9\n",
    "        far = np.ndarray.max(bds) * 1.\n",
    "\n",
    "    else:\n",
    "        near = 0.\n",
    "        far = 1.\n",
    "    print('NEAR FAR', near, far) # 0.4737630307674408 2.4794018268585205\n",
    "\n",
    "\n",
    "    # 1 Cast intrinsics to right types\n",
    "    H, W, focal = hwf\n",
    "    #print(hwf)  [378.     504.     407.5658]\n",
    "    H, W = int(H), int(W)\n",
    "    hwf = [H, W, focal]\n",
    "    \n",
    "    #print(hwf) [378, 504, 407.5658]\n",
    "\n",
    "    # Load data\n",
    "\n",
    "    K = np.array([\n",
    "        [focal, 0, 0.5*W],\n",
    "        [0, focal, 0.5*H],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    print(K)    \n",
    "    \n",
    "    basedir = basedir\n",
    "    expname = expname\n",
    "    os.makedirs(os.path.join(basedir, expname), exist_ok=True)    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    # create_nerf()\n",
    "    \n",
    "    print(\"================[1][create_nerf()]=================\")\n",
    "    # Positional encoding\n",
    "    \n",
    "    embed_fn, input_ch = get_embedder(multires, i_embed)\n",
    "\n",
    "    input_ch_views = 0\n",
    "    embeddirs_fn = None\n",
    "    if use_viewdirs:\n",
    "        embeddirs_fn, input_ch_views = get_embedder(multires_views, i_embed)\n",
    "    output_ch = 5 if N_importance > 0 else 4\n",
    "    skips = [4]\n",
    "    \n",
    "    \n",
    "    model = NeRF(D=netdepth, W=netwidth,\n",
    "                 input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                 input_ch_views=input_ch_views, use_viewdirs=use_viewdirs).to(device)\n",
    "    grad_vars = list(model.parameters())\n",
    "\n",
    "    \n",
    "    model_fine = None\n",
    "    if N_importance > 0:\n",
    "        model_fine = NeRF(D=netdepth_fine, W=netwidth_fine,\n",
    "                          input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                          input_ch_views=input_ch_views, use_viewdirs=use_viewdirs).to(device)\n",
    "        grad_vars += list(model_fine.parameters())\n",
    "\n",
    "    network_query_fn = lambda inputs, viewdirs, network_fn : run_network(inputs, viewdirs, network_fn,\n",
    "                                                                embed_fn=embed_fn,\n",
    "                                                                embeddirs_fn=embeddirs_fn,\n",
    "                                                                netchunk=netchunk)\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.Adam(params=grad_vars, lr=lrate, betas=(0.9, 0.999))\n",
    "\n",
    "    start = 0\n",
    "    basedir = basedir\n",
    "    expname = expname\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    # Load checkpoints\n",
    "    if ft_path is not None and ft_path!='None':\n",
    "        ckpts = [ft_path]\n",
    "    else:\n",
    "        ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if 'tar' in f]\n",
    "\n",
    "    print('Found ckpts', ckpts)\n",
    "    if len(ckpts) > 0 and not no_reload:\n",
    "        ckpt_path = ckpts[-1]\n",
    "        print('Reloading from', ckpt_path)\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "\n",
    "        start = ckpt['global_step']\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "\n",
    "        # Load model\n",
    "        model.load_state_dict(ckpt['network_fn_state_dict'])\n",
    "        if model_fine is not None:\n",
    "            model_fine.load_state_dict(ckpt['network_fine_state_dict'])\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    render_kwargs_train = {\n",
    "        'network_query_fn' : network_query_fn,\n",
    "        'perturb' : perturb,\n",
    "        'N_importance' : N_importance,\n",
    "        'network_fine' : model_fine,\n",
    "        'N_samples' : N_samples,\n",
    "        'network_fn' : model,\n",
    "        'use_viewdirs' : use_viewdirs,\n",
    "        'white_background' : white_background,\n",
    "        'raw_noise_std' : raw_noise_std,\n",
    "    }\n",
    "\n",
    "    # NDC only good for LLFF-style forward facing data\n",
    "    if dataset_type != 'llff' or no_ndc:\n",
    "        print('Not ndc!')\n",
    "        render_kwargs_train['ndc'] = False\n",
    "        render_kwargs_train['lindisp'] = lindisp\n",
    "\n",
    "    render_kwargs_test = {k : render_kwargs_train[k] for k in render_kwargs_train}\n",
    "    render_kwargs_test['perturb'] = False\n",
    "    render_kwargs_test['raw_noise_std'] = 0.\n",
    "    \n",
    "    \n",
    "    print(\"================[2][get training data, validation data]=================\")\n",
    "\n",
    "    global_step = start\n",
    "\n",
    "    bds_dict = {\n",
    "        'near' : near,\n",
    "        'far' : far,\n",
    "    }\n",
    "\n",
    "    # near, far to dictionary\n",
    "    render_kwargs_train.update(bds_dict)\n",
    "    render_kwargs_test.update(bds_dict)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Move testing data to GPU\n",
    "    render_poses = torch.Tensor(render_poses).to(device)\n",
    "    #print(render_poses)\n",
    "    #print(render_poses.shape) torch.Size([3, 3, 4])    \n",
    "    \n",
    "    # Prepare raybatch tensor if batching random rays\n",
    "    N_rand = N_rand # 1024\n",
    "    use_batching = not no_batching # no_batching = True\n",
    "    \n",
    "    \n",
    "    \n",
    "    if use_batching:\n",
    "        # For random ray batching\n",
    "        print('get rays')\n",
    "        rays = np.stack([get_rays_np(H, W, K, p) for p in poses[:,:3,:4]], 0) # [N, ro+rd, H, W, 3]\n",
    "        print('done, concats')\n",
    "        rays_rgb = np.concatenate([rays, images[:,None]], 1) # [N, ro+rd+rgb, H, W, 3]\n",
    "        rays_rgb = np.transpose(rays_rgb, [0,2,3,1,4]) # [N, H, W, ro+rd+rgb, 3]\n",
    "        rays_rgb = np.stack([rays_rgb[i] for i in i_train], 0) # train images only\n",
    "        rays_rgb = np.reshape(rays_rgb, [-1,3,3]) # [(N-1)*H*W, ro+rd+rgb, 3]\n",
    "        rays_rgb = rays_rgb.astype(np.float32)\n",
    "        print('shuffle rays')\n",
    "        np.random.shuffle(rays_rgb)\n",
    "\n",
    "        print('done')\n",
    "        i_batch = 0\n",
    "\n",
    "    # Move training data to GPU\n",
    "    if use_batching:\n",
    "        images = torch.Tensor(images).to(device)\n",
    "    poses = torch.Tensor(poses).to(device)\n",
    "    \n",
    "    #print(poses)\n",
    "    #print(poses.shape) #torch.Size([20, 3, 4])\n",
    "    \n",
    "    if use_batching:\n",
    "        rays_rgb = torch.Tensor(rays_rgb).to(device)    \n",
    "\n",
    "    N_iters = 10000 + 1\n",
    "    print('Begin')\n",
    "    print('TRAIN views are', i_train)\n",
    "    print('TEST views are', i_test)\n",
    "    print('VAL views are', i_val)\n",
    "\n",
    "    start = start + 1   \n",
    "    for i in trange(start, N_iters):\n",
    "        time0 = time.time()\n",
    "        \n",
    "                # Sample random ray batch\n",
    "        if use_batching:\n",
    "            # Random over all images\n",
    "            batch = rays_rgb[i_batch:i_batch+N_rand] # [B, 2+1, 3*?]\n",
    "            batch = torch.transpose(batch, 0, 1)\n",
    "            batch_rays, target_s = batch[:2], batch[2]\n",
    "\n",
    "            i_batch += N_rand\n",
    "            if i_batch >= rays_rgb.shape[0]:\n",
    "                print(\"Shuffle data after an epoch!\")\n",
    "                rand_idx = torch.randperm(rays_rgb.shape[0])\n",
    "                rays_rgb = rays_rgb[rand_idx]\n",
    "                i_batch = 0\n",
    "                \n",
    "                \n",
    "        #####  Core optimization loop  #####\n",
    "        rgb, disp, acc, extras = render(H, W, K, chunk=chunk, rays=batch_rays, verbose=i < 10, return_raw=True, **render_kwargs_train)   \n",
    "        \n",
    "        #print(\"================[3][loss]=================\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # get the loss of model prediction and the target image\n",
    "        img_loss = img2mse(rgb, target_s)\n",
    "        trans = extras['raw'][...,-1]\n",
    "        loss = img_loss\n",
    "        psnr = mse2psnr(img_loss)\n",
    "\n",
    "        #input(\"Press Enter to continue...\")\n",
    "        \n",
    "        if 'rgb0' in extras:\n",
    "            img_loss0 = img2mse(extras['rgb0'], target_s)\n",
    "            loss = loss + img_loss0\n",
    "            psnr0 = mse2psnr(img_loss0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # NOTE: IMPORTANT!\n",
    "        ###   update learning rate   ###\n",
    "        decay_rate = 0.1\n",
    "        decay_steps = lrate_decay * 1000\n",
    "        new_lrate = lrate * (decay_rate ** (global_step / decay_steps))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = new_lrate\n",
    "        ################################\n",
    "\n",
    "        dt = time.time()-time0\n",
    "\n",
    "        # Rest is logging\n",
    "        if i%i_weights==0:\n",
    "            path = os.path.join(basedir, expname, '{:06d}.tar'.format(i))\n",
    "            torch.save({\n",
    "                'global_step': global_step,\n",
    "                'network_fn_state_dict': render_kwargs_train['network_fn'].state_dict(),\n",
    "                'network_fine_state_dict': render_kwargs_train['network_fine'].state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, path)\n",
    "            print('Saved checkpoints at', path)\n",
    "\n",
    "        if i%i_video==0 and i > 0:\n",
    "            # Turn on testing mode\n",
    "            with torch.no_grad():\n",
    "                rgbs, disps = render_path(render_poses, hwf, K, chunk, render_kwargs_test)\n",
    "            print('Done, saving', rgbs.shape, disps.shape)\n",
    "            moviebase = os.path.join(basedir, expname, '{}_spiral_{:06d}_'.format(expname, i))\n",
    "            imageio.mimwrite(moviebase + 'rgb.mp4', to8b(rgbs), fps=30, quality=8)\n",
    "            imageio.mimwrite(moviebase + 'disp.mp4', to8b(disps / np.max(disps)), fps=30, quality=8)\n",
    "\n",
    "        if i%i_testset==0 and i > 0:\n",
    "            testsavedir = os.path.join(basedir, expname, 'testset_{:06d}'.format(i))\n",
    "            os.makedirs(testsavedir, exist_ok=True)\n",
    "            print('test poses shape', poses[i_test].shape)\n",
    "            with torch.no_grad():\n",
    "                render_path(torch.Tensor(poses[i_test]).to(device), hwf, K, chunk, render_kwargs_test, gt_imgs=images[i_test], savedir=testsavedir)\n",
    "            print('Saved test set')\n",
    "\n",
    "\n",
    "    \n",
    "        if i%i_print==0:\n",
    "            tqdm.write(f\"[TRAIN] Iter: {i} Loss: {loss.item()}  PSNR: {psnr.item()}\")\n",
    "\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64a16c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================[0][load_data]=================\n",
      "[[ 0.99569476 -0.02079598 -0.09033062 -0.3081002 ]\n",
      " [ 0.02503342  0.9986262   0.04603354  0.1346772 ]\n",
      " [ 0.0892492  -0.04809664  0.99484736  0.03989876]]\n",
      "Loaded llff (20, 378, 504, 3) (120, 3, 5) [378.     504.     407.5658] ./data/nerf_llff_data/fern\n",
      "Auto LLFF holdout, 8\n",
      "DEFINING BOUNDS\n",
      "NEAR FAR 0.0 1.0\n",
      "[[407.5657959   0.        252.       ]\n",
      " [  0.        407.5657959 189.       ]\n",
      " [  0.          0.          1.       ]]\n",
      "================[1][create_nerf()]=================\n",
      "Found ckpts []\n",
      "================[2][get training data, validation data]=================\n",
      "get rays\n",
      "done, concats\n",
      "shuffle rays\n",
      "done\n",
      "Begin\n",
      "TRAIN views are [ 1  2  3  4  5  6  7  9 10 11 12 13 14 15 17 18 19]\n",
      "TEST views are [ 0  8 16]\n",
      "VAL views are [ 0  8 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                      | 101/10000 [00:17<28:12,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 100 Loss: 0.05000138282775879  PSNR: 16.092008590698242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                      | 201/10000 [00:34<27:23,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 200 Loss: 0.040701426565647125  PSNR: 16.866830825805664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▏                                     | 301/10000 [00:51<28:28,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 300 Loss: 0.035592541098594666  PSNR: 17.498321533203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▌                                     | 401/10000 [01:09<28:26,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 400 Loss: 0.029034169390797615  PSNR: 18.322460174560547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▉                                     | 501/10000 [01:26<27:43,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 500 Loss: 0.028635818511247635  PSNR: 18.571069717407227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▎                                    | 601/10000 [01:43<28:32,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 600 Loss: 0.026848917827010155  PSNR: 18.720518112182617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▋                                    | 701/10000 [02:01<27:01,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 700 Loss: 0.02584030292928219  PSNR: 18.807573318481445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███                                    | 801/10000 [02:18<26:35,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 800 Loss: 0.02019038051366806  PSNR: 19.903182983398438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▌                                   | 901/10000 [02:35<26:38,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 900 Loss: 0.02085859328508377  PSNR: 19.741737365722656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▊                                  | 1001/10000 [02:53<26:39,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 1000 Loss: 0.020692788064479828  PSNR: 19.81845474243164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▏                                 | 1101/10000 [03:10<25:10,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 1100 Loss: 0.018079832196235657  PSNR: 20.492544174194336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▌                                 | 1201/10000 [03:28<26:31,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 1200 Loss: 0.018537279218435287  PSNR: 20.537519454956055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|████▉                                 | 1301/10000 [03:46<25:39,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 1300 Loss: 0.01819085143506527  PSNR: 20.61172103881836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▎                                | 1401/10000 [04:03<24:06,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 1400 Loss: 0.018735887482762337  PSNR: 20.297653198242188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▋                                | 1501/10000 [04:20<25:30,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 1500 Loss: 0.016106752678751945  PSNR: 20.887611389160156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████                                | 1601/10000 [04:38<24:10,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 1600 Loss: 0.018316052854061127  PSNR: 20.456167221069336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████▍                               | 1701/10000 [04:56<24:25,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 1700 Loss: 0.01708853244781494  PSNR: 20.769855499267578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████▊                               | 1801/10000 [05:13<24:15,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 1800 Loss: 0.016948916018009186  PSNR: 20.67842674255371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████▏                              | 1901/10000 [05:31<23:12,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 1900 Loss: 0.01610463485121727  PSNR: 20.805158615112305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▌                              | 2001/10000 [05:48<23:04,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 2000 Loss: 0.015509312972426414  PSNR: 21.055789947509766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████▉                              | 2101/10000 [06:06<23:06,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 2100 Loss: 0.012322069145739079  PSNR: 22.089405059814453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████▎                             | 2201/10000 [06:23<22:34,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 2200 Loss: 0.015002301894128323  PSNR: 21.328351974487305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████▋                             | 2301/10000 [06:41<22:45,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 2300 Loss: 0.014528477564454079  PSNR: 21.472349166870117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████                             | 2401/10000 [06:58<23:05,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 2400 Loss: 0.014301163144409657  PSNR: 21.61503028869629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▌                            | 2501/10000 [07:16<21:32,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 2500 Loss: 0.01277136616408825  PSNR: 22.009958267211914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|█████████▉                            | 2601/10000 [07:33<22:55,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 2600 Loss: 0.01306568831205368  PSNR: 21.905452728271484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████                            | 2647/10000 [07:42<21:22,  5.73it/s]"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d01132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1fa0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
